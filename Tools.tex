\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Tools for calculating scattering amplitudes}
In this chapter, we focus on various aspects of the computation of two-loop QCD scattering amplitudes for high-multiplicity processes. It is important to note that there is currently no one-size-fits-all approach that would allow us to compute all the desired amplitudes at a press of a button. In practice, we use a collection of methods that are most appropriate for the task at hand. For processes at the limit of current capabilities, these tools need to be further improved or replaced with novel ideas. To this end, much work has been done by the theory community in recent years. Unfortunately, due to the overwhelming algebraic and analytic complexity, many calculations still present challenges beyond the reach of current technology. 
\begin{figure}[t]
\begin{tikzpicture}
	\node (feynman) [greenrec] {Feynman diagrams};
	\node (colour)  [redrec,right of=feynman,xshift=4cm] {Colour decomposition};
	\node (topos)   [redrec,right of=colour,xshift=4cm] {Helicity amplitudes};
	\node (reduction) [bluerec,below of=topos,yshift=-2cm] {\parbox{0.3\textwidth}{\centering Integrand reduction onto \\ maximal topologies}};
	\node (IBPs)    [bluerec,left of=reduction,xshift=-4cm] {IBP reduction};
	\node (spfns)   [bluerec,left of=IBPs, xshift=-4cm] {\parbox{0.3\textwidth}{\centering Expansion of MIs onto \\ special function basis}};
	\node (sub)     [bluerec,below of=spfns,yshift=-2cm] {Pole subtraction};
	\node (finrem)  [bluerec,right of=sub,xshift=4cm] {Finite remainder};
	\node (rec)  [bluerec,right of=finrem,xshift=4cm] {Reconstruction};
	\node (qgraf) [below of=feynman, opacity=0.7] {\textcolor{green}{\Large QGRAF}};
	\node (mma) [below right of=colour, xshift=1.5cm, yshift=-0.3cm, opacity=0.7] {\textcolor{red}{\Large Mathematica/FORM}};
	\node (ff) [below of=IBPs, yshift=-0.5cm,opacity=0.7] {\textcolor{blue}{\Large finite fields}};
		
	\draw[->] (feynman.east) -- (colour.west);
	\draw[->] (colour.east) -- (topos.west);
	\draw[->] (topos.south) -- (reduction.north) node[midway,right] {$d=4-2\epsilon$};
	\draw[->] (reduction.west) -- (IBPs.east);
	\draw[->] (IBPs.west) -- (spfns.east);
	\draw[->] (spfns.south) -- (sub.north);
	\draw[->] (sub.east) -- (finrem.west) node[midway,above] {$\epsilon \rightarrow 0$};
	\draw[->] (finrem.east) -- (rec.west);
\end{tikzpicture}
\caption{A schematic overview of the workflow we adopt to compute scattering amplitudes.}
\label{fig:outline}
\end{figure}
The goal of this chapter is to provide an overview of the method we adopt in amplitude computations, as well as the problems that invariably follow. The procedure involves several highly non-trivial steps. To help the reader retain the `big picture' of the workflow, we present its schematic outline in Fig.~\ref{fig:outline}.  Each step is discussed in more detail below. 

\section{Feynman diagrams}
The starting point of our amplitude computation for a given process is the generation of all Feynman diagrams contributing to this process at the desired loop order. Feynman diagrams provide a pictorial representation of the ways in which the interaction can occur.
%time-ordered correlation functions that contribute to the $S$-matrix element
At the same time, the corresponding mathematical expressions can be easily recovered using Feynman rules, which can be derived form the Lagrangian of the theory under consideration \JK{(Feynman rules relevant to our work are listed in Appendix~\ref{app:feynmanrules})}. As such, these diagrams are an indispensable tool of any perturbative calculation. In practice, it can be observed that usually their number grows faster than exponentially as we increase the loop order or multiplicity (see Table~\ref{tab:ndiags} for an example). To handle the combinatorial complexity, we generate the relevant Feynman diagrams using $\texttt{QGRAF}$~\cite{Nogueira:1991ex}. This programme has the advantage of granting the user a large degree of control over the diagrams. For example, one can constrain it to generate diagrams without self-energy insertions or with a specified total power of the coupling constant. \JK{should I even mention that?}
\begin{table}[b]
	\begin{center}
		\begin{tabular}{r|c|c|c|c|c|c|c|c}
			  $n$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
			\hline
			$n$ gluons   & -- & -- & 1 & 4 & 25 & 220 & 2485 & 34300 \\
			$q\bar{q} + n$ gluons & 1 & 3 & 16 & 123 & 1240 & 15495 & 231280 & 4016775 \\
		\end{tabular}
	\end{center}
 \caption{Number of tree-level diagrams contributing to selected processes with $n$ gluons.}
 \label{tab:ndiags}
\end{table}

\section{Colour decomposition}
Having generated the Feynman diagrams, we substitute the Feynman rules for the propagators and vertices using \texttt{Mathematica}. At this point, our QCD amplitude contains both colour and kinematic information. The idea of colour ordering is to reorganise the amplitude such that these two components separate: a purely kinematic amplitude is multiplied by the corresponding colour factor. In other words, we perform the decomposition of the full amplitude in colour space, according to a chosen colour basis. Roughly speaking:
\begin{equation}
    \ampl{n}{} = \sum_i \text{(colour)}_i \times A_{n\,i} \,, 
\end{equation}
where $A_{n\,i}$ are the colour-ordered amplitudes (also known as colour-stripped or partial amplitudes). The motivation behind this decomposition is that the colour-ordered amplitudes turn out to be significantly simpler to calculate.

The choice of the colour basis is not unique. We adopt the decomposition according to traces of the $SU(N_c)$ generators in the fundamental representation. As an example, let's look at the 4-gluon scattering at tree-level\JK{Redo this diagram using tikzfeynman}:
\begin{equation} \label{eq:3grule}
\feynmandiagram [baseline = (i.base), horizontal = i to j] {
    a1 [particle={$A^{a_1}_{\mu_1}$}] -- [gluon, momentum=$p_1$] i;
    a2 [particle={$A^{a_2}_{\mu_2}$}] -- [gluon, momentum=$p_2$] i;
    i -- [gluon] j;
    a3 [particle={$A^{a_3}_{\mu_3}$}] -- [gluon, momentum=$p_3$] j;
    a4 [particle={$A^{a_4}_{\mu_4}$}] -- [gluon, momentum=$p_4$] j;
    };
    \xrightarrow{colour}
    f^{a_1a_2b}f^{ba_3a_4} \,.
\end{equation}
The colour factor of this diagram can be expressed in terms of the generators using Eqs.~\ref{eq:liealgebra} and ~\ref{eq:fierz}:
\begin{equation}
    f^{a_1a_2b}f^{ba_3a_4} = -\frac{1}{T_F}\left(\tr[T^{a_1} T^{a_2} T^{a_3} T^{a_4}] - \tr[T^{a_1} T^{a_2} T^{a_4} T^{a_3}] - \tr[T^{a_1} T^{a_3} T^{a_4} T^{a_2}] + \tr[T^{a_1} T^{a_4} T^{a_3} T^{a_3}] \right)\,.
\end{equation}
The colour factors of the $t$- and $u$-channels can be expressed in a similar way. Combining the contributions from the three channels and using the cyclicity of the trace, we can organise the 4-gluon amplitude at tree-level as follows:
\begin{equation}
    \ampl{4}{(0)} =  g^2 \left( \tr[T^{a_1} T^{a_2} T^{a_3} T^{a_4}]\,A_{4}^{(0)}[1234] + \text{permutations of } (234) \right) \,.
\end{equation}
In the general case, this formula reads:
\begin{equation} \label{eq:colour-decomposition}
    \ampl{n}{(0)} = g^{n-2} \sum_{\sigma \in S_{n-1}} \tr\left[T^{a_1} T^{\sigma(a_2} \ldots T^{a_n)}\right] A_n^{(0)} \left[1\,\sigma(2\ldots n)\right] \, ,
\end{equation}
where the sum is over the set of all \textit{non-cyclic} permutations of $n-1$ particles. Similar colour decompositions can be derived for amplitudes involving quarks, as well as beyond tree-level~\cite{Dixon:1996wi}. At loop-level, the colour basis contains products of traces, in addition to single trace structures. 

The colour-ordered amplitudes are calculated by adding up the kinematic parts of all Feynman diagrams contributing to a given colour factor. They are gauge invariant and satisfy a number of important identities:
\begin{align}
    A_n[123\ldots n] &= A_n[23\ldots n\,1]\,, && \text{cyclicity} \\
    A_n[123\ldots n] &= (-1)^n A_n[n \ldots 231] \,, && \text{reflection} \\
    A_n[123 \ldots n] &+ A_n[213 \ldots n] +  A_n[231 \ldots n] +  \ldots + && A_n[23 \ldots 1\,n]  = 0 \,, \nonumber \\ 
    & && U(1) \text{ decoupling} \\
    A_n[1, {\alpha}, n, {\beta}] &= (-1)^{|\beta|} \sum_{\mathclap{\sigma \in OP(\{\alpha\} \cup \{\beta^T\})}} A_n[1, \sigma, n] \,, && \text{Kleiss-Kuiff relations}
\end{align}
\JK{fix alignment!!!} where the sum is over permutations in the joint set $\{\alpha\} \cup \{\beta^T\}$ such that the order within the individual sets is preserved, and $\{\beta^T\}$ is the reversal of the set $\{\beta\}$~\cite{Mangano:1990by, Kleiss:1989616, Bern:2008qj}. Crucially, these properties allow us to reduce the number of independent amplitudes that need to be computed. In fact, for $n$-gluon scattering, this number is just $(n-2)!$\,.

\section{Helicity amplitudes}
After colour decomposition, our $L$-loop scattering amplitude contains purely kinematic information. The kinematic part of the Feynman rules for external states carries information about the spins and polarisations of particles: for massless spin\=/1/2 fermions, we use $\pm$ helicity states to differentiate between the two solutions to the Dirac equation, while for massless spin\=/1 bosons, they denote the two polarisation vectors. From the experimental perspective, we are rarely interested in differentiating between the spin states of individual particles. Usually, a beam of particles with random spins undergoes scattering and we look at the total number of particles outgoing in a certain direction. Thus, to calculate the corresponding cross-section, we should average over the initial spin states and sum over the final ones. This can be achieved in two different ways: 
\begin{enumerate}
    \item perform the amplitude calculation without specifying the helicity states, i.e. square the amplitude, then do the spin sums that appear at the level of $|\ampl{}{}|^2$ using completeness relations Eqs.~\ref{eq:completeness:fermionsm, eq:completeness:bosons}
    \item specify the helicity states of external particles, compute each \textbf{helicity amplitude} separately, square them and sum over all relevant helicity configurations
\end{enumerate}
In our computations, we will adopt the latter method. We denote an $L$-loop helicity amplitude as:
\begin{equation} \label{eq:helampdef}
    \ampl{n}{(L),\,\{h\}} \equiv \ampl{n}{(L)}(1^{h_1}2^{h_2} \ldots n^{h_n})\,,
\end{equation}
where the superscript $\{h\}$ is understood as the set of helicities of the $n$ particles, but will be usually omitted since we will exclusively compute helicity amplitudes. The full, spin-summed amplitude can then be recovered through:
\begin{equation}
    \ampl{n}{(L)} = \sum_{\mathclap{\substack{\text{helicity} \\ \text{configurations}}}} \ampl{n}{(L),\,\{h\}} \,.
\end{equation}
At the cross-section level:
\begin{equation}
    \left|\ampl{n}{(L)}\right|^2 = \sum_{\mathclap{\substack{\text{helicity} \\ \text{configurations}}}} \left|\ampl{n}{(L),\,\{h\}} \right|^2 \,.
\end{equation}
It is important to note that the sum only includes the squares of individual helicity amplitudes --- there are no interferences between different helicity configurations.

There are several strong advantages to this approach. Firstly, it is easy to see that the number of terms that need to be processed is significantly smaller than in method~(1) \JK{check with SB if the argument below even makes sense...}. Consider the expansion of an amplitude in the coupling constant $\alpha$ up to NNLO:
\begin{equation}
    \ampl{}{} = \ampl{}{(0)} + \alpha \ampl{}{(1)} + \alpha^2 \ampl{}{(2)} + \order{\alpha^3} \,.
\end{equation}
Then, at the level of the cross-section, we have the following contributions:
\begin{equation}
    |\ampl{}{}|^2 = |\ampl{}{(0)}|^2 \,+\,\alpha\,2\,\mathrm{Re}\left(\ampl{}{(0)\ast}\ampl{}{(1)}\right) \,+\, \alpha^2 \left(2\,\mathrm{Re} \left(\ampl{}{(0)\ast}\ampl{}{(2)}\right) + |\ampl{}{(1)}|^2 \right) \,+\, \order{\alpha^3}\,.
\end{equation}
Let us also schematically write each $L$-loop amplitude as a sum of $m_L$ Feynman diagrams: $\ampl{}{(L)} = d^{(L)}_1 + d^{(L)}_2 + \ldots + d^{(L)}_{m_L}$. Then, at LO:
\begin{equation}
    \left|\ampl{n}{(0)}\right|^2 = \sum_{\mathclap{\substack{\text{hel.} \\ \text{confs.}}}} \left|\ampl{n}{(0),\,\{h\}} \right|^2 = \sum_{\mathclap{\substack{\text{hel.} \\ \text{confs.}}}} \left|d^{(0)\,,\{h\}}_1 + d^{(0)\,,\{h\}}_2 + \ldots + d^{(0)\,,\{h\}}_{m_0} \right|^2 \,.
\end{equation}
Each term in this sum has all helicities fixed and there are no spin sums to be performed (when evaluated at a chosen phase-space point, it is just a complex number). Thus, the number of terms we need to process at LO is $m_0 n_h$, where $n_h$ is the number of independent helicity configurations. On the other hand, according to method (1), we have:
\begin{equation}
    \left|\ampl{n}{(0)}\right|^2 = \left(d^{(0)}_1 + d^{(0)}_2 + \ldots + d^{(0)}_{m_0} \right) \left(d^{(0)\ast}_1 + d^{(0)\ast}_2 + \ldots + d^{(0)\ast}_{m_0} \right)\,,
\end{equation}
which means we need to interfere the diagrams with each other and perform the spin sums. Thus, there are $m_0^2$ terms to be processed. The scaling for higher loop orders is listed in Table~\ref{tab:nterms}. For small $L$, the advantage of using helicity amplitudes might be minimal (or in fact, it might be detrimental to do so). For $L\geq2$, however, the advantage becomes apparent, especially that due to the symmetries of colour-ordered amplitudes, $n_h$ is usually much smaller than $2^n$. Moreover, it turns out that not all helicity amplitudes are equally challenging to compute, as we will see in the next section. In fact, a host of them vanishes altogether (at least at tree-level). Finally, guided by experience, it is possible to choose the reference vectors of external polarisations such that the computation of non-zero amplitudes becomes easier. 

When dealing with helicity amplitudes, it is conventional to introduce nomenclature encoding the number of positive/negative helicity particles involved in the process. Consider $2\rightarrow n$ gluon scattering where the outgoing momenta all have opposite helicities to the incoming ones: $1^-2^- \rightarrow 3^+ \ldots n^+$. We call such a configuration `helicity violating'. We can cross particles 1 and 2 to the final state, which changes their helicities: $0\rightarrow 1^+2^+ \ldots n^+$. This corresponds to the `all-plus' amplitude $A_n(1^+2^+ \ldots n^+)$ with all momenta outgoing. In the next section, we will show that for gluon scattering at tree-level, this amplitude vanishes for all $n$. If we flip one helicity in the final state: $1^-2^- \rightarrow 3^- \ldots n^+$, this corresponds to $A_n(1^+2^+3^-4^+ \ldots n^+)$, which also turns out to vanish at tree-level. The first non-zero configuration is $1^-2^- \rightarrow 3^-4^- \ldots n^+$, which corresponds to: $A_n(1^+2^+3^-4^-5^+ \ldots n^+)$. For this reason, amplitudes with exactly two negative helicity particles are called \textbf{maximally helicity violating (MHV)}. Similarly, amplitudes with exactly two positive helicities are known as anti-MHV. Furthermore, configurations with $2+k$ negative/positive helicities are referred to as $\text{N}^k\text{MHV}/\text{anti-N}^k\text{MHV}$. Tree-level MHV amplitudes are remarkably simple, as we will demonstrate in the next section. 
\begin{table}[t]
	\begin{center}
		\begin{tabular}{c|c|c|c}
			  \# terms & LO & NLO & NNLO \\
			\hline
			Method (1) & $m_0^2$ & $m_0^2 + m_0 m_1$ & $m_0^2 + m_0 m_1 + m_1^2 + m_0 m_2$ \\
			Method (2) & $m_0 n_h$ & $(m_0+m_1)n_h$ & $(m_0+m_1+m_2)n_h$ \\
		\end{tabular}
	\end{center}
 \caption{Number of terms to be processed in the computation of the squared amplitude $\left|\ampl{}{}\right|^2$ \textit{up to and including} a given order in the coupling constant, assuming there are $m_L$ diagrams at $L$ loops. The meaning of methods (1) and (2) is outlined in the text around Eq.~\ref{eq:helampdef}.}
 \label{tab:nterms}
\end{table}
\section{Spinor helicity formalism} \label{sec:spinhelform}
In Section~\ref{sec:QEDintro}, we saw that for massless particles, the Dirac spinor splits into two Weyl spinors that do not mix and are associated with the helicity of the particle. Therefore, we might be tempted to think that helicity amplitudes are better described using a notation specific to the two-component Weyl spinors, which are acted on by the familiar Pauli matrices. Indeed, the powerful \textbf{spinor-helicity formalism} provides a neat way to express helicity amplitudes based on these considerations\footnote{In case the notation that follows appears daunting, we refer the reader to Ref.~\cite{ElvangHuang} for an in-depth discussion of the topic and useful exercises.}.

As a first step, let's see how we can move from working with the four-component objects to two-component ones. We can write a `slashed' momentum $\slashed{p}$ as:
\begin{equation} \label{eq:pslashed1}
    \slashed{p} = p_\mu \gamma^\mu = p_\mu
    \begin{pmatrix}
    0 & (\sigma^\mu)_{a\dot{b}} \\
    (\bar{\sigma}^\mu)^{\dot{a}b} & 0
    \end{pmatrix}
    \equiv
    \begin{pmatrix}
    0 & p_{a\dot{b}} \\
    p^{\dot{a}b} & 0 
    \end{pmatrix} \, ,
\end{equation}
with both dotted and un-dotted indices running over $\{1,2\}$ and $(\sigma^\mu)_{a\dot{b}} \equiv (1, \, \sigma^i), \, (\bar{\sigma}^\mu)^{\dot{a}b} \equiv (1, \, -\sigma^i)$, where $\sigma^i$ are the three Pauli matrices. The momentum bispinors $p^{\dot{a}b}$ and $p_{a\dot{b}}$ can be thought of as $(2 \times 2)$ matrices and it is straightforward to show that:
\begin{equation}
    \det p_{a\dot{b}} = \det p^{\dot{a}b} = m^2\,.
\end{equation}
For massless particles, this determinant vanishes and the matrix can be expressed as an outer product of two vectors\footnote{The determinant of a matrix is 0 only if its column/row vectors are linearly dependent, which implies that its rank is 1. A rank-1, $(n \times n)$ matrix can always be expressed as the outer product of two nonzero vectors of length $n$.}. The vectors we will choose are the momentum space Weyl spinors $\lambda_a$  and $\tilde{\lambda}_{\dot{a}}$ (sometimes referred to as helicity spinors). They are the two-component, left- and right-handed equivalents of the $u(p)$ and $v(p)$ Dirac spinors, with the corresponding helicities $-$ and $+$, respectively. We thus write:
\begin{align} \label{eq:outerproduct}
    p_{a\dot{b}} = \lambda_a \tilde{\lambda}_{\dot{b}} && p^{\dot{a}b} = \tilde{\lambda}^{\dot{a}} \lambda^b\,,
\end{align}
and the raising and lowering of indices is achieved through:
\begin{align}
    \lambda^a = \varepsilon^{ab} \lambda_b && \tilde{\lambda}^{\dot{a}} = \varepsilon^{\dot{a}\dot{b}} \lambda_{\dot{b}} \,,
\end{align}
with the two-dimensional Levi-Civita tensor defined as:
\begin{equation}
    \varepsilon^{ab} = \varepsilon^{\dot{a}\dot{b}} = -\varepsilon_{ab} = -\varepsilon_{\dot{a}\dot{b}} = 
    \begin{pmatrix}
        0 & 1 \\
        -1 & 0
    \end{pmatrix}\,.
\end{equation}
In practice, it can be rather cumbersome to keep track of the dotted and un-dotted indices, as well as their lower or upper positions at the spinors. It is more intuitive to trade this notation for spinor brackets\footnote{The choice of assignment of dotted/un-dotted indices and angle/square brackets to either $\lambda$ or $\tilde{\lambda}$ is arbitrary. Different conventions are seen throughout literature - the only requirement is internal consistency.}:
\begin{align} \label{eq:lambdatobraket}
    \lambda_a  &\rightarrow \ketsq{p}_a,  &&\tilde{\lambda}_{\dot{a}} \rightarrow \bra{p}_{\dot{a}}\,, \nonumber \\
    \lambda^a  &\rightarrow \brasq{p}^a  &&\tilde{\lambda}^{\dot{a}} \rightarrow \ket{p}^{\dot{a}} \,.
\end{align}
We can then write Eq.~\ref{eq:pslashed1} as:
\begin{align} \label{eq:pslashed2}
    \slashed{p} = \ket{p}\brasq{p} && \slashed{p} = \ketsq{p} \bra{p} \,,
\end{align}
while the massless Dirac equation becomes the massless Weyl equation:
\begin{align} \label{eq:weyleq}
    \slashed{p}\ket{p} = 0 && \slashed{p}\ket{p} = 0\,.
\end{align}
In the above, $\slashed{p}$ is a small abuse of the `slashed' notation --- what it really means is a contraction of $p_\mu$ with $\sigma^\mu$ or $\bar{\sigma}^\mu$, rather than $\gamma^\mu$. The appropriate Lorentz vector can be chosen by looking at the indices of the square/angle spinors that $p^\mu$ is sandwiched between. However, the power of spinor-helicity formalism lies in the fact that in practice, we do not ever need to perform such explicit summation over indices and instead work with identities at the level of angle and square brackets:
\begin{align}
    \braket{ij} \equiv \bra{i}_{\dot{a}} \ket{j}^{\dot{a}}&& \braketsq{ij} \equiv \brasq{i}^a \ketsq{j}_a \,.     
\end{align}
It is straightforward to show that because the indices are raised and lowered using the Levi-Civita symbol, these brackets must be \textit{antisymmetric}:
\begin{align}
    \braket{ij} = -\braket{ji} && \braketsq{ij} = -\braketsq{ji}\,.
\end{align}
We can also formulate angle-square or square-angle brackets as follows:
\begin{align}
    \langle ikj ] \equiv \bra{i} \slashed{k} \ketsq{j} && [ikj\rangle \equiv \brasq{i} \slashed{k} \ket{j}\,.
\end{align}
To choose the right $\slashed{k}$ from Eq.~\ref{eq:pslashed2}, we just need to remember that $\langle ik] = [ik\rangle = 0$. These brackets can be extended to arbitrary lengths by inserting additional slashed momenta inside. 

We note some very useful identities\footnote{Naturally, the Schouten, Gordon and Fierz identities also hold if we exchange all angle and square brackets. Wherever $\gamma^\mu$ appears, it is understood as either $\sigma^\mu$ or $\bar{\sigma}^\mu$, as explained earlier.}:
\begin{subequations} \label{eq:spinorsids}
    \begin{align}
        \braket{ii} = \braketsq{ii} = 0 && \text{by antisymmetry} \label{eq:iieq0} \\
        s_{ij} = -\braket{ij}\braketsq{ij} && \text{(for massless momenta)} \\
        \braketsq{ij}^{\ast} = \braket{ji} && \text{(for real momenta)} \label{eq:complexconjbracket} \\
        \sum_{i=1}^n \ketsq{i}\bra{i} = \sum_{i=1}^n \ket{i}\brasq{i} = 0 && \text{momentum conservation} \label{eq:spinmomcons}\\
        \ket{i}\braket{jk} + \ket{j}\braket{ki} + \ket{k}\braket{ij} = 0 && \text{Schouten identity} \label{eq:schouten} \\
        \bra{i}\gamma^\mu \ketsq{i} = 2p_i^\mu && \text{Gordon identity} \label{eq:gordon} \\
        \bra{i} \gamma^\mu \ketsq{j} = \brasq{j}\gamma^\mu \ket{i} && \\
        \bra{i}\gamma^\mu \ketsq{j}^{\ast} = \bra{j}\gamma^\mu \ketsq{i} && \text{(for real momenta)} \\
        \bra{i}\gamma^\mu \ketsq{j} \bra{k}\gamma_\mu \ketsq{l} = -2\braket{ik}\braketsq{jl} && \text{Fierz identity} \label{eq:fierz} \,.
    \end{align}

\end{subequations}

In addition to massless fermions, we need to be able to write the polarisation vectors of massless spin\=/1 bosons in the spinor-helicity language. In analogy to Eq.~\ref{eq:outerproduct}, the Coulomb gauge identity $\varepsilon_{\pm} \cdot \varepsilon_{\pm} = 0$ \JK{Is this only true in Coulomb gauge?} allows us to decompose $\varepsilon^\mu$ into an outer product of two vectors. To this end, we introduce a reference vector $q^\mu$ and write \JK{double check the signs}:
\begin{align} \label{eq:pols}
    \varepsilon^\mu_{-}(p, q) = \frac{\bra{p}\gamma^\mu\ketsq{q}}{\sqrt{2}\braketsq{pq}} && \varepsilon^\mu_{+}(p, q) = \frac{\bra{q}\gamma^\mu\ketsq{p}}{\sqrt{2}\braket{qp}} \,.
\end{align}
The choice of the reference vector is arbitrary (apart from the condition $q^\mu \neq p^\mu$), which reflects gauge invariance. We can see this by noting that the Weyl spinors are two-component object and can be decomposed as $\ket{r} = \frac{\braket{rq}}{\braket{pq}}\ket{p} - \frac{\braket{rp}}{\braket{pq}}\ket{q}$. Therefore, any shift in $q$ must be of the form $q \rightarrow Aq + Bp$, where $A, B$ are constants. Then, from Eq.~\ref{eq:pols} (and using Eq.~\ref{eq:gordon}), it is easy to see that this shift will correspond to $\varepsilon^\mu \rightarrow \varepsilon^\mu + Cp^\mu$. Thus, the Ward identity $p_\mu \ampl{}{^\mu}$ is satisfied for any choice of the reference vector $q^\mu$. In practice, it is useful to choose it such that contracting $\varepsilon^\mu$ with external momenta leads to the formation of vanishing spinor brackets, thus greatly simplifying the algebra. Finally, the validity of Eq.~\ref{eq:pols} can be verified by observing that these expressions obey all identities we would expect from a polarisation vector \JK{I don't like this}:
\begin{subequations}
    \begin{align}
        p \cdot \varepsilon_\pm(p, q) &= 0 \\
        q \cdot \varepsilon_\pm(p, q) &= 0 \\
        \varepsilon_\pm (p, q) \cdot \varepsilon_\pm(p', q) &= 0 \\
        \varepsilon_\pm (p, q) \cdot \varepsilon_\mp(p', p) &= 0 \\
        \varepsilon_\pm^\ast (p, q) \cdot \varepsilon_\pm(p, q) &= -1
    \end{align}
\end{subequations}
At this point, we have all the tools we need to express any amplitude of massless fermions and spin\=/1 bosons through the angle and square brackets. Its usefulness, however, may not be immediately clear, especially given the notation which at first appears daunting. As a quick demonstration of the power of spinor-helicity formalism, let us consider the special case of 3-particle kinematics. For any three massless momenta satisfying $p_1^\mu + p_2^\mu + p_3^\mu = 0$, we have:
\begin{equation}
    \braket{12}\braketsq{21} = s_{12} = p_3^2 = 0\,.
\end{equation}
Thus, either $\braket{12} = 0$ or $\braketsq{12} = 0$. If we assume $\braket{12}$ is non-vanishing, then by momentum conservation and the massless Weyl equation:
\begin{equation}
    \braket{12}\braketsq{23} = \langle 1|\slashed{2}\ketsq{3} = -\langle 1|(\slashed{1}+\slashed{3})\ketsq{3} = 0\,.
\end{equation}
Thus, $\braketsq{23} = 0$ and in an analogous manner, we can show that $\braketsq{13} = 0$ as well. Had we assumed $\braketsq{12} \neq 0$, we would have found that all angle brackets vanish instead:
\begin{equation}
    \braketsq{12} = \braketsq{13} = \braketsq{23} = 0 \qquad \text{or} \qquad \braket{12} = \braket{13} = \braket{23} = 0 \,.
\end{equation}
Therefore, for 3-particle kinematics, the amplitude must depend on either square or angle brackets only. However, note that this result makes sense only if we work with complex momenta. Otherwise, through Eq.~\ref{eq:complexconjbracket}, the angle and square brackets are complex conjugates of each other and so both types must vanish simultaneously. Amplitudes constructed from complex momenta are of course not physical, nonetheless they provide a useful building block for higher-point amplitudes in recursive techniques \JK{reference to BCFW? Maybe I should have a mini-section on BCFW when talking about the tree amps for $\ppbbh$}.
\subsection{Little group scaling} \label{sec:littlegroup}
In the previous section, we saw that the freedom in choosing the reference vector $q^\mu$ reflects gauge invariance of the amplitude. Here we will see how another physical principle places strong constrains on the form of helicity amplitudes. We begin by observing that when trading the Weyl spinors for the bracket notation, there is some freedom in how exactly we write down Eq.~(\ref{eq:lambdatobraket}). That is, note that both $p_{a\dot{b}} = \ketsq{p}_a \bra{p}_{\dot{b}}$ and $p^{\dot{a}b} = \ket{p}^{\dot{a}} \brasq{p}^b$ are invariant under the transformation:
\begin{align} \label{eq:littlegroupscaling}
    \ket{i} \rightarrow z \ket{i} && \ketsq{i} \rightarrow z^{-1} \ketsq{i}, \qquad z \in \mathbb{C}  
\end{align}
This is known as \textbf{little group scaling}. Each external momentum has its own little-group transformation. This implies the following relations for spin-$1$ massless boson polarisations in Eq.~(\ref{eq:pols}):
\begin{align}
    \varepsilon^\mu_- (p,q) \rightarrow z^2 \varepsilon^\mu_- (p,q) &&  \varepsilon^\mu_+ (p,q) \rightarrow z^{-2} \varepsilon^\mu_+ (p,q) \,.
\end{align}
Note that the polarisation vectors are invariant under re-scalings of the reference momenta. Thus, for scattering of massless particles, the little-group scaling of the corresponding amplitude is determined by the helicities of the external particles. Specifically, if we re-scale the spinor brackets associated with the momentum $p_i$, the amplitude scales according to:
\begin{equation}
    A_n \left(\ldots, \{p_i, h_i\}, \ldots \right) \xrightarrow[\ketsq{i} \rightarrow z_i^{-1}\ketsq{i}]{\ket{i} \rightarrow z_i\ket{i}} z_i^{-2h_i}  A_n \left(\ldots, \{p_i, h_i\}, \ldots \right),
\end{equation}
where $h_i=\pm\frac{1}{2}$ for fermions and $h_i=\pm1$ for massless spin\=/1 bosons. It turns out that this property places a strong constraint on the spinor bracket expression of the amplitude. We will consider tree-level 3-gluon scattering as a basic example. We have already seen that for 3-particle kinematics, the amplitude must be written in terms of angle \textit{or} square brackets only, but we do not know the general form of the expression. Let us re-scale all three momenta with separate shifts $z_1, z_2, z_3$ in an MHV configuration. Then:
\begin{equation}
    A_3^{(0)} (1^-2^-3^+) \rightarrow z_1^2 z_2^2 z_3^{-2} A_3^{(0)}(1^-2^-3^+) \,.
\end{equation}
Assuming that this amplitude depends only on angle brackets:
\begin{equation} \label{eq:3gscaling}
    A_3^{(0)} (1^-2^-3^+) \propto \braket{12}^{x_{12}} \braket{13}^{x_{13}} \braket{23}^{x_{23}} \,,
\end{equation}
Using Eq.~\ref{eq:littlegroupscaling}, we solve for the exponents and get $\{x_{12} = 3, x_{13} = -1, x_{23} = -1\}$. An identical exercise can be performed (assuming square brackets this type) for the anti-MHV amplitude $A_3^{(0)} (1^+2^+3^-)$, leading to an analogous result. Therefore, the 3-point gluon amplitudes are fixed (up to an overall constant) by the special 3-point kinematics and little group scaling:
\begin{align} \label{eq:3gMHV}
    A_3^{(0)} (1^-2^-3^+) = \kappa_1 \frac{\braket{12}^3}{\braket{23}\braket{31}} && A_3^{(0)} (1^+2^+3^-) = \kappa_2 \frac{\braketsq{12}^3}{\braketsq{23}\braketsq{31}} \,.
\end{align}
We also note that flipping all helicities corresponds to exchanging $\braket{\phantom{p}} \leftrightarrow \braketsq{\phantom{p}}$. With a wrong choice of the bracket type in Eq.~\ref{eq:3gscaling}, one can show by considering the mass dimension of the amplitude that the couplings $\kappa_1$ and $\kappa_2$ would have to come from terms in the Lagrangian that are non-local\footnote{The mass dimension of $\ampl{n}{}$ in $D=4$ is $4-n$. From Eq.~\ref{eq:pslashed2}, we see that both $\braket{\phantom{i}}$ and $\braketsq$ must have mass dimension 1. Thus, the constants $\kappa_1$ and $\kappa_2$ in Eq.~\ref{eq:3gMHV} have mass dimension 0, which is consistent with the fact that they must have come from the 3-gluon interaction term in the Lagrangian (Eq.~\ref{eq:QCDLagrangian}), \textasciitilde$A^\mu A_\mu \partial^\nu A_\nu$. Had we assumed incorrect bracket types, both constants would need to have dimension 2. Thus, the corresponding term in the Lagrangian would be \textasciitilde$A^\mu A_\mu \frac{\partial^\nu}{\square} A_\nu$, which is non-local (it describes an interaction whose effects become more important with distance).}. We thus reject them as unphysical. Moreover, the same argument can be used to show that the $(---)$ and $(+++)$ configurations cannot have a non-vanishing amplitude:
\begin{align}
    A_3^{(0)} (1^-2^-3^-) = 0 && A_3^{(0)} (1^+2^+3^+) = 0\,.
\end{align}
In fact, the simplicity we have seen so far generalises to higher-point gluon amplitudes. With a smart choice of reference vectors, it can be shown that:
\begin{align}
    A_n^{(0)} (1^-2^- \ldots n^-) = 0 && A_n^{(0)} (1^+2^+ \ldots n^+) = 0\,,
\end{align}
as well as:
\begin{align}
    A_n^{(0)} (1^+2^- \ldots n^-) = 0 && A_n^{(0)} (1^-2^+ \ldots n^+) = 0\,.
\end{align}
The first non-vanishing amplitudes are the MHV/anti-MHV configurations:
\begin{align} \label{eq:ParkeTaylor}
    A_n^{(0)} (1^+2^+\ldots i^- \ldots j^- \ldots n^+) &= \frac{\braket{ij}^4}{\braket{12} \braket{23} \ldots \braket{n1}} \,, \\
    A_n^{(0)} (1^-2^-\ldots i^+ \ldots j^+ \ldots n^-) &= \frac{\braketsq{ij}^4}{\braketsq{12} \braketsq{23} \ldots \braketsq{n1}} \,.
\end{align}
This result is known as the Parke-Taylor formula \cite{Mangano:1990by}. It can be proved inductively using the BCFW recursion relations~\cite{Britto:2004ap, Britto:2005fq}, with the 3-point MHV amplitudes of Eq.~\ref{eq:3gMHV} serving as the starting point. Overall, it is now clear that the spinor-helicity formalism, together with little group scaling and locality of the Lagrangian, produce astonishingly compact results for amplitudes which are traditionally calculated as sums of hundreds or thousands of Feynman diagrams.
\section{Momentum twistors} \label{sec:MTs}
In the previous section, we have seen that the spinor-helicity formalism provides a convenient framework to describe helicity amplitudes. By using spinor brackets, which are intrinsically tied to helicity, we were able to exploit properties of the amplitude to arrive at remarkably neat expressions. Nonetheless, this formalism comes with certain drawbacks. Firstly, note that kinematic identities such as momentum conservation are not automatically satisfied by the spinor brackets. We can in fact use the properties listed in Eq.~\ref{eq:spinorsids} to arrive at a minimal set of these variables. In practice, however, this proves to be cumbersome, especially for high-multiplicity processes. Moreover, the appearance of square roots such as~\ref{eq:delta3} and \ref{eq:delta5} complicates our computational setup, which will be made clear in the next section. We would therefore like to have a parametrisation of external kinematics which solves both these problems simultaneously.

In recent years, many amplitude computations have exploited variables known as \textbf{momentum twistors} (MTs)~\cite{Hodges:2009hk, Badger:2013gxa, Badger:2016uuq}. As the first step, we define dual-space coordinates $x_i^\mu$ as:
\begin{equation} \label{eq:dualspacedef}
    p_i^\mu = x_i^\mu - x_{i+1}^\mu\,.
\end{equation}
Using the massless Weyl equation (Eq.~\ref{eq:weyleq}) and the slash notation in the sense of Eq.~\ref{eq:pslashed1}, it then follows that:
\begin{equation} \label{eq:incidencerel}
    \brasq{\mu_i} \equiv \bra{i}\slashed{x}_i = \bra{i}\slashed{x}_{i+1}\,,
\end{equation}
The new variables allow us to define the momentum twistors $Z_i^I$:
\begin{equation} \label{eq:momtwistor}
    Z_i^I = 
    \begin{pmatrix}
        \ket{i} \\
        \brasq{\mu_i} 
    \end{pmatrix}\,.
\end{equation}
where the index $I$ is understood as $I=\{\dot{a},a\}$. Since $\slashed{p} = \ket{i}\brasq{i}$, we can also express $\brasq{i}$ in terms of $Z_i^I$. To this end, the dual twistor is defined as:
\begin{equation} \label{eq:dualmomtwistor}
    W_i^I = 
    \begin{pmatrix}
        \ket{\mu_i} \\
        \brasq{i}
    \end{pmatrix} = 
    \frac{\varepsilon^{ABCD} Z_{(i-1)B} Z_{iC} Z_{(i+1)D}}{\braket{i-1, i} \braket{i, i+1}}\,,
\end{equation}
where $\varepsilon^{ABCD}$ is the 4-dimensional Levi\=/Civita symbol. We can then expand this equation and read off the last two components:
\begin{equation}
    \brasq{i} = \frac{\braket{i, i+1}\brasq{\mu_{i-1}} + \braket{i+1, i-1}\brasq{\mu_i} + \braket{i-1, i}\brasq{\mu_{i+1}} }{\braket{i-1, i}\braket{i, i+1}}\,.
\end{equation}
Each momentum twistor $Z_i^I$ has four components, thus the matrix of all twistors has $4n$ entries for $n$\=/particle scattering. However, not all of them are independent. Firstly, momentum twistors are invariant under the 10-dimensional Poincar√© group. Additionally, they exhibit the $U(1)$ symmetry as well, for each particle separately. We can see this from Eq.~\ref{eq:incidencerel}, which implies that under the little group scaling of $\ket{i}\rightarrow t_i\ket{i}$, with $t_i \in \mathbb{C}$, the momentum twistors scale as: $Z_i \rightarrow t_i Z_i$. At the same time, this transformation does not affect the underlying momentum $p_i^\mu$, thus $Z_i$ are defined projectively. Therefore, the number of independent \textbf{momentum-twistor variables} needed to generate the $Z_i$ for $n$\=/particles is\footnote{In amplitude jargon, the term `momentum twistors' most often refers to these $3n-10$ independent variables, rather than the $Z_i$'s themselves. We will also adopt this terminology in subsequent sections.}: $4n-10-n\times1 = 3n-10$. \JK{Make sure to understand the $3n-10$ thing.} \JK{I forgot what it means to be defined projectively.}.

The momentum twistors $Z_i$, together with their dual equivalent $W_i$, serve as a useful way to generate numerical phase-space points according to the following recipe:
\begin{enumerate}
    \item Fill the twistor matrix $Z_i^I, i=1, \ldots, n$ with random integers.
    \item Compute the dual twistor matrix $W_i^I$.
    \item Read off spinors $\ket{i}$ and $\brasq{i}$.
    \item Calculate the momenta according to the Gordon identity, $p^\mu = \frac{1}{2}\brasq{i}\gamma^\mu |i\rangle$.
\end{enumerate}
Phase-space points generated in this manner are complex and rational. We can also populate the twistor matrix with rational functions instead. The corresponding phase-space parametrisation is guaranteed to automatically implement momentum conservation and the Schouten identity. It is also possible to make a specific choice which leads to $\tr_5$ being rational, which is a trick that will be useful in the next section\JK{Why do MTs rationalise $\tr_5$?}. It is not clear, however, how to choose these functions such that the corresponding parametrisation leads to the simplest possible amplitude expressions \JK{Mention how the MTs break symmetries of helicity amplitudes}. We will present two judicious \JK{Maybe `judicious' is too much?...} choices in Sections~\ref{sec:Hbb} and \ref{sec:Wyj}\JK{Why do we use different MT parametrisations in these papers?}.

A small drawback of using the momentum-twistor variables is that they lose the phase information carried by the spinor brackets $\ket{i}$ and $\brasq{i}$. This is because in reducing the number of independent variables from $4n$ to $3n-10$, imposing the symmetries essentially fixes the frame in which we evaluate the kinematics. Thus, strictly speaking, only phase-free expressions can be obtained in MT-variables. On the other hand, each helicity amplitude is a `phase-full' quantity (as opposed to the squared, spin-summed amplitude), therefore we need to restore this information at the end of our computation. This can be achieved by multiplying our MT-variable expression by any factor with the same phase content, normalised such that its magnitude is 1. In practice, we most often choose the spinor-bracket expression of the tree-level amplitude for the corresponding helicity, and divide it by its MT-variable expression.
\section{Finite fields} \label{sec:FF}
In the previous section, we have introduced a new, minimal set of independent variables that automatically implement constraints such as momentum conservation, as well as rationalise all square roots that appear in our kinematics. This is not just an elegant mathematical exercise. It turns out that momentum-twistor variables (henceforth referred to as just momentum twistors, MTs) provide us with a powerful computational framework that goes hand in hand with yet another tool we employ in amplitude computations.
\subsection{Rational numbers} \label{sec:ratnums}
The problem of enormous algebraic expressions plagues almost every calculation in QFT. At the same time, sweeping cancellations often occur, leading to much more compact answers. Indeed, we have already seen how at tree-level the MHV gluon amplitudes can be described by remarkably simple expressions, despite the fact that they come from hundreds, if not thousands, of Feynman diagrams. A key idea that has emerged over the past several years is to avoid this complexity at the intermediate stages of the computation by working with numerical expressions instead~\cite{Peraro:2016wsq, Peraro:2019svx, Klappert:2019emp, Klappert:2020aqs, vonManteuffel:2014ixa, Abreu:2020xvt}. Crucially, the analytic dependence can still be recovered from the numerics at the very end. In this section, we introduce the concept of \textbf{finite fields} and show how it can be used to our advantage.

A finite field is a field with a finite number of elements. We are interested in finite fields of non-negative integers:
\begin{equation} \label{eq:ffdefinition}
    \mathbb{Z}_n = \{0, \ldots, n-1\}\,,    
\end{equation}
where $n$ is referred to as the size of the field. In particular, we will work with fields whose size is a large prime number $p$, as prime fields satisfy many properties which make the corresponding arithmetic especially simple. Basic operations, such as addition, subtraction and multiplication, are defined over $\mathbb{Z}_p$ through the standard modular arithmetic $\text{mod } p$. We can also define a multiplicative inverse $b\in \mathbb{Z}_p$ for all $a\neq0 \in \mathbb{Z}_p$:
\begin{equation}
    a^{-1} \equiv b \mod p \qquad \Longleftrightarrow \qquad ab = 1 \mod p\,.
\end{equation}
In fact, the existence of the inverse for all non-zero $a$ is guaranteed only for prime fields. We can see this by considering the following set:
\begin{equation}
    S = \{a, 2a, 3a, \ldots, (p-1)a\}\,.
\end{equation}
Now, note that for any two integers $x, x'$ such that: $x \neq x' \text{ mod } p$, we have:
\begin{equation}
    a(x-x') \neq 0 \mod p \qquad \qquad (a \neq 0) \,.
\end{equation}
This inequality, however, holds only because $\gcd(a, p) = 1$. It follows that the set $S \text{ mod } n$ contains all unique, non-zero elements of $\mathbb{Z}_p$, one of which must be 1. This proves the existence of the multiplicative inverse for all $a \neq 0$ (it can be calculated using the \textit{extended Euclidean algorithm}). Consequently, we can conclude that rational operations over $\mathbb{Z}_p$ are well-defined. Moreover, it allows us to define a map from rational numbers to the prime field, $\mathbb{Q} \rightarrow \mathbb{Z}_p\,$. For $q=\frac{x}{y} \in \mathbb{Q}\,$: 
\begin{equation}
    q \text{ mod } p = \left(x \times (y^{-1} \text{ mod } p \right) \text{ mod } p\,.
\end{equation}
This map is not invertible, since it maps infinitely many elements of $\mathbb{Q}$ onto the finite set $\mathbb{Z}_p$. Nonetheless, the rational numbers $q$ can be recovered from their image in $\mathbb{Z}_p$ with a very high probability using \textit{Wang's algorithm} \cite{10.1145/800206.806398, 10.1145/1089292.1089293}. This process is referred to as \textbf{rational reconstruction}. We remark that this algorithm is successful if  $|x|, |y| < \sqrt{p/2}\,$. Therefore, $p$ should be chosen sufficiently large so that it is possible to reconstruct all rational numbers appearing in the problem. However, this defeats the purpose of using finite fields in the first place, which was to keep the size of numbers below a certain bound imposed by modular arithmetic $\text{mod } p\,$. Moreover, from the practical point of view, we want to use the efficiency of machine-size integers, which means we are usually constrained to $p<2^{64}$. Fortunately, rational numbers exceeding such thresholds can be reconstructed without using prohibitively large prime fields. Recall the essence of the \textit{Chinese remainder theorem}: knowledge of the congruences of an integer $x$ modulo $\{n_1, n_2, \ldots, n_k\}$, where all the $n_i$ are pairwise co-prime, allows us to obtain the congruence of $x$ modulo $n_1n_2\ldots n_k$. The same idea holds even for our map $\mathbb{Q} \rightarrow \mathbb{Z}_p$. Thus, by calculating:
\begin{align}
    q = a_{p_1} &\mod p_1 \nonumber \\
    q = a_{p_2} &\mod p_2 \nonumber \\
    &\vdots \nonumber \\
    q = a_{p_k} &\mod p_k\,,
\end{align}
we can obtain:
\begin{equation}
    q = a_{p_1 p_2 \ldots p_k} \mod (p_1 p_2 \ldots p_k)\,.
\end{equation}
Hence, combining the images of $q$ over several prime fields $\mathbb{Z}_{p_i}$ allows us to use Wang's algorithm on $\mathbb{Z}_{p_1 p_2 \ldots p_k}$ and successfully reconstruct $q$ in $\mathbb{Q}$.
\subsection{Rational functions} \label{sec:ratfuncs}
So far, we have seen how we can exploit finite fields to keep the size of numerical expressions from growing throughout our computation\footnote{An alternative approach would be to use floating-point numbers instead of rational numbers, however this would quickly lead to issues with precision.}. It should not come as a surprise that this concept can be extended to allow for the reconstruction of not only rational numbers, but also rational functions in multiple variables. 

Let us consider the so-called \textbf{black box interpolation problem}. Suppose we have a set of $n$ variables $\mathbf{x}=\{x_1, x_2, \ldots, x_n\}$. These variables will serve as the arguments of a rational function $f(\mathbf{x})$. In general, the analytic form of $f$ is obtained by applying a series of rational operations on $\mathbf{x}$ . We do not know $f$ analytically at any of these steps, however we assume that we have a way of implementing them \textit{numerically} --- this is what we call the `black box'. Specifically, the numerical operations will be done over a prime field $\mathbb{Z}_p$. We start by evaluating the variables $\mathbf{x}$ at random numerical values in $\mathbb{Z}_p$. \JK{are they actually random? Idk how FF chooses the sample points.}We then apply the rational operations represented by $f$, all within the same prime field. After passing through this black box, the result is a number within that field, which we denote as $f(\mathbf{x}) \text{ mod } p$. Therefore, we have obtained one \textbf{sample point} of the analytic result corresponding to the initial values we chose for $\mathbf{x}$\JK{Maybe a bit pedantic: does `sample point' refer to the initial values of $\mathbf{x}$ or to the corresponding numerical evaluations of $f(\mathbf{x})$?}.

They key idea of finite field methods is that it is possible to reconstruct the full analytic dependence of $f(\mathbf{x})$, with coefficients of $x_i$ in $\mathbb{Q}$, by sampling it in this manner at multiple points. The first step of this procedure is in essence a linear fit problem \JK{Check if that's the correct term.}. Any multivariate rational function can be written as:
\begin{equation} \label{eq:ratfun}
     R(\mathbf{x}) = \frac{
     \sum_{\bm{\alpha}} a_{\bm{\alpha}} \mathbf{x}^{\bm{\alpha}}
     }{
     \sum_{\bm{\beta}} b_{\bm{\beta}} \mathbf{x}^{\bm{\beta}}
     }\,.
\end{equation}
Here, $a_{\bm{\alpha}}, b_{\bm{\beta}} \in \mathbb{Z}_p$ are coefficients of the multivariate monomials $\mathbf{x}^\alpha$:
\begin{equation}
    \mathbf{x}^\alpha = \prod_{i=1}^n x_i^{\alpha_i}
\end{equation}
and $\bm{\alpha}$ denotes a collective set of exponents $\bm{\alpha} = \{\alpha_1, \alpha_2, \ldots, \alpha_n\}$. It is also useful to define the \textit{total degree} of the monomial as the sum of all its exponents:
\begin{equation}
    \deg (\mathbf{x}^{\bm{\alpha}}) \equiv |\bm{\alpha}| = \sum_{i=1}^n \alpha_i\,.
\end{equation}
In the context of a rational function, the total degree $\degmax(f)$ is understood as the maximal total degree of any of its monomials.

With this representation of $f(\mathbf{x})$ in Eq.~\ref{eq:ratfun}, we can try to reconstruct its analytic dependence from the numerical samples over the prime field $\mathbb{Z}_p$. In a very naive approach, we would construct the most general ansatz covering all possible monomials up to degree $\degmax (f)$. We make this explicit by writing the ansatz as:
\begin{equation}
    R(\mathbf{x}) = \frac{
    \sum\limits_{\bm{\alpha}:\, |\bm{\alpha}| \le u} a_{\bm{\alpha}} \mathbf{x}^{\bm{\alpha}}
    }{
    \sum\limits_{\bm{\beta}:\, |\bm{\beta}| \le v} b_{\bm{\beta}} \mathbf{x}^{\bm{\beta}}
    }\,,
\end{equation}
where we have abbreviated the numerator/denominator degrees as $u=\degmax(\text{num}(f))$ and $v=\degmax(\text{den}(f))$. We can then formulate a system of linear equations in $a_{\bm{\alpha}}, b_{\bm{\beta}}$ by evaluating both the monomials $\mathbf{x}^{\bm{\alpha}}$ in the ansatz and the black box function $f(\mathbf{x})$ at a chosen value $\mathbf{x}_j$ in $\mathbb{Z}_p$:
\begin{align}
    \sum_{\bm{\alpha}} a_{\bm{\alpha}} \mathbf{x}_j^{\bm{\alpha}} - 
    f(\mathbf{x}_j) \sum_{\bm{\beta}} b_{\bm{\beta}} \mathbf{x}_j^{\bm{\beta}} = 0 
&&    
j \in \{1, \ldots, |R(\mathbf{x})|\} \,.
\end{align}
Finding the values of these coefficients requires solving the system using linear algebra methods. In order for the system to close \JK{Is that the right terminology?}, we need to perform such evaluations on as many sample points as the number of ansatz terms. This is far from optimal, however, since such a generic ansatz grows rapidly with both the degree as well as the number of variables. In fact, one can show that the number of terms present in $R(\mathbf{x})$ is \JK{I took this from SAGEX lectures, not sure if I should do a reference.}:
\begin{equation} \label{eq:naiveansatzlength}
    |R(\mathbf{x})| = 
    \begin{pmatrix}
        u + n \\
        n
    \end{pmatrix}
    +
    \begin{pmatrix}
        v + n \\
        n
    \end{pmatrix}\,.
\end{equation}
Since the time complexity of the corresponding Gaussian elimination is $\order{|R(\mathbf{x})|^3}$, this can prove prohibitively expensive. As an example, in practice we will be dealing with cases such as six-variable functions with $u=30, v=10$, which gives $|R(\mathbf{x})| \approx 2\times 10^6$. Row reducing such a system is simply not feasible \JK{Is this true for us or in general? Maybe on some supercomputer it would be possible. Idk what the corresponding time estimate is.}. Another complication arises due to the fact that in general, even though the black box operations are implemented numerically over finite fields, obtaining each evaluation of $f(\mathbf{x})$ in the field $\mathbb{Z}_p$ might still take a long time due to the number and complexity of these operations. We refer to this as the \textbf{evaluation time per point}.

Overall, it is clear that we need to avoid using such a naive ansatz to interpolate a rational function from its evaluation over finite fields. Besides, in most applications the polynomial degrees $u$ and $v$ are not known \textit{a priori}, so it is difficult to construct an ansatz in the first place. Fortunately, we can make use of more elaborate interpolation methods\footnote{For a detailed description of these methods and their implementation, see~\cite{Peraro:2016wsq}.}. For univariate polynomials, the strategy is based on Newton's polynomial representation~\cite{Abramowitz1965HandbookOM}. This method is particularly useful in cases where the total degree is not known, as it allows for the inclusion of higher-degree terms until their coefficients are found to be 0, at which point the iterative procedure terminates. For univariate rational functions, we distinguish between two further cases based on whether the degrees $u$ and $v$ are known or not. If they are known, it turns out that the naive ansatzing described above performs well enough, as for $n=1$ the ansatz length $|R(x_1)|$ in Eq.~\ref{eq:naiveansatzlength} is sufficiently small to allow for efficient row reduction of the system. Finally, if the degrees are not known, the reconstruction strategy is based on a rational generalisation of Newton's formula known as Thiele's interpolation formula~\cite{Abramowitz1965HandbookOM}. 

Multivariate reconstruction from finite fields can be achieved as well. For multivariate polynomials, it is sufficient to apply the univariate Newton's formula recursively. That is, a multivariate polynomial $P(\mathbf{x})$ is first treated as a univariate polynomial in $x_1$ with coefficients that are polynomials in $x_2, x_3, \ldots, x_n$. These coefficients then become the subject of $(n-1)$-variable reconstruction and so forth, up until $n=1$. Reconstructing multivariate rational functions is significantly more complicated. The strategy is also based on a recursive use of Newton's formula, but with some important modifications such as adding an auxiliary variable to $\mathbf{x}$. We refer the reader to Refs.~\cite{CUYT20111445, Peraro:2016wsq} for details.

Having completed the interpolation through one of the methods above, the only thing left to do is to recover the monomial coefficients in $\mathbb{Q}$ from their images $a_{\bm{\alpha}}, b_{\bm{\beta}} \in \mathbb{Z}_p$ using Wang's algorithm. As explained earlier, if one prime field $\mathbb{Z}_{p_1}$ is not enough, we can always perform the interpolation in another field $\mathbb{Z}_{p_2}$ and combine these results with the Chinese remainder theorem to obtain the interpolation in $\mathbb{Z}_{p_1 p_2}$. In this way, a rational function with arbitrarily large coefficients can be recovered. The reconstruction time can be estimated according to:
\begin{equation} \label{eq:rectimeschematic}
    \text{Reconstruction time} \approx (\text{number of sample points}) \times (\text{evaluation time per point})\,.
\end{equation}
It is of great practical importance to reduce both these factors as much as possible, which renders the reconstruction of increasingly complicated rational functions possible. We will elaborate on this topic in Sections~\ref{sec:Hbb} and \ref{sec:Wyj}.

Let us make three final remarks. Firstly, we emphasise that the reconstructed function is minimal in terms of the numerator and denominator degrees, that is $\gcd \left(\text{num}(f), \text{den}(f) \right) = 1$. Note that this is also needed for the reconstruction ansatz to be unique. Secondly, for functions which are homogeneous, in the sense that they satisfy:
\begin{align}
    f(\lambda \mathbf{x}) = \lambda^u f(\mathbf{x}) && \lambda \in \mathbb{C} \,,
\end{align}
where $u=\deg (f)$, it is possible to reduce the number of variables in the problem by one. This is easy to see if we define an auxiliary function:
\begin{equation}
    \tilde{f}(x_2, \ldots, x_n) \equiv f(x_1=1, x_2, \ldots, x_n)\,.
\end{equation}
Then, requiring the correct scaling gives:
\begin{equation}
    f(x_1, \ldots, x_n) = x_1^u \, \tilde{f}\left(\frac{x_2}{x_1}, \ldots, \frac{x_n}{x_1}\right) \,.
\end{equation}
\JK{Check out Weinzierl's Feynman Integrals, Eq. 2.144}
Thus, we can reconstruct $\tilde{f}$ and restore the homogeneity of $f$ a posteriori. In our applications, even though the functions we will be dealing with are in general not homogeneous, it turns out we can still discard one variable. We will usually set $s_{12} = 1$ and recover its analytic dependence a posteriori through dimensional analysis \JK{I don't think I understand this.} \JK{Are these two methods of discarding a variable the same?}. Finally, recall from Sec.~\ref{sec:kinematics} the presence of square roots in the kinematics associated with the amplitude. This is a problem because it is not always possible to take a square root of a field element $a \in \mathbb{Z_p}$. Specifically, for a field of size $p>2$, there are only $(p+1)/2$ so-called quadratic residues, i.e. solutions to this equation $x^2=a \text{ mod } p$ \cite{hardy2008introduction}. This fact is easy to understand as we can actually enumerate all the residues. Note that this equation admits two solutions, since it is equivalent to $(x-b)(x+b)=0 \text{ mod } p \Longrightarrow x = \pm b \text{ mod } p$. Thus, the set of solutions $\{b_i\} \in \mathbb{Z}_p$ will lead to distinct quadratic residues only if no two $b_i$ are negatives of each other in the field. This constrains us to the first half of the elements, i.e. $\{0, 1, 2, \ldots, (p-1)/2 \}$. Indeed, if we try to add another solution, $(p+1)/2$, it would square to the same residue as $(p-1)/2$, because $b_i^2 = (p-b_i)^2 \text{ mod }p$
and we have precisely $p - (p+1)/2 = (p-1)/2 \text{ mod }p$. Thus, the full set of distinct residues is:
\begin{equation}
    \left\{0^2, 1^2, \ldots, \left(\frac{p-1}{2}\right)^2  \right\}\,.
\end{equation}
This fact implies that we can take the square root of a number almost exactly $50\%$ of the time. One approach to dealing with the other half would be to simply reject the points for which expressions in Eqs.~\ref{eq:delta3} and \ref{eq:delta5tr5} do not correspond to residues and repeat the black-box sampling procedure at another point\footnote{It is also possible to adjoin the needed square roots to the field, i.e. $\mathbb{Z}_p \rightarrow \mathbb{Z}_p \left( \sqrt{a} \right)$}. However, in practice we find it convenient to deal with the square roots in a different manner, which will be explained in detail in Sections~\ref{sec:Hbb} and \ref{sec:Wyj}.

Overall, we have seen that any algorithm which can be expressed as a chain of rational operations can be implemented over finite fields. This applies to both pure rational numbers, as well as analytic expressions in the form of rational functions. This turns out to be tremendously useful, since many of the steps required in amplitude computations are precisely such rational transformations. By exploiting the finite field methods, we can entirely sidestep the analytic complexity in the intermediate stages, yet still enjoy the \textit{exact} cancellations that occur, since we are not forced to resort to floating-point numbers. The often insurmountable task of computing a function analytically has been turned into the much simpler task of providing its fast numerical evaluation over finite fields. Armed with this knowledge, we can move on to the next steps in our procedure.

\section{Reduction onto scalar integrals} \label{sec:reduction}
Before we begin, let us briefly summarise what we have learnt so far about our workflow for computing scattering amplitudes (see Fig.~\ref{fig:outline}). We started by generating all Feynman diagrams that contribute to a desired loop amplitude. We then decomposed this amplitude in colour space and defined an new object, the colour-ordered amplitude, by considering only the diagrams which contribute to a particular colour factor. We then specified the helicities of external fermions and bosons in the so-called helicity amplitudes. We have learned that not all such amplitudes are equally challenging to compute (in fact, some will vanish or be free of divergences) and due to symmetries, we will not even have to compute all possible helicity configurations. We subsequently decided to employ a language which naturally captures the helicity information of the particles, that is the spinor-helicity formalism. However, we have also seen that it suffers from several drawbacks which can be remediated by one last variable change --- into momentum twistor variables. Not only does this parametrisation of external momenta automatically satisfy kinematic identities of Eq.~\ref{eq:spinorsids}, but it also allows us to rationalise some of the square roots we need to deal with. Last, but not least, we have also learnt that by performing numerical calculations over finite fields, we can bypass the analytic complexity which typically characterises QFT problems.

At this point, the task of computing a colour-ordered helicity amplitude for $n$-particle scattering amounts to computing loop integrals of the form:
\begin{equation} \label{eq:ampschematic}
    	A_n^{(L)} \left(1^{h_1}, 2^{h_2}, \ldots, n^{h_n} \right) =  \sum_{T\in \text{topologies}} \left[\, \prod_{l=1}^{L} \left(\int \mathrm{d}^d k_l \right) \frac{\sum_i c_i(p(\mathbf{x}), \varepsilon )\times m_i(k,p)}{\prod_{t\in T} D_t(k,p)} \right]\,.
\end{equation}
\JK{Should there be a factor of $(i \pi^{d/2})^L$?}\\
The general structure of this formula can be understood simply by considering the QCD Feynman rules and all the possible Lorentz contractions that follow from them. The main sum runs over distinct integral topologies, that is sets of inverse propagators $D_t$ associated with a given Feynman diagram (see Fig.~\ref{fig:topologies} for a few examples). Each inverse propagator depends on external momenta $p$, as well as one or more loop momenta $k$, which act as the integration variables for the $d$-dimensional integrals (see Sec.~\ref{sec:divergences}). In the numerator of each topology, we have a sum of monomials $m_i$ of \textit{both} loop and external momenta, multiplied by coefficients $c_i$ that depend on external momenta \textit{only}. The monomials are composed of scalar products $p_i \cdot p_i,\, k_i \cdot k_j,\, k_i \cdot p_j$; as well as the spinor brackets $\braket{ij},\, \braketsq{ij},\, \langle i |k_i \ketsq{j},\, \langle i |p_5 \ketsq{j},\, \bra{i}k_i p_5\ket{j},\, \brasq{i}k_i p_5 \ketsq{j}\,$. Similar objects appear in their coefficients, but because they do not contain any dependence on the loop momenta, we express the external kinematics in the coefficients in terms of the momentum twistor variables $\mathbf{x} = \{x_1, \ldots, x_{3n-10}\}$. This ensures the coefficients are rational, parametrised through a minimal set of variables and can be processed using the finite field framework. In fact, all the remaining steps in our workflow that deal with the computation of the coefficients will be implemented over finite fields. 
%Finally, we note that unless otherwise stated, in the following we will specialise to the case $L=2$ since we are interested in computing two-loop amplitudes.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \hspace*{1em}\raisebox{1.1cm}{ % needed to align the diagrams
        \begin{tikzpicture}
        	\begin{feynman}[small]
        		\vertex (v1);
          
        		\vertex[above left = 0.8cm of v1, yshift=-0.2cm] (i1);
        		\vertex[below left =  0.8cm of v1, yshift=+0.2cm] (i2);
        		\vertex[right = of v1] (v2);			
        		\vertex[right = of v2] (v3);			
        		
        		\vertex[above right = 0.8cm of v3, yshift=-0.2cm] (f1);
        		\vertex[right = 0.70cm of v3] (f2);
        		\vertex[below right = 0.8cm of v3, yshift=+0.2cm] (f3);

        		\diagram*{
        			(i1) -- (v1) -- [out=60, in=120] (v2) -- [out=60, in=120] (v3) -- (f1),
        			(i2) -- (v1) -- [out=-60, in=-120] (v2) -- [out=-60, in=-120] (v3) -- (f3),
                    (v3) -- (f2),
                
        		};
        	\end{feynman}
        \end{tikzpicture}}
    \caption{} \label{fig:topologya}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \hspace*{1em}
        \begin{tikzpicture}
    	\begin{feynman}[small]
    		\vertex (v1);
      
    		\vertex[above left = 0.8cm of v1, yshift=-0.2cm] (i1);
    		\vertex[below left =  0.8cm of v1, yshift=+0.2cm] (i2);
    		\vertex[above right = of v1] (v2);			
    		\vertex[below right = of v1] (v3);			
    		\vertex[right = of v2] (v4);			
    		\vertex[right = of v3] (v5);			
    		
    		\vertex[above right = 0.8cm of v4] (f1);
    		\vertex[below right = 0.5cm of v5, yshift=-0.4cm] (f2);
    		\vertex[below right = 0.5cm of v5, xshift=+0.4cm] (f3);
    		
    		\diagram*{
    			(i1) -- (v1) -- (v2) -- (v4) -- (f1),
    			(i2) -- (v1) -- (v3) -- (v5) -- (f2),
                (v2) -- (v3),
                (v4) -- (v5),
                (v5) -- (f3),
    		};
    	\end{feynman}
    \end{tikzpicture}
    \caption{} \label{fig:topologyb}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \hspace*{1em}
        \begin{tikzpicture}
    	\begin{feynman}[small]
    		\vertex (i1);
    		\vertex[below right= 0.8cm of i1] (v1);
    		\vertex[right = of v1] (v2);
    		\vertex[yshift=0.3cm, right = 0.8cm of v2] (v3);
    		\vertex[above right = 0.8cm of v3] (f1);			
    		
    		\vertex[below = of v1] (v7) ;
    		\vertex[below left = 0.8cm of v7] (i2) ;
    		\vertex[right = of v7] (v6) ;
    		\vertex[yshift=-0.3cm, right = 0.8cm of v6] (v5) ;
    		\vertex[below right = 0.8cm of v5] (f3) ;
    		
    		\vertex[xshift=0.6cm, yshift=0.2cm, below = of v3] (v4);
    		\vertex[right = 0.8cm of v4] (f2) ;		
    		
    		\diagram*{
    			(i1) -- (v1) -- (v2) -- (v3),
    			(i2) -- (v7) -- (v6) -- (v5),
    			(v3) -- (f1),
    			(v3) -- (v4) -- (v5),
    			(v4) -- (f2),
    			(v5) -- (f3),
    			(v1) -- (v7),
    			(v2) -- (v6),
    		};
    	\end{feynman}
    \end{tikzpicture}
    \caption{} \label{fig:topologyc}
    \end{subfigure}
\caption{Examples of diagram topologies associated with five-particle Feynman diagrams. It is easy to see that topologies (a) and (b) can be obtained from the maximal topology (c) by pinching some of its propagators.}
\label{fig:topologies}
\end{figure}

We point out that not all topologies contributing to each helicity amplitude are independent, in the sense that some topologies can be written as \textit{sub}topologies of others. This is illustrated in Fig.~\ref{fig:topologies}. It is clear that topologies (a) and (b) can be viewed as subtopologies of topology (c) with a few propagators absent. Topologies with the maximum number of propagators allowed for $L$-loop, $n$-particle kinematics are referred to as \textbf{maximal topologies}. All other topologies can be obtained from them by `pinching' appropriate propagators. Thus, the next step in our procedure is to express all topologies which contribute to Eq.~\ref{eq:ampschematic} as subtopologies of these maximal topologies. Moreover, after this mapping we will write the monomials $m_i(k, p)$ in terms of the propagators of the target maximal topology. In this way, we will remove all loop momentum dependence from the numerators, so that the amplitude will be a linear combination of scalar integrals over the maximal topologies, with rational coefficients of external kinematics parametrised by the momentum twistor variables.
\subsection{Reduction onto maximal topologies}
\JK{Figure out how the reduction is done in our code}
\subsection{A simple example}
\JK{Maybe 1L box? Idk if a 2L example will be too complicated.}

\section{Integration-by-parts relations} \label{sec:IBP}
\JK{Need to mention somewhere earlier that an integral family is defined as propagators + ISPs.}\\
Thanks to the effort in the previous section, the helicity amplitude we need to compute is now expressed as a sum of \textit{scalar} loop integrals over maximal topologies:
\begin{equation} \label{eq:ampreducedschematic}
    	A_n^{(L)} \left(1^{h_1}, 2^{h_2}, \ldots, n^{h_n} \right) =  
     \sum_{\mathclap{\substack{T \in \\ \text{maximal} \\ \text{topologies}}}} \sum_{\bm{\nu}} e_{T, \bm{\nu}}(p(\mathbf{x}), \varepsilon) \left[\, \prod_{l=1}^{L} \left(\int \mathrm{d}^d k_l \right) \frac{1}{\prod_{t\in T} D^{\nu_t}_t(k,p)} \right]\,.
\end{equation}
\JK{Is the notation here and below clear?...}.\\
Here, we have made explicit the dependence of $D_t$ on the exponents $\alpha_t$. As explained earlier, the index $t$ must now run over not only the inverse propagators associated with a given maximal topology, but also the ISPs that were introduced to build a complete integral family. Overall, each element in the square brackets is a $d$-dimensional scalar integral defined by its maximal topology $T$ and a set of exponents $\bm{\nu}$. We write:
\begin{equation} \label{eq:ibpintegral}
    I_T^{(L)}(\nu_1, \nu_2, \ldots) = \prod_{l=1}^{L} \left(\int \mathrm{d}^d k_l \right) \prod_{t\in T} \frac{1}{D^{\nu_t}_t(k,p)}\,,
\end{equation}
where $\nu_t \geq 0$ if $D_t$ is an inverse propagator and $\nu_t\le0$ if $D_t$ is an ISP. As one might expect, not all $I_T$ are linearly independent. Within each family, it turns out we can reduce the integrals onto a basis of so-called \textbf{master integrals}, which we denote as $\text{MI}_T$. The problem of identifying such a basis and computing the reduction coefficients of each $I_T$ onto that basis is one of the most important steps in our workflow. However, it also turns out to be extremely computationally expensive. In this section, we introduce the reader to the concept of integration-by-parts reduction, as well as to the related concepts of differential equations, symbols and uniform transcendentality. These key ideas are crucial not only from the computational perspective, but also provide an insight into the analytic structure of the amplitudes.
\subsection{A brief introduction to IBP relations} \label{sec:ibpintro}
We start by making a trivial observation: the choice of the loop momentum $k$ in Feynman integrals is arbitrary. It can be re-scaled by a constant, shifted by the external momenta or even shifted by another loop momentum (in the case of multi-loop integrals) --- the physics this integral represents does not change. Surprisingly, this simple fact leads to powerful results. To see this, consider an integrand $f(k)$ and let us shift $k \rightarrow k + \lambda q$, where $\lambda$ is a small constant and $q$ is an arbitrary momentum. First, assume that $q^\mu = p^\mu$. Then, by translational invariance~\ref{eq:translationalinvariance}:
\begin{equation}
    \int \dd^d k\, f(k) = \int \dd^d k\, f(k+\lambda p) \,.
\end{equation}
Expanding the integrand for small $\lambda$, we have:
\begin{equation}
    \int \dd^d k\, f(k) = \int \dd^d k\, \left[ f(k) + \lambda p^\mu \frac{\partial f(k)}{\partial k^\mu} + \order{\lambda^2} \right]\,.
\end{equation}
Thus, at $\order{\lambda}$:
\begin{equation}
    \int \dd^d k\, \frac{\partial (p^\mu f(k))}{\partial k^\mu} = 0\,,
\end{equation}
where we deliberately moved $p^\mu$ into the derivative.

In fact, the same equation holds even if $q^\mu = k^\mu$ and the derivation is very similar. In this case, note that due to the scaling relation Eq.~\ref{eq:scaling}:
\begin{equation}
    \int \dd^d k\, f(k) = (1+\lambda)^d \int \dd^d k\, f(k+\lambda k) \,.
\end{equation}
Expanding for small $\lambda$:
\begin{equation}
    \int \dd^d k\, f(k) = \left(1+\lambda d + \order{\lambda^2}\right) \int \dd^d k\, \left[ f(k) + \lambda k^\mu \frac{\partial f(k)}{\partial k^\mu} + \order{\lambda^2} \right]\,.
\end{equation}
Collecting terms at $\order{\lambda}$:
\begin{equation}
    \int \dd^d k\, \left[ k^\mu \frac{\partial f(k)}{\partial k^\mu} + d f(k) \right] = \int \dd^d k\, \frac{\partial(k^\mu f(k))}{\partial k^\mu} = 0\,,
\end{equation}
where we have moved $k^\mu$ into the derivative by using $\partial k^\mu / \partial k^\mu = \delta^\mu_\mu=d$. Overall, due to the linearity of $d$-dimensional integrals, Eq.~\ref{eq:linearity}, these equations must also hold for any linear combination of external and loop momenta. Thus, for a generic $q$:
\begin{equation} \label{eq:IBPboundaryterm}
    \int \dd^d k\, \frac{\partial(q^\mu f(k))}{\partial k^\mu} = 0\,.
\end{equation}
In other words, within dimensional regularisation, the boundary terms vanish. This property is the basis of the so-called \textbf{integration-by-parts} (IBP) relations \cite{Chetyrkin:1981qh}. The idea is to use Eq.~\ref{eq:IBPboundaryterm} to generate a system of equations between $d$-dimensional Feynman integrals $I_T^{(L)}(\nu_1, \nu_2, \ldots)$ of Eq.~\ref{eq:ibpintegral}. Note that each such identity leads to a relation between integrals within the same integral family $T$, but with different denominator exponents $\bm{\nu}$ (for plenty of worked out examples, see Chapter 5 of ~\cite{smirnov2006feynman} or Chapter 6 of \cite{Weinzierl:2022eaz})\footnote{Let us remark that there exist other kinds of integral identities, such as Lorentz-invariance and homogeneity relations (for details, see Refs. \cite{Grozin:2011mt, Lee:2008tj, Lee:2012cn}). In fact, they can be shown to be equivalent to linear combinations of the IBP relations. Nonetheless, they can still be useful in practice. They are typically appended to the `pure' IBP system and make it less under-determined.}. Therefore, we can use these identities to lower or raise the indices until we arrive at a simpler integral. Traditionally, this reduction was done by hand through tedious analysis of all identities. This of course becomes infeasible, as the number of identities is in theory infinite (and grows quickly with the loop order and number of external particles). On the other hand, modern computations typically employ the Laporta algorithm, which provides an algorithmic way of reducing integrals to master integrals by solving a \textit{finite} linear system of identities using Gaussian elimination \cite{Laporta:2001dd}. Many computer programmes based on the Laporta algorithm---often in combination with other IBP algorithms---are publicly available, e.g. \texttt{AIR} \cite{Anastasiou:2004vj}, \texttt{LiteRed} \cite{Lee:2012cn, Lee:2013mka}, \texttt{FIRE} \cite{Smirnov:2008iw, Smirnov:2019qkx} \texttt{Kira} \cite{Maierhofer:2017gsa, Klappert:2020nbg} and \texttt{Reduze} \cite{Studerus:2009ye, vonManteuffel:2012np}.

The IBP system is under-determined, which means that the values of the MIs cannot be determined from it. However, we can express any integral included in the system as a linear combination of these MIs, with coefficients being rational functions of the kinematics and the dimension $d$ (usually expressed as $d=4-2\varepsilon$. The rationality is an important property in the context of our finite field tools in Sec.~\ref{sec:FF}. Very often, the growth of coefficients at the intermediate stages of the IBP reduction represents a computational bottleneck. For this reason, it is extremely useful to be able to exploit numerical arithmetic over finite fields in order to bypass this complexity (see Ref.~\cite{Peraro:2019svx}).

We remark that in practice it is common to force the algorithm to reduce the families onto a predetermined MI basis. This is because --- while the choice of MIs is arbitrary --- some choices turn out to be better than others, in the sense that the integrals satisfy certain properties that make them more elegant and also easier to evaluate. We will explore this idea further in Sections \ref{sec:DEs} and \ref{sec:UT}. The definitions of such predetermined MIs can be added as additional equations to the IBP system. It is then possible to assign a lower weight to these integrals during Gaussian elimination, which means that they will be preferably chosen as the independent variables \cite{Peraro:2019svx}.

In order to agree which integrals are considered `simpler', it is useful to define the concept of sectors. A sector $\bm{\theta} = \{\theta_1, \ldots, \theta_N\}$, where $N$ - total number of propagators and ISPs in the family, and $\theta_i\in \{0, 1\}$, is the set of points $\bm{\nu} = \{\nu_1, \ldots, \nu_N\}$ in the lattice $\mathbb{Z}^N$ such that:
\begin{equation}
    H \left(\nu_i - \frac{1}{2} \right) = \mathbf{\theta}_i \qquad i \in \{1, \ldots, N\} \,,
\end{equation}
where $H(x)$ is the Heaviside step function. For example, an integral defined by the indices $\bm{\nu} = \{1, 0, 2, 1\}$ belongs to the sector $\bm{\theta} = \{1, 0, 1, 1\}$, but $\bm{\nu} = \{1, 0, 2, -1\}$ belongs to $\bm{\theta} = \{1, 0, 1, 0\}$. For the case $N=2$, it is straightforward to visualise how sectors divide the space (see Fig.~\ref{fig:sectors}). The sector $\bm{\theta} = \{1, 1, \ldots, 1\}$ is known as the `top sector', while sectors with progressively more $\theta_i = 0$ become `sub-sectors' of the top sector and of each other \JK{Is it clear what I mean here?}. Rather trivially, the strictly non-positive sector $\bm{\theta} = \{0, 0, \ldots, 0\}$ vanishes. Furthermore, the point $\bm{\nu} = \{\theta_1, \ldots, \theta_N\}$ is called the `corner point' of sector $\bm{\theta}$. A nice result is that if the integral at the corner point of a sector is 0, then the whole sector is also 0 \cite{Lee:2008tj}.

It is now possible to order (albeit not uniquely) the sectors $\bm{\theta}$ with respect to each other, as well as the integrals $I_T(\bm{\nu})$ within each sector. It is somewhat natural to think of sectors with fewer unique denominators as simpler. Thus, $\sum_{i=1}^N \theta_i$ defines an ordering between the sectors. In the $N=2$ example, we say that the sector $\{1, 1\}$ is higher than its sub-sectors $\{1, 0\}$ and $\{0, 1\}$, which are both higher than their sub-sector $\{0, 0\}$ (which vanishes anyway). However, sectors $\{1, 0\}$ and $\{0, 1\}$ are equal. Within such equal sectors, we can define further criteria, for example based on the total power of the denominators, followed by the total power of the numerators (ISPs), etc. With the help of an arbitrary ordering, we can replace higher $I_T(\bm{\nu})$ by lower ones through the IBP reduction procedure, as visualised in Fig.~\ref{fig:IBPschematic}. It should not be surprising that the set of master integrals remaining after this reduction depends on the ordering. We remark that, depending on our needs, it is possible to choose an ordering that leads to an ISP-free basis or conversely a dot-free basis.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
    \begin{tikzpicture}
        %axes
        \draw[thick,->] (-3.5,0) -- (3.5,0) node[anchor=north west] {$\nu_1$};
        \draw[thick,->] (0,-3.5) -- (0,3.5) node[anchor=south east] {$\nu_2$};
        %dots and axes labels
        \draw[fill] (0,0) circle (1pt) node[anchor=north east] {$0$};
        \foreach \i in {-3,-2,-1,1,2,3}
            {
            \draw[fill] (0,\i ) circle (1pt) node[anchor=east] {$\i$};
            \draw[fill] (\i,0) circle (1pt) node[anchor=north] {$\i$};
            \foreach \j in {-3,-2,-1,1,2,3}
                \draw[fill] (\j, \i) circle (1pt);
            }
        %lines separating sectors
        \draw[thick, blue] (0.5, -3.5) -- (0.5, 3.5);
        \draw[thick, blue] (-3.5, 0.5) -- (3.5, 0.5);

        \node at (2, 2.5) {$\bm{\theta_I} = \{1, 1\}$};
        \node at (-2, 2.5) {$\bm{\theta_{II}} = \{0, 1\}$};
        \node at (2, -2.5) {$\bm{\theta_{III}} = \{1, 0\}$};
        \node at (-2, -2.5) {$\bm{\theta_{IV}} = \{0, 0\}$};

        \clip (-3.3,-3.3) rectangle (0.3, 0.3);
        \foreach \i in {-12,...,0}
            \draw[dashed, red, samples=100] plot function{-x+(0.3+0.5*\i)} node[right] {};
    \end{tikzpicture}
    \caption{Each point $(\nu_1, \nu_2)$ in the $\mathbb{Z}^2$ lattice corresponds to the $d$-dimensional Feynman integral $I(\nu_1, \nu_2)$. The lattice is divided into sectors as defined through Eq.~\ref{eq:sectors}. They are ordered $\bm{\theta}_{I}>\bm{\theta}_{II},\,\bm{\theta}_{III}>\bm{\theta}_{IV}$. In particular, sector $\bm{\theta}_{IV}$ is trivially 0.} 
    \label{fig:IBPsectors}
    \end{subfigure}
    \hfill
    \hspace*{1em}\raisebox{0.9cm}{ % needed to align the diagrams
    \begin{subfigure}[b]{0.45\textwidth}
    \begin{tikzpicture}
      %axes
        \draw[thick,->] (-0.5,0) -- (5.5,0) node[anchor=north west] {$\nu_1$};
        \draw[thick,->] (0,-0.5) -- (0,6.5) node[anchor=south east] {$\nu_2$};

        %dots and axes labels
        \draw[fill] (0,0) circle (1pt) node[anchor=north east] {$0$};
        \draw[fill] (0,6) circle (1pt) node[anchor=east] {$6$};
        \foreach \i in {1,...,5}
            {
            \draw[fill] (\i,0) circle (1pt) node[anchor=north] {$\i$};
            \draw[fill] (0,\i ) circle (1pt) node[anchor=east] {$\i$};
            \foreach \j in {1,...,6}
                {
                \draw[fill] (\i, \j) circle (1pt);
                }
            }

        %lines separating sectors
        \draw[thick, blue] (0.5, -0.5) -- (0.5, 6.5);
        \draw[thick, blue] (-0.5, 0.5) -- (5.5, 0.5);

        \draw[orange, thick, ->] (5, 4) edge (5,3) (5, 3) edge (4,3) (4,3) edge (4,2) (4,2) edge (3,2) (3,2) edge (2,2) (2,2) edge (2,1) (2, 1) -- (1,1);
        
        \node at (5, 6.5) {$\bm{\theta_I}$};
        \node at (1, 1) [anchor=south] {$\text{MI}(1, 1)$};
        \node at (5, 4) [anchor=south] {$I(5, 4)$};
    \end{tikzpicture}
    \caption{A hypothetical reduction pathway within the top sector $\bm{\theta}_I$. An integral $I(5, 4)$ is reduced to the master integral $\text{MI}(1,1)$ through a series of IBP relations lowering the denominator exponents.} \label{fig:IBPreduction}
    \end{subfigure}
    }
    \caption{A lattice of points visualising the integrals and sectors used in the IBP reduction ($N=2$).}
    \label{fig:IBPschematic}
\end{figure}
\begin{figure}[t]
    \centering
        \begin{tikzpicture}
        	\begin{feynman}
        		\vertex (v1);
          
        		\vertex[right = of v1] (v2);			
        		\vertex[below = of v2] (v3);			
        		\vertex[left = of v3] (v4);			
          
        		\vertex[above left = 0.7cm of v1] (i1) {$p_1$};
        		\vertex[above right =  0.7cm of v2] (i2) {$p_2$};
        		\vertex[below right =  0.7cm of v3] (i3) {$p_3$};
        		\vertex[below left =  0.7cm of v4] (i4) {$p_4$};

                \node (dot) at ($(v2)!0.5!(v3)$);
                \draw[fill] (dot) circle (2pt);
          
        		\diagram*{
                    (v1) -- (v2) -- (v3) -- (v4) -- [momentum=$k_1$] (v1),
                    (i1) -- (v1),
                    (i2) -- (v2),
                    (i3) -- (v3),
                    (i4) -- (v4),
                    };
        	\end{feynman}
        \end{tikzpicture}
    \caption{One-loop box Feynman diagram $I_\text{box}(1,1,2,1)$ with a `dotted' propagator. The corresponding denominators are: $\{k_1^2, (k_1-p_1)^2, \left((k_1-p_1-p_2)^2\right)^2, (k_1+p_4)^2\}$\,.} \label{fig:1Lboxdot}
\end{figure}

Overall, IBP relations allow us to express each helicity amplitude of Eq.~\ref{eq:ampreducedschematic} in terms of a much smaller number of master integrals:
\begin{equation} \label{eq:ampafterIBP}
    	A_n^{(L)} \left(1^{h_1}, 2^{h_2}, \ldots, n^{h_n} \right) =  
     \sum_{i=1}^{|\text{MI}|} g_i(p(\mathbf{x}), \varepsilon) \times \text{MI}_i(p,\varepsilon)\,,
\end{equation}
where $|\text{MI}|$ denotes the total number of linearly independent master integrals in all the families. Once again, the coefficients are rational functions of external kinematics (expressed through the momentum twistor variables), as well as the dimensional parameter $\varepsilon$. It is worth pointing out an interesting subtlety of this formula:
\begin{equation} \label{eq:noofmis}
    |\text{MI}| \equiv |\text{MI}_{\sum_T T}| \neq \sum_{\mathclap{\substack{T \in \\ \text{maximal} \\ \text{topologies}}}} |\text{MI}_T| \,.
\end{equation}
That is, the number of MIs for all families considered together is not the same as the sum of numbers of MIs for these families considered separately. This statement might seem surprising. Indeed, if we formulate one big system of IBP relations between integrals in all the families and perform the IBP reduction, the resultant MI basis is equal to the sum of the bases obtained by solving smaller systems of IBP relations within each family one by one. This is because integrals in different families live in different vector spaces and there cannot be inter-family IBP relations. However, integrals belonging to sectors other than the top sector can often be mapped between families. We present an easy to understand example in Fig.~\ref{fig:symmetryrelation}. We will return to the observation of Eq.~\ref{eq:noofmis} in Sec.~\ref{sec:altibps}. 
\begin{figure}
    \centering
    \begin{adjustbox}{minipage=\textwidth,scale=0.9}
    \raisebox{0.2cm}{
    \begin{subfigure}[b]{0.25\linewidth}
        \hspace*{1em}
        \begin{tikzpicture}
    	\begin{feynman}[small]
    		\vertex (v1);
    		\vertex[above left = 0.8cm of v1] (i1) {$p_5$};
    		\vertex[right = of v1] (v2);
    		\vertex[yshift=0.3cm, right = 0.8cm of v2] (v3);
    		\vertex[above right = 0.8cm of v3] (f1) {$p_1$};			
    		
    		\vertex[below = of v1] (v7) ;
    		\vertex[below left = 0.8cm of v7] (i2) {$p_4$};
    		\vertex[right = of v7] (v6) ;
    		\vertex[yshift=-0.3cm, right = 0.8cm of v6] (v5) ;
    		\vertex[below right = 0.8cm of v5] (f3) {$p_3$};
    		
    		\vertex[xshift=0.6cm, yshift=0.2cm, below = of v3] (v4);
    		\vertex[right = 0.8cm of v4] (f2) {$p_2$};		

            \draw ($(v7)!0.5!(v6)$) node[cross,red] (5cm) {};
            \draw ($(v4)!0.5!(v5)$) node[cross,red,rotate=45] (5cm) {};
            
    		\diagram*{
    			(i1) -- (v1) -- (v2) -- (v3),
    			(i2) -- (v7) -- (v6) -- (v5),
    			(v3) -- (f1),
    			(v3) -- (v4) -- (v5),
    			(v4) -- (f2),
    			(v5) -- (f3),
    			(v1) -- (v7),
    			(v2) -- (v6),
    		};
    	\end{feynman}
    \end{tikzpicture}
    \caption{} \label{fig:pentabox}
    \end{subfigure}
    }
    %\end{adjustbox}
    \hspace{1.3cm}
    %\begin{adjustbox}{minipage=\linewidth,scale=0.4}
    \begin{subfigure}[b]{0.25\linewidth}
        \hspace*{1em}
        \begin{tikzpicture}
        \begin{feynman}[small]
			\vertex (v1);
			\vertex[left = 0.8cm of v1] (i1) {$p_5$};
			\vertex[above right =1cm of v1] (v2);
			\vertex[below right =1cm of v1] (v3);				
			\vertex[yshift=0.3cm, right = 0.7cm of v2] (v4);	
			\vertex[yshift=-0.6cm, right = 0.7cm of v4] (v5);
			\vertex[yshift=-0.3cm, right = 0.7cm of v3] (v7);
			\vertex[yshift=0.6cm, right = 0.7cm of v7] (v6);	
			\vertex[above right = 0.8cm of v4] (i2) {$p_1$};
			\vertex[yshift=0.2cm, right = 0.8cm of v5] (i3) {$p_3$};
			\vertex[yshift=-0.2cm, right = 0.8cm of v6] (i4) {$p_2$};
			\vertex[below right = 0.8cm of v7] (i5) {$p_4$};
			
            \draw ($(v7)!0.5!(v3)$) node[cross,red,rotate=60] (5cm) {};
            \draw ($(v6)!0.5!(v5)$) node[cross,red] (5cm) {};
            
			\diagram*{
				(i1) -- (v1) -- (v2) -- (v3) -- (v1),
				(v2) -- (v4) -- (v5) -- (v6) -- (v7) -- (v3),
				(v4) -- (i2),
				(v5) -- (i3),
				(v6) -- (i4),
				(v7) -- (i5),
			};
		\end{feynman}
        \end{tikzpicture}
    \caption{} \label{fig:hexatriangle}
    \end{subfigure}
    %\end{adjustbox}
    \hspace{1.3cm}
    %\begin{adjustbox}{minipage=\linewidth,scale=0.4}
    \raisebox{0.2cm}{
    \begin{subfigure}[b]{0.25\linewidth}
        \hspace*{1em}
        \begin{tikzpicture}
    	\begin{feynman}[small]
    		\vertex (v1);
      
    		\vertex[left = 0.8cm of v1] (i1) {$p_5$};
    		\vertex[above right = of v1] (v2);			
    		\vertex[below right = of v1] (v3);			
    		\vertex[right = of v2] (v4);			
    		\vertex[right = of v3] (v5);			
    		
    		\vertex[above right = 0.8cm of v4] (i2) {$p_1$};
    		\vertex[below right = 0.7cm of v5, yshift=-0.2cm] (i3) {$p_3$};
    		\vertex[below right = 0.5cm of v5, xshift=+0.4cm] (i4) {$p_2$};
    		\vertex[below =  0.7cm of v3] (i5) {$p_4$};
    		
    		\diagram*{
                (i1) -- (v1) -- (v2) -- (v4) -- (i2),
                (v1) -- (v3) -- (v5) -- (i3),
                (v2) -- (v3),
                (v4) -- (v5),
                (v5) -- (i4),
                (v3) -- (i5),
    		};
    	\end{feynman}
    \end{tikzpicture}
    \caption{} \label{fig:topologyc}
    \end{subfigure}
    }
    \end{adjustbox}
\caption{A simple example of possible non-IBP relations between integrals within different (maximal) topologies. Integrals in lower sectors often map to each other. The red crosses denote a vanishing denominator, i.e. the corresponding $\nu_i$ is 0. Here, diagrams $(a)$ and $(b)$ both collapse onto diagram $(c)$. Such relations are often hard to discover just from analysing the propagators of the integrals, especially if the two families use vastly different conventions for loop momentum routing. However, a visual representation often makes spotting these relations trivial.}
\label{fig:topologies}
\end{figure}
\subsection{Differential equations} \label{sec:DEs}
Differential equations (DEs) satisfied by Feynman integrals were studied even before the invention of the IBP reduction technique (see, for example, Ref.~\cite{Golubeva:1976}). However, it was these two concepts combined together that led to the development of a powerful method for evaluating the MIs \cite{Kotikov:1990kg, Kotikov:1991314, Bern:1993kr, Remiddi:1997ny, Gehrmann:1999as}. The idea is as follows. If we differentiate an integral $I_T(\bm{\nu})$ with respect to the Mandelstam invariants $s_{ij}$ or internal masses $m_i$, we will obtain a linear combination of integrals within the same family $T$, but with different exponents $\bm{\nu}'$. These new integrals $I_T(\bm{\nu}')$ can then be IBP reduced onto the corresponding MIs. Thus, if we apply the differentiation to the MI basis itself, we obtain a set of first-order partial DEs (one for each kinematic variable). It is convenient to group the MIs as a vector $\vv{\text{MI}}$. We then have:
\begin{equation} \label{eq:DEspartial}
    \frac{\partial}{\partial \lambda} \vv{\text{MI}} = A_{\lambda}(\lambda, \varepsilon) \vv{\text{MI}}\,,
\end{equation}
where $\lambda \in \{s_{ij}, m_i\}$ are the kinematic variables and $A_\lambda$ is a $\left(|\text{MI}| \times |\text{MI}|\right)$ matrix. Its entries are rational functions of the kinematic variables and $\varepsilon$, which is due to the nature of IBP relations. It is also common to work with the total differential rather than partial derivatives:
\begin{equation}
    \dd \,\vv{\text{MI}} = \sum_{\lambda} \left(\frac{\partial \vv{\text{MI}}}{\partial \lambda} \right) \dd \lambda\,,
\end{equation}
as well as to define:
\begin{equation}
    A = \sum_\lambda A_\lambda \dd\lambda\,.
\end{equation}
Then, the system of DEs can be written as:
\begin{equation} \label{eq:DEsoneform}
    \dd \,\vv{\text{MI}} = A(\lambda, \varepsilon) \,\vv{\text{MI}}\,.
\end{equation}
Naturally, to solve this equation, we also need to provide boundary values. It is convenient to use the values of MIs at a special kinematic point, for example where one of the kinematic variables $\lambda_i$ vanishes or is equal to another kinematic variable. There, the MIs become easier to evaluate. 

In general, solving the DEs satisfied by MIs is hard. Note however, that typically we are interested in only the first few coefficients of the Laurent expansion of Eq.~\ref{eq:DEsoneform} around $\varepsilon=0$. Consider a change of basis:
\begin{equation} \label{eq:basistransformation}
    \vv{\text{MI}} \rightarrow \vv{\text{MI}}' = B\,\vv{\text{MI}}\,,
\end{equation}
where $B$ is an arbitrary matrix that can depend on $\lambda$ and $\varepsilon$. Under this transformation, the partial DEs become:
\begin{equation}
    \frac{\partial \vv{\text{MI}}'}{\partial \lambda} = \left( \frac{\partial B}{\partial \lambda}B^{-1} + B A_{\lambda} B^{-1} \right) \vv{\text{MI}}'
\end{equation}
A key conjecture due to Ref.~\cite{Henn:2013pwa} is that it is always possible to choose $B$ such that:
\begin{equation}
    \left( \frac{\partial B}{\partial \lambda}B^{-1} + B A_{\lambda}(\lambda, \, \varepsilon) B^{-1} \right) = \varepsilon \dd \tilde{A}_{\lambda}(\lambda)\,.
\end{equation}
That is, with an appropriate transformation, the $\varepsilon$-dependence factorises out of the matrices, which now contain only kinematic dependence. Note, however, that they might now contain algebraic (i.e. non-rational) factors such as square roots. Overall, Eq.~\ref{eq:DEspartial} becomes:
\begin{equation} \label{eq:canonicalDEspartial}
    \frac{\partial}{\partial \lambda} \vv{\text{MI}}' = \varepsilon\dd \tilde{A}_{\lambda}(\lambda) \vv{\text{MI}}'\,, 
\end{equation}
while Eq.~\ref{eq:DEsoneform}:
\begin{equation} \label{eq:canonicalDEsoneform}
    \dd \,\vv{\text{MI}}' = \varepsilon \dd \tilde{A}(\lambda) \,\vv{\text{MI}}'\,.
\end{equation}
\JK{Should this be $\dd \tilde{A}(\lambda)$?}
This is known as DEs in \textbf{canonical form} (also known as the $\varepsilon$-form)\footnote{Several packages exist for transforming the DEs into the canonical form. See \texttt{Fuchsia}~\cite{Gituliar:2017vzm}, \texttt{Libra}~\cite{Lee:2020zfb}, \texttt{INITIAL}~\cite{Dlapa:2020cwj} and \texttt{CANONICA}~\cite{Meyer:2018feh}. See also \cite{Dlapa:2022nct} for a comprehensive review of different techniques.}. In particular, Eq.~\ref{eq:canonicalDEsoneform} admits a solution in terms of the path-ordered exponential:
\begin{equation} \label{eq:pathexponential}
    \dd \,\vv{\text{MI}}'(\lambda,\, \varepsilon) = \mathbb{P} \exp \left(\varepsilon\int_\gamma \dd \tilde{A} \right) \vv{\text{MI}}'(\lambda(\gamma=0),\,\varepsilon )\,,
\end{equation}
where $\gamma\,:\,[0, 1] \rightarrow \mathbb{C}^{|\lambda|}$ is a path in the space of the kinematic invariants and internal masses $\lambda$. We note that the integration is independent of the path taken. Because the solution is expressed through the series expansion of the matrix exponential, it is easy to see that the MIs are obtained as iterated integrals of $\tilde{A}$. Given the Laurent expansion of canonical MI vector:
\begin{equation} \label{eq:MIlaurentexp}
    \vv{\text{MI}}'(\lambda, \, \varepsilon) = \sum_{i=0}^\infty \varepsilon^i\,\vv{\text{MI}}'^{(i)}(\lambda) \,,
\end{equation}
we can insert it into Eq.~\ref{eq:pathexponential} and series expand both its sides in $\varepsilon$. Then the order-by-order DE solution becomes:
\begin{equation} \label{eq:DEssolution}
    \vv{\text{MI}}'^{(i)}(\lambda) = \sum_{j=0}^i \int_\gamma \underbrace{\dd \tilde{A} \cdot \ldots \cdot \dd \tilde{A}}_{j\text{ times}} \, \cdot \, \vv{\text{MI}}'^{(i-j)}(\lambda(\gamma=0))\,,
\end{equation}
where the path integral for $j=0$ is defined as 1\cite{Chen:1977oja}. Note that we can assume that the Laurent expansion starts from $\varepsilon=0$, because the DEs are insensitive to a re-scaling of the integrals by a factor which does not depend on the kinematics. Thus, the MIs can be normalised to be finite, which moves any potential singularities into the coefficients of the IBP reduction. 

Overall, we see that the MIs at $\order{\varepsilon^i}$ are given by sums of up to $i$-fold integrals. The integration kernels are determined by the structure of $\tilde{A}$, which deserves further discussion. Let us consider the singularities of the DEs. It can be shown, for example by studying the Feynman parameter representation, that the Feynman integrals cannot contain so-called essential singularities, e.g. singularities of the form $\mathrm{e}^{1/\lambda} = 1+ 1/\lambda + 1/(2!\lambda^2)+\ldots\,$ at $\lambda=0$. In particular, for each singularity $\lambda_k$, we expect the leading behaviour of the Feynman integrals to be $\sim (\lambda- \lambda_k)^\alpha$ for some power $\alpha$. This implies that the corresponding DEs must have only regular singularities. In particular, around each singular point $\lambda_k$, they must have a simple pole $\sim \alpha/(\lambda-\lambda_k)$. This strongly constrains the form that DEs can take\footnote{In practice, when constructing canonical DEs, we might encounter spurious double poles or higher. However, they can be removed by a suitable basis change which leaves the DEs with only simple poles. \JK{Is this always true?}}\cite{Henn:2014qga}. In many cases of practical interest, it is possible to construct MI bases which result in DEs in the so-called \textbf{$\dd \log$ form}:
\begin{equation} \label{eq:dlogform}
    \dd\tilde{A} = \sum_{i=1}^{|\bm{w}|} a_i \times \dd \log w_i\,.
\end{equation}
Here, $w_i$ are known as `letters' (note that sometimes the differentials $\dd \log w_i$ are referred to as letters instead), while their collection $\bm{w} = \{w_1, w_2, \ldots\}$ is the `alphabet'. The matrices $a_i$ contain rational numbers only, free of any kinematic of $\varepsilon$ dependence. The letters play a central role in the analysis of DEs in the $\dd \log$ form, since they control their singularities, determine which class of special functions the MIs are written in terms of, as well as play a central role in the symbol formalism, which we will introduce soon. As mentioned before, in general the kinematic dependence of $\tilde{A}$ might no longer be rational, as the transformation Eq.~\ref{eq:basistransformation} needed to bring DEs into canonical form might introduce non-rational functions into the definitions of the corresponding canonical MIs. In this case, the letters $w_k$ are algebraic functions of $\lambda$. However, if the form of Eq.~\ref{eq:dlogform} can be reached by using rational transformations only, the letters are rational functions of the form $w_k = \lambda - \lambda_k$. Then, it is particularly easy to write down the order-by-order solution Eq.~\ref{eq:DEssolution}, as the iterated integrals become the well-known Goncharov Polylogarithms (GPLs)\footnote{Also known as Hyperlogarithms and Multiple Polylogarithms (MPLs).}\cite{2001math......3059G, 2011arXiv1105.2076G, Vollinga:2004sn}:
\begin{equation} \label{eq:GPLs}
    G_n(a_1, a_2, \ldots, a_n; \lambda) = \int_0^\lambda \frac{\dd t}{t-a_1} G_{n-1}(a_2, \ldots, a_n; t)\, \qquad \text{with } \quad G_0(; x) = 1\,.
\end{equation}
Here, despite the somewhat suggestive notation, the indices $a_i$ do not have to be numbers and are considered fully-fledged arguments of $G_n$, alongside $\lambda$. The length of the vector $\vec{a} = (a_1, \ldots a_n)$, i.e. $|\vec{a}|=n$ is called the weight (or depth) of $G_n$. The GPLs are related to the usual logarithms through:
\begin{equation}
    G_n(a, a, \ldots, a; \lambda) = 
    \begin{cases}
        \frac{1}{n!}\log^n\left(1-\frac{\lambda}{a}\right) & \text{if  $a \neq 0$} \\ 
        \frac{1}{n!}\log^n x & \text{if $a = 0$}\,.
    \end{cases}
\end{equation}  
Aside from this special case of GPLs, the complexity of the integration kernels can be estimated by studying the maximal cuts of the relevant Feynman integrals~\cite{Tancredi:2017onthemaxcut, Abreu:2020jxa, abreu2021twoloop}. More generally, the letters might not be rational, or the kernels overall might not be of the $\dd \log$ form. Then, more complicated functions appear in the solution of the DEs.  For example, the presence of internal masses in the Feynman diagrams often \JK{Often or always?} leads to Elliptic Multiple Polylogarithms (eMPLs) (see Refs.~ \cite{Broedel:2017kkb, Broedel:2017siw, Adams:2017ejb, Broedel:2018qkq, Adams:2018yfj, Walden:2020odh} for a discussion of eMPLs and also Refs.~\cite{Bonciani:2016qxi, Bonciani:2019jyb, Frellesvig:2019byn} for the application to Higgs+jet production with quark mass dependence). 

\subsection{Uniform transcendentality} \label{sec:UT}
When talking about canonical DEs, it is also useful to introduce the idea of `transcendentality'. For an iterated integral $f$, its \textbf{transcendental weight} $\mathcal{T}$ is simply the count of iterated integrations needed to define $f$\cite{Henn:2013pwa}. For example, $\mathcal{T}(\log x) = 1$, while $\mathcal{T}(G_n(a_1, \ldots, a_n;\, x)) = n$. From the definition, it follows that $\mathcal{T}(f_1 f_2) = \mathcal{T}(f_1) + \mathcal{T}(f_2)$, but note that $\mathcal{T}(f_1 + f_2)$ cannot be defined unless $\mathcal{T}(f_1) = \mathcal{T}(f_2)$. Furthermore, algebraic functions and constants have weight 0. Special constants which can be considered as derived from iterated integrals have the corresponding weight, e.g. $\mathcal{T}(\pi) = 1$, since $\log (-1) = \pm \i \pi$ and $\mathcal{T}(i) = \mathcal{T}(-1) = 0$, while $\mathcal{T}(\zeta(n)) = n$, since $\zeta(n) = \text{Li}_n(1)$ (for $n>1$). It is also convenient to assign $\mathcal{T}(\varepsilon)=-1$. With this choice, it is clear that every term in the solution of Eq.~\ref{eq:pathexponential} has the same transcendental weight. This property is referred to as \textbf{uniform transcendentality} (UT). A simple example of a UT function could be $f(x) = 1 + \varepsilon (\log x + \pi) + \varepsilon^2 (\log^2 x + G_2(1, 1;\, x))$, with $\mathcal{T}(f) = 0$. Additionally, 
UT functions satisfying a more stringent condition:
\begin{equation} \label{eq:purecondition}
    \mathcal{T}(\dd f) = \mathcal{T}(f) - 1
\end{equation}
are known as \textit{pure} function. In practice, this means that a pure function cannot contain algebraic factors that are not constant --- while they do not affect the transcendental weight of a function, they affect the DE it satisfies\footnote{Note that in literature, the term 'UT' is often implicitly taken to mean `UT and pure'.}. It immediately follows that the iterated integral solution to the canonical DEs is built out of pure functions (the reverse is also true: given a pure basis of MIs, the DEs they satisfy will be canonical). In practice, when constructing a MI basis, we can verify its purity by checking a few conditions that the DE matrices $\tilde{A}_\lambda$ of Eq.~\ref{eq:canonicalDEspartial} have to satisfy:
\begin{subequations}
\begin{align}
    [ \tilde{A}_i, \tilde{A}_j] &= 0\,, \\
    \partial_i \tilde{A}_j - \partial_j \tilde{A}_i &= 0\,, \\
    \sum_i \lambda_i \tilde{A}_i &= [\vv{\text{MI}'}] \, \mathds{1} \,,
\end{align}
\end{subequations}
where in the last equation, the sum runs over all kinematic scales, $[\vv{\text{MI}'}]$ are the mass dimensions of the MIs. \JK{I don't understand the scaling property}. Finally, we point out an interesting observation on the nature of dimensionally regulated amplitudes (see e.g. Ref.~\cite{Duhr:2014woa}). For a Laurent expanded $L$-loop amplitude in $d = 4- 2\varepsilon$, it is \textit{conjectured} that the $\order{\varepsilon^k}$ term contains functions of transcendental weight up to $2L+k$. For example, to calculate a two-loop amplitude up to $\order{\varepsilon^0}$, we need to supply the MI expansions up to weight 4. It is expected that in $\mathcal{N}=4$ Super Yang-Mills theory, this bound is saturated, i.e. functions of weight exactly $2L+k$ are required.

Overall, pure integrals have played a central role in the derivation and evaluation of MI bases relevant to this thesis, that is bases for processes with a high number of kinematic scales. For future convenience, in Table~\ref{tab:MIs} we collect (without claiming to be exhaustive) the publications dealing with two-loop, five-point integrals with up to one external mass and massless propagators.
\begin{table}[b]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
            \hline
            External masses & Type & Topology & Publications \\
			\hline
            \multirow{2}{0cm}{0} & planar & penta-box & \cite{Gehrmann:2015bfy,Papadopoulos:2015jft, Gehrmann:2018yef, Abreu:2018aqd, Chicherin:2020oor} \\
            \cline{2-4}
            & \multirow{2}{2cm}{non-planar} & hexa-box & \cite{Chicherin:2018mue, Chicherin:2017dob, Chicherin:2018ubl, Chicherin:2018wes, Abreu:2018rcw, Chicherin:2020oor, Abreu:2018aqd} \\
            & & double-pentagon & \cite{Chicherin:2018old, Abreu:2018aqd, Chicherin:2020oor} \\
            \hline
            \multirow{2}{0cm}{1} & planar & penta-box & \cite{Abreu:2020jxa, Chicherin:2021dyp, Canko:2020ylt} \\
            \cline{2-4}
            & \multirow{2}{2cm}{non-planar} & hexa-box & \cite{abreu2021twoloop, Kardos:2022tpo, Papadopoulos:2019iam, Chicherin:2021dyp} \\
            & & double-pentagon & N/A \\
            \hline
		\end{tabular}
\end{center}
\caption{Selected works relevant to the computation of two-loop, five-point pure master integral bases. All propagators are massless. At the time of writing, results for the one-mass non-planar double-pentagon family are not available in literature.}
\label{tab:MIs}
\end{table}

\JK{Ask Simone to explain why mappings between UT MIs cannot contain $\varepsilon$ or kinematic dependence.}
\subsection{Symbols} \label{sec:symbols}
In this section, we introduce yet another useful concept related to iterated integrals. We begin by noting that the representation of the solution to the canonical DEs in Eq.~\ref{eq:canonicalDEsoneform} is not unique. Most generally, the answer is written using Chen's iterated integrals (CIIs)~\cite{Chen:1977oja}. This form is usually the most compact one. However, as pointed out above Eq.~\ref{eq:GPLs}, if the `letters' are rational in at least one variable, CIIs be expressed in terms of GPLs~\cite{Henn:2014qga}. This typically increases the number of terms, but has the advantage that robust and stable numerical evaluation of GPLs is available (see e.g. \cite{Bauer:2000cp, Vollinga:2004sn}). Moreover, due to the conjecture pointed out in the previous section, typically we will be interested in computing the solution up to transcendental weight 4. Another remarkable conjecture exists: any transcendental function of weight $k < 4$ can be expressed in terms of the usual logarithms and polylogarithms $\text{Li}_k$ only~\cite{MR1265551, Goncharov:2010jf}, while at weight $k=4$ all functions can be expressed in terms of the following functions~\cite{Henn:2014qga}:
\begin{equation} \label{eq:weight4basis}
    \left\{\log(x)\log(y)\log(z)\log(w), \Li_2(z) \log(x) \log(y), \Li_2(x)\Li_2(y), \Li_3(y) \log (x), \Li_4(x), \Li_{2,2}(x, y) \right\}\,,
\end{equation}
Here, the classical polylogarithms are defined as:
\begin{equation} \label{eq:Lidefinition}
    \Li_n(z) = \sum_{k=1}^\infty \frac{z^k}{k^n}\,, \qquad z, n \in \mathbb{C}\,,
\end{equation}
while the multiple polylogarithms $\Li_{m_1, \ldots, m_k}$ are their generalisations~\cite{2011arXiv1105.2076G, Duhr:2019tlz}:
\begin{subequations}
\begin{align} \label{eq:multiLidefinition}
    \Li_{m_1, \ldots, m_n}(z_1, \ldots, z_n) &= \sum_{0<k_1<\ldots<k_n} \frac{z_1^{k_1} \ldots z_n^{k_n}}{k_1^{m_1} \ldots k_n^{m_n}} \\
    &= \sum_{k_n=1}^\infty \frac{z_n^{k_n}}{k_n^{m_n}} \sum_{k_{n-1}=1}^{k_n-1} \frac{z_{n-1}^{k_{n-1}}}{k_{n-1}^{m_{n-1}}}\ldots \sum_{k_1=1}^{k_2-1} \frac{z_1^{k_1}}{k_1^{m_1}}\,. \label{eq:multiLidefinition2}
\end{align}
\end{subequations}
Note that in these two definitions, $|z_i|<1$, but the formulas can be analytically continued to cover regions where $|z_i| \ge 1$. The $\Li_{m_1, \ldots, m_k}$ MPLs can be viewed as the series representation of the GPLs defined in Eq.~\ref{eq:GPLs}. They are related through the following equation~\cite{Duhr:2019tlz}:
\begin{equation} \label{eq:LiToG}
    \Li_{m_1, \ldots, m_n}(z_1, \ldots, z_n) = (-1)^n\,G\left(\vec{0}_{m_n-1}, \frac{1}{z_n}, \ldots, \vec{0}_{m_1-1}, \frac{1}{z_1\ldots z_n}; \,1 \right)\,.
\end{equation}
For example, $\Li_{2,2}(x, y)$ in the weight-4 basis Eq.~\ref{eq:weight4basis} can be written as:%admits the integral representation:
\begin{equation}
     \Li_{2,2}(x, y) = G\left(0, \frac{1}{y}, 0, \frac{1}{xy};\,1 \right)\,.
\end{equation}
%\begin{equation}
%    \Li_{2,2}(x, y) = -\int_0^1 \frac{x \dd t}{1- xt} \log t \, \Li_2(xyt)\,.
%\end{equation}
Overall, we see that the most naive solution to the canonical DEs, written in terms of CIIs, is likely to contain a large degree of redundancy. However, due to the abundance of integral representations, it is not easy to choose the most appropriate set of functions or even verify if two expressions are equivalent. We would like to have a tool which can find relations between transcendental functions and be representation independent \JK{Is it correct to say that the symbol is representation independent?}. To this end, we introduce the notion of a \textbf{symbol}\cite{Goncharov:2010jf, Duhr:2011zq}. After giving its definition and properties, we will demonstrate its power based on several illustrative examples. To motivate our effort, we remark that the main result of Ref.~\cite{Goncharov:2010jf} was the re-writing of a 17-page expression for a quantity known as the two-loop, six-point remainder function (related to Wilson loops) in $\mathcal{N}=4$ SYM~\cite{DelDuca:2009au, DelDuca:2010zg} as a remarkably compact combination of classical polylogarithms. Moreover, we will have a chance to exploit the power of symbols on our own when we study the structure of the `all-plus' $\phi$+gluon amplitude in Section~\ref{sec:selfdual}.

The symbol of a CII with $\dd \log$ is kernels defined as:
\begin{equation} \label{eq:symboldef}
    \SS \left(\int_\gamma \dd \log w_1 \cdot \ldots \cdot \dd \log w_n \right) \equiv w_1 \oo \ldots \oo w_n \,.
\end{equation}
We can think of it as an elementary $n$-fold tensor where each entry $w_i$ is implicitly understood as to mean its corresponding $\dd \log$ form. It satisfies certain properties that we would expect from the behaviour of logarithms\footnote{for a detailed discussion of symbol properties, see Ref.~\cite{Duhr:2011zq}}:
\begin{subequations}
\begin{align}
    A \oo (ab) \oo B &= A \oo a \oo B + A \oo b \oo B\,, \\
    A \oo \left(\frac{a}{b}\right) \oo B &= A \oo a \oo B - A \oo b \oo B\,, \\
    A \oo a^n \oo B &= n(A \oo a \oo B)\,,
\end{align}
\end{subequations}
where $A, B$ are elementary tensors and $a, b$ are algebraic functions. Note that in the last line, $n$ becomes a coefficient of the symbol, rather than a part of it. Indeed, a symbol which contains a constant (not only rational) vanishes:
\begin{equation} \label{eq:symbolAcB}
    A \oo c \oo B = 0\,,
\end{equation}
which can be understood as a consequence of $\dd \log c = 0$. This means that, when working with the symbol, we are insensitive to constants, including transcendental ones like $\pi$. 

Since CIIs are defined as repeated integrations over some kernels, it is natural to expect that the total differential acts on the symbol defined through Eq.~\ref{eq:symboldef} in the following way:
\begin{equation} \label{eq:symboldiff}
    \dd (w_1 \oo \ldots \oo w_n) = \dd \log w_n \, (w_1 \oo \ldots \oo w_{n-1})\,.
\end{equation}
This should remind the reader of the definition of pure functions in Eq.~\ref{eq:purecondition}. Crucially, this differential property can be reversed and allows us to define the symbol recursively~\cite{Goncharov:2010jf}. For a transcendental function $F$ with $\mathcal{T}(F) = k$ whose total differential can be written as:
\begin{equation}
    \dd F = \sum_i f_i \, \dd \log w_i\,,
\end{equation}
where $f_i$ are functions with $T(f_i)=k-1$, we have:
\begin{equation} \label{eq:symbolrecursive}
    \SS(F) = \sum_i \SS(f_i)\oo w_i \,.
\end{equation}
The starting point of the recursion is a single weight-1 function --- the logarithm: $\SS(\log w) = w$, where the $w$ on the RHS is understood as an elementary tensor in the sense of Eq.~\ref{eq:symboldef} (remember that $\SS(\pi) = \SS(\zeta(n))=0$ \JK{Is this relevant?}). This immediately allows us to obtain the symbol of any classical polylogarithm $\Li_k$. Note the weight-1 polylogarithm is just the normal logarithm:
\begin{equation}
    \Li_1(z) = -\log (1-z)\,. 
\end{equation}
Moreover, the following integral relation holds:
\begin{equation} \label{eq:Liiterative}
    \Li_n(z) = \int_0^z \frac{\dd t}{t} \Li_{n-1}(t)\,.
\end{equation}
Thus, we have:
\begin{align} \label{eq:Lisymbols}
    \SS(\Li_1(z)) &= -(1-z)\,, \nonumber \\
    \SS(\Li_2(z)) &= -(1-z) \oo z \,, \nonumber \\
    &\,\,\, \vdots \nonumber \\
    \SS(\Li_n(z)) &= -(1-z) \oo \underbrace{z \oo \ldots \oo z}_{n-1 \text{ times}}\,.
\end{align}
Note that in deriving the first line, we used $\SS(-f) = -\SS(f)$\JK{Make sure this is correct.} and this minus sign cannot be dropped or absorbed, i.e. $\SS(\Li_1(z)) \neq (z-1)$. Moreover, it should not be confused with the symbol expression $-1 \oo (1-z)$, which vanishes due to Eq.~\ref{eq:symbolAcB}. To avoid confusion, the symbol is often written using a bracket and with the `$\SS$' retained, e.g. $\SS(\Li_2(z)) = -\SS[1-z, z]$.

\JK{How to take $\SS(G(\vec{a}; z))$?.}
\begin{equation} \label{eq:Gsymbol}
    \textcolor{red}{\SS(G(a_1, \ldots a_n; z)) = }
\end{equation}

We also need to know how to obtain the symbol of a product of functions, e.g. $\SS(\log(z) \Li_2(z))$. To this end, we point out that the CIIs satisfy so-called shuffle relations. Specialising to the case of GPLs, we have:
\begin{equation}
    G(\vec{a};z)G(\vec{b};z) = \sum_{\vec{c}=\vec{a} \shuffle \vec{b}} G(\vec{c};z)\,.
\end{equation}
The shuffle product $\vec{a} \shuffle \vec{b}$ produces all possible ways of permuting the union of indices $\vec{a}$ and $\vec{b}$ such that the ordering of indices within each set is preserved. Perhaps a simple example illustrates this definition better:
\begin{align} \label{eq:shuffleexample}
    G(a_1,a_2;z) G(b_1,b_2;z) &= G(a_1,a_2,b_1,b_2;z) + G(a_1,b_1,a_2,b_2;z) + G(a_1,b_1,b_2,a_2;z) \nonumber \\ 
    &+ G(b_1,a_1,a_2,b_2;z) + G(b_1,a_1,b_2,a_2;z) + G(b_1,b_2,a_1,a_2;z)\,.
\end{align}
Therefore, the shuffle relations allow us to write a product of GPLs of weight $|\vec{a}|$ and $|\vec{b}|$ as a GPL of weight $|\vec{a}|+|\vec{b}|$. The symbol itself also satisfies the shuffle algebra:
\begin{equation} \label{eq:symbolshuffle}
    (w_1 \oo \ldots \oo w_n) \times (v_1 \oo \ldots \oo v_m) = \sum_{\vec{z}=\vec{w} \shuffle \vec{v}} (z_1 \oo \ldots \oo z_{n+m})\,.
\end{equation}
For example, 
\begin{align}
    \SS(\log(z) \Li_2(z)) &= (z) \times (-(1-z)\oo z) \nonumber \\
    &= -z \oo (1-z) \oo z - 2 (1-z) \oo z \oo z \\
    &\textcolor{gray}{= -\SS[z, 1-z, z] -2 \SS[1-z, z, z]} \,, \nonumber
\end{align}
where in the last line we used the alternative notation mentioned above in order to make absolutely clear the position of the minus sign and a constant\JK{Is the last line actually correct???}.

Overall, we see that thanks to symbols of the classical polylogarithms Eqs.~\ref{eq:Lisymbols} and GPLs Eq.~\ref{eq:Gsymbol}, the symbol shuffling Eq.~\ref{eq:symbolshuffle} and finally the relation Eq.~\ref{eq:LiToG}, we can now compute the symbol of each function in the weight-4 basis of Eq.~\ref{eq:weight4basis}. Thinking ahead to our amplitude applications, it should now be clear that the symbol will allow us to find relations between linearly dependent objects such as GPLs and express our results in a much more compact form. Having expended the effort of understanding the last few pages, let us enjoy the fruits of our labour sprouting in the form of several illustrative examples.

%examples
%First entry condition
%Disc - Simone's thesis
\subsection{An alternative approach to IBP reduction} \label{sec:altibps}
As mentioned earlier, the IBP relations can only provide the reduction of integrals within one family. Therefore, in practice we typically need to build an extensive IBP system which covers all the families in the problem. This often leads to a prohibitively long time required to solve the system. There is, however, a more efficient way of performing the IBP reduction. Recall that the integral families fed into the reduction are defined by their propagators and ISPs. Each family can therefore be identified by its `type' and permutation of external legs within that type. For example, we refer to the topology of Fig.~\ref{fig:pentabox} as $\text{double-box}(12345)$, and to that of Fig.~\ref{fig:hexatriangle} as $\text{hexa-triangle}(13245)$. It is not hard to notice that that if two given families are of the same type and differ only by a permutation $\sigma$ of the external momenta, i.e. $T'=T(\sigma(ijklm))$, then the IBP reduction of any integral Eq.~\ref{eq:ibpintegral} within the permuted family $T'$ can be expressed as a permutation of the reduction of the corresponding integral in the original family $T$:
\begin{align}
    I_{T(ijklm)}(\bm{\nu}) &= \sum_{i=1}^{|\text{MI}_T|} c_i(p, \varepsilon) \times \text{MI}_{T(ijklm),\,i} \\
    \implies I_{T(\sigma(ijklm))}(\bm{\nu}) &= \sum_{i=1}^{|\text{MI}_T|} c_i(\sigma(p), \varepsilon) \times \text{MI}_{T(\sigma(ijklm)),\,i}\,.
\end{align}
\JK{I think the notation here is crap, but I can't think of anything better for now.} \JK{Does this fact need any further justification?} Since the reduction coefficients $c_i$ are rational functions of the $\varepsilon$ and the momenta $p$, permuting the external kinematics to obtain $c_i(\sigma(p), \varepsilon)$ is trivial, whether we express them through the Mandelstam invariants $s_{ij}$ or the momentum twistor variables $\bm{x}$. Moreover, the rationality means that this new approach to IBP reduction is perfectly compatible with the finite field workflow. Permuting the coefficients amounts to a change of variables, which can be implemented over the field\footnote{When working with the $s_{ij}$ variables, the permutation $s_{\sigma(ij)}$ will in general take us outside of the minimal set such as Eq.~\ref{eq:sijs}, but we can always express the new Mandelstam variables through the same minimal set by using momentum conservation. This subtlety is not present at the level of momentum twistor variables.}. Thus, we never have to know and permute the analytic form of the IBP coefficients, which leads to a significant reduction in the computational cost. For an $n$-particle integral family, we only need to consider the IBP system composed of one permutation of this family, rather than up to $n!$ permutations of it. The choice of this main permutation is arbitrary and we will denote it as $\sigma_\text{id}$.

\JK{Should I---and if yes, then where---describe the permutations of $\sqrt{}$`s and $\tr_5$ in the basis definitions?}
As pointed out below Eq.~\ref{eq:noofmis}, the presence of relations other than IBP relations allows us to express some MIs of one family in terms of the MIs of other families. Thus, the total number of MIs is smaller than the sum of MIs in the individual families. This fact is automatically taken care of in the traditional approach to IBP reduction, where all families are treated by one shared IBP system, which also includes inter-family relations. However, in the alternative approach described above, these extra relations are not taken into account, as we are performing the reduction for families $T$ only in their canonical permutations $\sigma_\text{id}$. Thus, the amplitude obtained in this approach will contain MIs which are in fact not fully linearly independent. Strictly speaking, there is no requirement for the amplitude to be expressed in terms of independent objects. However, it is always desirable to do so, as this ensures that any cancellations happen analytically, therefore it avoids introducing instabilities and issues with precision during numerical evaluation of the amplitude. Moreover, it is more computationally efficient, as it reduces the number of independent coefficients that have to be processed when expanding the amplitude into special functions.

In practice, we obtain these missing relations between MIs by performing their reduction in the traditional approach (i.e. all families included together in one system) in \texttt{LiteRed}\footnote{Note that this reduction will not rely on identities of the IBP nature, as inter-family IBP relations do not exist. Instead, it will compare different integrals based on their representation in terms of the $\mathcal{U}$ and $\mathcal{F}$ Symanzik polynomials (see Section 4 of Ref.~\cite{Lee:2012cn} and also Ref.~\cite{Pak:2011xt}).}, which expresses the MIs from all families in terms of the truly independent ones:
\begin{equation}
    \text{MI}_{i} = \sum_j^{|\text{MI}|} f_{ij}(p, \varepsilon) \times \text{MI}_j \qquad i \in \left\{1, \ldots, \sum_T |\text{MI}_T|\right\}\,,
\end{equation}
where $|\text{MI}| = |\text{MI}_{\sum_T T}|$, as defined earlier. This additional reduction does not present such complexity as the reduction needed for the amplitude. Firstly, the identities generated through the Laporta algorithm only need to cover the rank and number of dotted propagators present in the MI bases, rather than the full amplitude. Secondly, all the missing relations between MIs can be worked out just once and they will be applicable to any process at the same loop order and number of particles, since the IBP identities are sensitive only to the kinematic setup. Finally, an extremely useful property of UT integrals is that any inter-family relations between them cannot contain any dependence on the kinematic variables, i.e. $f(p, \varepsilon) \rightarrow f(\varepsilon)$. Thus, when using UT bases, this additional reduction can be performed with the kinematic variables set to random values (subject to momentum conservation). Having obtained the missing relations between MIs, we apply them to our amplitude and collect the coefficients of the true, linearly independent MIs. In the end, this is equivalent to performing the traditional IBP reduction of all families considered together simultaneously, but much more efficient.
\subsection{Syzygy relations} \label{sec:syzygies} 
%
%\section{Special functions} \label{sec:specialfunctions}
%\subsection{Pentagon functions} \label{sec:Pentagon functions}
%\subsection{Generalised series expansion} \label{sec:DiffExp}
%\subsection{Auxiliary mass flow} \label{sec:AMFlow}

%\section{Finite remainder} \label{sec:finrem}
%\section{Reconstruction} \label{sec:reconstruction}
%Describe the various reconstruction tricks?

\end{document}