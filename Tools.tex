\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Tools for calculating scattering amplitudes} \label{sec:tools}
In this chapter, we focus on various aspects of the computation of two-loop QCD scattering amplitudes for high-multiplicity processes. It is important to note that there is currently no one-size-fits-all approach that would allow us to compute all the desired amplitudes at a press of a button. In practice, we use a collection of methods that are most appropriate for the task at hand. For processes at the limit of current capabilities, these tools need to be further improved or replaced with novel ideas. To this end, much work has been done by the theory community in recent years. Unfortunately, due to the overwhelming algebraic and analytic complexity, many calculations still present challenges beyond the reach of current technology. 
\begin{figure}[t]
\begin{tikzpicture}
	\node (feynman) [greenrec] {Feynman diagrams};
	\node (colour)  [redrec,right of=feynman,xshift=4cm] {Colour decomposition};
	\node (topos)   [redrec,right of=colour,xshift=4cm] {Helicity amplitudes};
	\node (reduction) [bluerec,below of=topos,yshift=-2cm] {\parbox{0.3\textwidth}{\centering Integrand reduction onto \\ maximal topologies}};
	\node (IBPs)    [bluerec,left of=reduction,xshift=-4cm] {IBP reduction};
	\node (spfns)   [bluerec,left of=IBPs, xshift=-4cm] {\parbox{0.3\textwidth}{\centering Expansion of MIs onto \\ special function basis}};
	\node (sub)     [bluerec,below of=spfns,yshift=-2cm] {Pole subtraction};
	\node (finrem)  [bluerec,right of=sub,xshift=4cm] {Finite remainder};
	\node (rec)  [bluerec,right of=finrem,xshift=4cm] {Reconstruction};
	\node (qgraf) [below of=feynman, opacity=0.7] {\textcolor{green}{\Large QGRAF}};
	\node (mma) [below right of=colour, xshift=1.5cm, yshift=-0.3cm, opacity=0.7] {\textcolor{red}{\Large Mathematica/FORM}};
	\node (ff) [below of=IBPs, yshift=-0.5cm,opacity=0.7] {\textcolor{blue}{\Large finite fields}};
		
	\draw[->] (feynman.east) -- (colour.west);
	\draw[->] (colour.east) -- (topos.west);
	\draw[->] (topos.south) -- (reduction.north) node[midway,right] {$d=4-2\epsilon$};
	\draw[->] (reduction.west) -- (IBPs.east);
	\draw[->] (IBPs.west) -- (spfns.east);
	\draw[->] (spfns.south) -- (sub.north);
	\draw[->] (sub.east) -- (finrem.west) node[midway,above] {$\epsilon \rightarrow 0$};
	\draw[->] (finrem.east) -- (rec.west);
\end{tikzpicture}
\caption{A schematic overview of the workflow we adopt to compute scattering amplitudes.}
\label{fig:outline}
\end{figure}
The goal of this chapter is to provide an overview of the method we adopt in amplitude computations, as well as the problems that invariably follow. The procedure involves several highly non-trivial steps. To help the reader retain the `big picture' of the workflow, we present its schematic outline in Fig.~\ref{fig:outline}.  Each step is discussed in more detail below. 

\section{Feynman diagrams}
The starting point of our amplitude computation for a given process is the generation of all Feynman diagrams contributing to this process at the desired loop order. Feynman diagrams provide a pictorial representation of the ways in which the interaction can occur.
%time-ordered correlation functions that contribute to the $S$-matrix element
At the same time, the corresponding mathematical expressions can be easily recovered using Feynman rules, which can be derived form the Lagrangian of the theory under consideration 
%\JK{(Feynman rules relevant to our work are listed in Appendix~\ref{app:feynmanrules})}.
As such, these diagrams are an indispensable tool of any perturbative calculation. In practice, it can be observed that usually their number grows faster than exponentially as we increase the loop order or multiplicity (see Table~\ref{tab:ndiags} for an example). To handle the combinatorial complexity, we generate the relevant Feynman diagrams using $\texttt{QGRAF}$~\cite{Nogueira:1991ex}. This programme has the advantage of granting the user a large degree of control over the diagrams. For example, one can constrain it to generate diagrams without self-energy insertions or with a specified total power of the coupling constant. \JK{should I even mention that?}
\begin{table}[b]
	\begin{center}
		\begin{tabular}{r|c|c|c|c|c|c|c|c}
			  $n$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
			\hline
			$n$ gluons   & -- & -- & 1 & 4 & 25 & 220 & 2485 & 34300 \\
			$q\bar{q} + n$ gluons & 1 & 3 & 16 & 123 & 1240 & 15495 & 231280 & 4016775 \\
		\end{tabular}
	\end{center}
 \caption{Number of tree-level diagrams contributing to selected processes with $n$ gluons.}
 \label{tab:ndiags}
\end{table}

\section{Colour decomposition} \label{sec:colourdec}
Having generated the Feynman diagrams, we substitute the Feynman rules for the propagators and vertices using \texttt{Mathematica}. At this point, our QCD amplitude contains both colour and kinematic information. The idea of colour ordering is to reorganise the amplitude such that these two components separate: a purely kinematic amplitude is multiplied by the corresponding colour factor. In other words, we perform the decomposition of the full amplitude in colour space, according to a chosen colour basis. Roughly speaking:
\begin{equation}
    \ampl{n}{} = \sum_i \text{(colour)}_i \times A_{n\,i} \,, 
\end{equation}
where $A_{n\,i}$ are the colour-ordered amplitudes (also known as colour-stripped or partial amplitudes). The motivation behind this decomposition is that the colour-ordered amplitudes turn out to be significantly simpler to calculate.

The choice of the colour basis is not unique. We adopt the decomposition according to traces of the $SU(N_c)$ generators in the fundamental representation. As an example, let's look at the 4-gluon scattering at tree-level\JK{Redo this diagram using tikzfeynman}:
\begin{equation} \label{eq:3grule}
\feynmandiagram [baseline = (i.base), horizontal = i to j] {
    a1 [particle={$A^{a_1}_{\mu_1}$}] -- [gluon, momentum=$p_1$] i;
    a2 [particle={$A^{a_2}_{\mu_2}$}] -- [gluon, momentum=$p_2$] i;
    i -- [gluon] j;
    a3 [particle={$A^{a_3}_{\mu_3}$}] -- [gluon, momentum=$p_3$] j;
    a4 [particle={$A^{a_4}_{\mu_4}$}] -- [gluon, momentum=$p_4$] j;
    };
    \xrightarrow{colour}
    f^{a_1a_2b}f^{ba_3a_4} \,.
\end{equation}
The colour factor of this diagram can be expressed in terms of the generators using Eqs.~\ref{eq:fierzcolour} and \ref{eq:fabc}:
\begin{align}
    f^{a_1a_2b}f^{ba_3a_4} = -\frac{1}{T_F} &\big(\tr[T^{a_1} T^{a_2} T^{a_3} T^{a_4}] - \tr[T^{a_1} T^{a_2} T^{a_4} T^{a_3}] \\
    & - \tr[T^{a_1} T^{a_3} T^{a_4} T^{a_2}] + \tr[T^{a_1} T^{a_4} T^{a_3} T^{a_3}]\big) \,.
\end{align}
The colour factors of the $t$- and $u$-channels can be expressed in a similar way. Combining the contributions from the three channels and using the cyclicity of the trace, we can organise the 4-gluon amplitude at tree-level as follows:
\begin{equation}
    \ampl{4}{(0)} =  g^2 \left( \tr[T^{a_1} T^{a_2} T^{a_3} T^{a_4}]\,A_{4}^{(0)}[1234] + \text{permutations of } (234) \right) \,.
\end{equation}
In the general case, this formula reads:
\begin{equation} \label{eq:colour-decomposition}
    \ampl{n}{(0)} = g^{n-2} \sum_{\sigma \in S_{n-1}} \tr\left[T^{a_1} T^{\sigma(a_2} \ldots T^{a_n)}\right] A_n^{(0)} \left[1\,\sigma(2\ldots n)\right] \, ,
\end{equation}
where the sum is over the set of all \textit{non-cyclic} permutations of $n-1$ particles. Similar colour decompositions can be derived for amplitudes involving quarks, as well as beyond tree-level~\cite{Dixon:1996wi}. At loop-level, the colour basis contains products of traces, in addition to single trace structures. 

The colour-ordered amplitudes are calculated by adding up the kinematic parts of all Feynman diagrams contributing to a given colour factor. They are gauge invariant and satisfy a number of important identities:
\begin{align}
    A_n[123\ldots n] &= A_n[23\ldots n\,1]\,, && \text{cyclicity} \\
    A_n[123\ldots n] &= (-1)^n A_n[n \ldots 231] \,, && \text{reflection} \\
    A_n[123 \ldots n] &+ A_n[213 \ldots n] +  A_n[231 \ldots n] +  \ldots + && A_n[23 \ldots 1\,n]  = 0 \,, \nonumber \\ 
    & && U(1) \text{ decoupling} \\
    A_n[1, {\alpha}, n, {\beta}] &= (-1)^{|\beta|} \sum_{\mathclap{\sigma \in OP(\{\alpha\} \cup \{\beta^T\})}} A_n[1, \sigma, n] \,, && \text{Kleiss-Kuiff relations}
\end{align}
\JK{fix alignment!!!} where the sum is over permutations in the joint set $\{\alpha\} \cup \{\beta^T\}$ such that the order within the individual sets is preserved, and $\{\beta^T\}$ is the reversal of the set $\{\beta\}$~\cite{Mangano:1990by, Kleiss:1989616, Bern:2008qj}. Crucially, these properties allow us to reduce the number of independent amplitudes that need to be computed. In fact, for $n$-gluon scattering, this number is just $(n-2)!$\,.

\section{Helicity amplitudes}
After colour decomposition, our $L$-loop scattering amplitude contains purely kinematic information. The kinematic part of the Feynman rules for external states carries information about the spins and polarisations of particles: for massless spin\=/1/2 fermions, we use $\pm$ helicity states to differentiate between the two solutions to the Dirac equation, while for massless spin\=/1 bosons, they denote the two polarisation vectors. From the experimental perspective, we are rarely interested in differentiating between the spin states of individual particles. Usually, a beam of particles with random spins undergoes scattering and we look at the total number of particles outgoing in a certain direction. Thus, to calculate the corresponding cross-section, we should average over the initial spin states and sum over the final ones. This can be achieved in two different ways: 
\begin{enumerate}
    \item perform the amplitude calculation without specifying the helicity states, i.e. square the amplitude, then do the spin sums that appear at the level of $|\ampl{}{}|^2$ using completeness relations of Eqs.~\ref{eq:completeness:fermions} and \ref{eq:completeness:bosons}
    \item specify the helicity states of external particles, compute each \textbf{helicity amplitude} separately, square them and sum over all relevant helicity configurations
\end{enumerate}
In our computations, we will adopt the latter method. We denote an $L$-loop helicity amplitude as:
\begin{equation} \label{eq:helampdef}
    \ampl{n}{(L),\,\{h\}} \equiv \ampl{n}{(L)}(1^{h_1}2^{h_2} \ldots n^{h_n})\,,
\end{equation}
where the superscript $\{h\}$ is understood as the set of helicities of the $n$ particles, but will be usually omitted since we will exclusively compute helicity amplitudes. The full, spin-summed amplitude can then be recovered through:
\begin{equation}
    \ampl{n}{(L)} = \sum_{\mathclap{\substack{\text{helicity} \\ \text{configurations}}}} \ampl{n}{(L),\,\{h\}} \,.
\end{equation}
At the cross-section level:
\begin{equation}
    \left|\ampl{n}{(L)}\right|^2 = \sum_{\mathclap{\substack{\text{helicity} \\ \text{configurations}}}} \left|\ampl{n}{(L),\,\{h\}} \right|^2 \,.
\end{equation}
It is important to note that the sum only includes the squares of individual helicity amplitudes --- there are no interferences between different helicity configurations.

There are several strong advantages to this approach. Firstly, it is easy to see that the number of terms that need to be processed is significantly smaller than in method~(1) \JK{check with SB if the argument below even makes sense...}. Consider the expansion of an amplitude in the coupling constant $\alpha$ up to NNLO:
\begin{equation}
    \ampl{}{} = \ampl{}{(0)} + \alpha \ampl{}{(1)} + \alpha^2 \ampl{}{(2)} + \order{\alpha^3} \,.
\end{equation}
Then, at the level of the cross-section, we have the following contributions:
\begin{equation}
    |\ampl{}{}|^2 = |\ampl{}{(0)}|^2 \,+\,\alpha\,2\,\mathrm{Re}\left(\ampl{}{(0)\ast}\ampl{}{(1)}\right) \,+\, \alpha^2 \left(2\,\mathrm{Re} \left(\ampl{}{(0)\ast}\ampl{}{(2)}\right) + |\ampl{}{(1)}|^2 \right) \,+\, \order{\alpha^3}\,.
\end{equation}
Let us also schematically write each $L$-loop amplitude as a sum of $m_L$ Feynman diagrams: $\ampl{}{(L)} = d^{(L)}_1 + d^{(L)}_2 + \ldots + d^{(L)}_{m_L}$. Then, at LO:
\begin{equation}
    \left|\ampl{n}{(0)}\right|^2 = \sum_{\mathclap{\substack{\text{hel.} \\ \text{confs.}}}} \left|\ampl{n}{(0),\,\{h\}} \right|^2 = \sum_{\mathclap{\substack{\text{hel.} \\ \text{confs.}}}} \left|d^{(0)\,,\{h\}}_1 + d^{(0)\,,\{h\}}_2 + \ldots + d^{(0)\,,\{h\}}_{m_0} \right|^2 \,.
\end{equation}
Each term in this sum has all helicities fixed and there are no spin sums to be performed (when evaluated at a chosen phase-space point, it is just a complex number). Thus, the number of terms we need to process at LO is $m_0 n_h$, where $n_h$ is the number of independent helicity configurations. On the other hand, according to method (1), we have:
\begin{equation}
    \left|\ampl{n}{(0)}\right|^2 = \left(d^{(0)}_1 + d^{(0)}_2 + \ldots + d^{(0)}_{m_0} \right) \left(d^{(0)\ast}_1 + d^{(0)\ast}_2 + \ldots + d^{(0)\ast}_{m_0} \right)\,,
\end{equation}
which means we need to interfere the diagrams with each other and perform the spin sums. Thus, there are $m_0^2$ terms to be processed. The scaling for higher loop orders is listed in Table~\ref{tab:nterms}. For small $L$, the advantage of using helicity amplitudes might be minimal (or in fact, it might be detrimental to do so). For $L\geq2$, however, the advantage becomes apparent, especially that due to the symmetries of colour-ordered amplitudes, $n_h$ is usually much smaller than $2^n$. Moreover, it turns out that not all helicity amplitudes are equally challenging to compute, as we will see in the next section. In fact, a host of them vanishes altogether (at least at tree-level). Finally, guided by experience, it is possible to choose the reference vectors of external polarisations such that the computation of non-zero amplitudes becomes easier. 

When dealing with helicity amplitudes, it is conventional to introduce nomenclature encoding the number of positive/negative helicity particles involved in the process. Consider $2\rightarrow n$ gluon scattering where the outgoing momenta all have opposite helicities to the incoming ones: $1^-2^- \rightarrow 3^+ \ldots n^+$. We call such a configuration `helicity violating'. We can cross particles 1 and 2 to the final state, which changes their helicities: $0\rightarrow 1^+2^+ \ldots n^+$. This corresponds to the `all-plus' amplitude $A_n(1^+2^+ \ldots n^+)$ with all momenta outgoing. In the next section, we will show that for gluon scattering at tree-level, this amplitude vanishes for all $n$. If we flip one helicity in the final state: $1^-2^- \rightarrow 3^- \ldots n^+$, this corresponds to $A_n(1^+2^+3^-4^+ \ldots n^+)$, which also turns out to vanish at tree-level. The first non-zero configuration is $1^-2^- \rightarrow 3^-4^- \ldots n^+$, which corresponds to: $A_n(1^+2^+3^-4^-5^+ \ldots n^+)$. For this reason, amplitudes with exactly two negative helicity particles are called \textbf{maximally helicity violating (MHV)}. Similarly, amplitudes with exactly two positive helicities are known as anti-MHV. Furthermore, configurations with $2+k$ negative/positive helicities are referred to as $\text{N}^k\text{MHV}/\text{anti-N}^k\text{MHV}$. Tree-level MHV amplitudes are remarkably simple, as we will demonstrate in the next section. 
\begin{table}[t]
	\begin{center}
		\begin{tabular}{c|c|c|c}
			  \# terms & LO & NLO & NNLO \\
			\hline
			Method (1) & $m_0^2$ & $m_0^2 + m_0 m_1$ & $m_0^2 + m_0 m_1 + m_1^2 + m_0 m_2$ \\
			Method (2) & $m_0 n_h$ & $(m_0+m_1)n_h$ & $(m_0+m_1+m_2)n_h$ \\
		\end{tabular}
	\end{center}
 \caption{Number of terms to be processed in the computation of the squared amplitude $\left|\ampl{}{}\right|^2$ \textit{up to and including} a given order in the coupling constant, assuming there are $m_L$ diagrams at $L$ loops. The meaning of methods (1) and (2) is outlined in the text around Eq.~\ref{eq:helampdef}.}
 \label{tab:nterms}
\end{table}
\section{Spinor helicity formalism} \label{sec:spinhelform}
In Section~\ref{sec:QEDintro}, we saw that for massless particles, the Dirac spinor splits into two Weyl spinors that do not mix and are associated with the helicity of the particle. Therefore, we might be tempted to think that helicity amplitudes are better described using a notation specific to the two-component Weyl spinors, which are acted on by the familiar Pauli matrices. Indeed, the powerful spinor-helicity formalism provides a neat way to express helicity amplitudes based on these considerations\footnote{In case the notation that follows appears daunting, we refer the reader to Ref.~\cite{ElvangHuang} for an in-depth discussion of the topic and useful exercises.}.

As a first step, let's see how we can move from working with the four-component objects to two-component ones. We can write a `slashed' momentum $\slashed{p}$ as:
\begin{equation} \label{eq:pslashed1}
    \slashed{p} = p_\mu \gamma^\mu = p_\mu
    \begin{pmatrix}
    0 & (\sigma^\mu)_{a\dot{b}} \\
    (\bar{\sigma}^\mu)^{\dot{a}b} & 0
    \end{pmatrix}
    \equiv
    \begin{pmatrix}
    0 & p_{a\dot{b}} \\
    p^{\dot{a}b} & 0 
    \end{pmatrix} \, ,
\end{equation}
with both dotted and un-dotted indices running over $\{1,2\}$ and $(\sigma^\mu)_{a\dot{b}} \equiv (1, \, \sigma^i), \, (\bar{\sigma}^\mu)^{\dot{a}b} \equiv (1, \, -\sigma^i)$, where $\sigma^i$ are the three Pauli matrices. The momentum bispinors $p^{\dot{a}b}$ and $p_{a\dot{b}}$ can be thought of as $(2 \times 2)$ matrices and it is straightforward to show that:
\begin{equation}
    \det p_{a\dot{b}} = \det p^{\dot{a}b} = m^2\,.
\end{equation}
For massless particles, this determinant vanishes and the matrix can be expressed as an outer product of two vectors\footnote{The determinant of a matrix is 0 only if its column/row vectors are linearly dependent, which implies that its rank is 1. A rank-1, $(n \times n)$ matrix can always be expressed as the outer product of two nonzero vectors of length $n$.}. The vectors we will choose are the momentum space Weyl spinors $\lambda_a$  and $\tilde{\lambda}_{\dot{a}}$ (sometimes referred to as helicity spinors). They are the two-component, left- and right-handed equivalents of the $u(p)$ and $v(p)$ Dirac spinors, with the corresponding helicities $-$ and $+$, respectively. We thus write:
\begin{align} \label{eq:outerproduct}
    p_{a\dot{b}} = \lambda_a \tilde{\lambda}_{\dot{b}} && p^{\dot{a}b} = \tilde{\lambda}^{\dot{a}} \lambda^b\,,
\end{align}
and the raising and lowering of indices is achieved through:
\begin{align}
    \lambda^a = \eps^{ab} \lambda_b && \tilde{\lambda}^{\dot{a}} = \eps^{\dot{a}\dot{b}} \lambda_{\dot{b}} \,,
\end{align}
with the two-dimensional Levi-Civita tensor defined as:
\begin{equation}
    \eps^{ab} = \eps^{\dot{a}\dot{b}} = -\eps_{ab} = -\eps_{\dot{a}\dot{b}} = 
    \begin{pmatrix}
        0 & 1 \\
        -1 & 0
    \end{pmatrix}\,.
\end{equation}
In practice, it can be rather cumbersome to keep track of the dotted and un-dotted indices, as well as their lower or upper positions at the spinors. It is more intuitive to trade this notation for spinor brackets\footnote{The choice of assignment of dotted/un-dotted indices and angle/square brackets to either $\lambda$ or $\tilde{\lambda}$ is arbitrary. Different conventions are seen throughout literature - the only requirement is internal consistency.}:
\begin{align} \label{eq:lambdatobraket}
    \lambda_a  &\rightarrow \ketsq{p}_a,  &&\tilde{\lambda}_{\dot{a}} \rightarrow \bra{p}_{\dot{a}}\,, \nonumber \\
    \lambda^a  &\rightarrow \brasq{p}^a  &&\tilde{\lambda}^{\dot{a}} \rightarrow \ket{p}^{\dot{a}} \,.
\end{align}
We can then write Eq.~\ref{eq:pslashed1} as:
\begin{align} \label{eq:pslashed2}
    \slashed{p} = \ket{p}\brasq{p} && \slashed{p} = \ketsq{p} \bra{p} \,,
\end{align}
while the massless Dirac equation becomes the massless Weyl equation:
\begin{align} \label{eq:weyleq}
    \slashed{p}\ket{p} = 0 && \slashed{p}\ket{p} = 0\,.
\end{align}
In the above, $\slashed{p}$ is a small abuse of the `slashed' notation --- what it really means is a contraction of $p_\mu$ with $\sigma^\mu$ or $\bar{\sigma}^\mu$, rather than $\gamma^\mu$. The appropriate Lorentz vector can be chosen by looking at the indices of the square/angle spinors that $p^\mu$ is sandwiched between. However, the power of spinor-helicity formalism lies in the fact that in practice, we do not ever need to perform such explicit summation over indices and instead work with identities at the level of angle and square brackets:
\begin{align}
    \braket{ij} \equiv \bra{i}_{\dot{a}} \ket{j}^{\dot{a}}&& \braketsq{ij} \equiv \brasq{i}^a \ketsq{j}_a \,.     
\end{align}
It is straightforward to show that because the indices are raised and lowered using the Levi-Civita symbol, these brackets must be \textit{antisymmetric}:
\begin{align}
    \braket{ij} = -\braket{ji} && \braketsq{ij} = -\braketsq{ji}\,.
\end{align}
We can also formulate angle-square or square-angle brackets as follows:
\begin{align}
    \langle ikj ] \equiv \bra{i} \slashed{k} \ketsq{j} && [ikj\rangle \equiv \brasq{i} \slashed{k} \ket{j}\,.
\end{align}
To choose the right $\slashed{k}$ from Eq.~\ref{eq:pslashed2}, we just need to remember that $\langle ik] = [ik\rangle = 0$. These brackets can be extended to arbitrary lengths by inserting additional slashed momenta inside. 

We note some very useful identities\footnote{Naturally, the Schouten, Gordon and Fierz identities also hold if we exchange all angle and square brackets. Wherever $\gamma^\mu$ appears, it is understood as either $\sigma^\mu$ or $\bar{\sigma}^\mu$, as explained earlier.}:
\begin{subequations} \label{eq:spinorsids}
    \begin{align}
        \braket{ii} = \braketsq{ii} = 0 && \text{by antisymmetry} \label{eq:iieq0} \\
        s_{ij} = -\braket{ij}\braketsq{ij} && \text{(for massless momenta)} \\
        \braketsq{ij}^{\ast} = \braket{ji} && \text{(for real momenta)} \label{eq:complexconjbracket} \\
        \sum_{i=1}^n \ketsq{i}\bra{i} = \sum_{i=1}^n \ket{i}\brasq{i} = 0 && \text{momentum conservation} \label{eq:spinmomcons}\\
        \ket{i}\braket{jk} + \ket{j}\braket{ki} + \ket{k}\braket{ij} = 0 && \text{Schouten identity} \label{eq:schouten} \\
        \bra{i}\gamma^\mu \ketsq{i} = 2p_i^\mu && \text{Gordon identity} \label{eq:gordon} \\
        \bra{i} \gamma^\mu \ketsq{j} = \brasq{j}\gamma^\mu \ket{i} && \\
        \bra{i}\gamma^\mu \ketsq{j}^{\ast} = \bra{j}\gamma^\mu \ketsq{i} && \text{(for real momenta)} \\
        \bra{i}\gamma^\mu \ketsq{j} \bra{k}\gamma_\mu \ketsq{l} = -2\braket{ik}\braketsq{jl} && \text{Fierz identity} \label{eq:fierz} \,.
    \end{align}

\end{subequations}

In addition to massless fermions, we need to be able to write the polarisation vectors of massless spin\=/1 bosons in the spinor-helicity language. In analogy to Eq.~\ref{eq:outerproduct}, the Coulomb gauge identity $\eps_{\pm} \cdot \eps_{\pm} = 0$ \JK{Is this only true in Coulomb gauge?} allows us to decompose $\eps^\mu$ into an outer product of two vectors. To this end, we introduce a reference vector $q^\mu$ and write \JK{double check the signs}:
\begin{align} \label{eq:pols}
    \eps^\mu_{-}(p, q) = \frac{\bra{p}\gamma^\mu\ketsq{q}}{\sqrt{2}\braketsq{pq}} && \eps^\mu_{+}(p, q) = \frac{\bra{q}\gamma^\mu\ketsq{p}}{\sqrt{2}\braket{qp}} \,.
\end{align}
The choice of the reference vector is arbitrary (apart from the condition $q^\mu \neq p^\mu$), which reflects gauge invariance. We can see this by noting that the Weyl spinors are two-component object and can be decomposed as $\ket{r} = \frac{\braket{rq}}{\braket{pq}}\ket{p} - \frac{\braket{rp}}{\braket{pq}}\ket{q}$. Therefore, any shift in $q$ must be of the form $q \rightarrow Aq + Bp$, where $A, B$ are constants. Then, from Eq.~\ref{eq:pols} (and using Eq.~\ref{eq:gordon}), it is easy to see that this shift will correspond to $\eps^\mu \rightarrow \eps^\mu + Cp^\mu$. Thus, the Ward identity $p_\mu \ampl{}{^\mu}$ is satisfied for any choice of the reference vector $q^\mu$. In practice, it is useful to choose it such that contracting $\eps^\mu$ with external momenta leads to the formation of vanishing spinor brackets, thus greatly simplifying the algebra. Finally, the validity of Eq.~\ref{eq:pols} can be verified by observing that these expressions obey all identities we would expect from a polarisation vector \JK{I don't like this}:
\begin{subequations}
    \begin{align}
        p \cdot \eps_\pm(p, q) &= 0 \\
        q \cdot \eps_\pm(p, q) &= 0 \\
        \eps_\pm (p, q) \cdot \eps_\pm(p', q) &= 0 \\
        \eps_\pm (p, q) \cdot \eps_\mp(p', p) &= 0 \\
        \eps_\pm^\ast (p, q) \cdot \eps_\pm(p, q) &= -1
    \end{align}
\end{subequations}
At this point, we have all the tools we need to express any amplitude of massless fermions and spin\=/1 bosons through the angle and square brackets. Its usefulness, however, may not be immediately clear, especially given the notation which at first appears daunting. As a quick demonstration of the power of spinor-helicity formalism, let us consider the special case of 3-particle kinematics. For any three massless momenta satisfying $p_1^\mu + p_2^\mu + p_3^\mu = 0$, we have:
\begin{equation}
    \braket{12}\braketsq{21} = s_{12} = p_3^2 = 0\,.
\end{equation}
Thus, either $\braket{12} = 0$ or $\braketsq{12} = 0$. If we assume $\braket{12}$ is non-vanishing, then by momentum conservation and the massless Weyl equation:
\begin{equation}
    \braket{12}\braketsq{23} = \langle 1|\slashed{2}\ketsq{3} = -\langle 1|(\slashed{1}+\slashed{3})\ketsq{3} = 0\,.
\end{equation}
Thus, $\braketsq{23} = 0$ and in an analogous manner, we can show that $\braketsq{13} = 0$ as well. Had we assumed $\braketsq{12} \neq 0$, we would have found that all angle brackets vanish instead:
\begin{equation}
    \braketsq{12} = \braketsq{13} = \braketsq{23} = 0 \qquad \text{or} \qquad \braket{12} = \braket{13} = \braket{23} = 0 \,.
\end{equation}
Therefore, for 3-particle kinematics, the amplitude must depend on either square or angle brackets only. However, note that this result makes sense only if we work with complex momenta. Otherwise, through Eq.~\ref{eq:complexconjbracket}, the angle and square brackets are complex conjugates of each other and so both types must vanish simultaneously. Amplitudes constructed from complex momenta are of course not physical, nonetheless they provide a useful building block for higher-point amplitudes in recursive techniques \JK{reference to BCFW? Maybe I should have a mini-section on BCFW when talking about the tree amps for $\ppbbh$}.
\subsection{Little group scaling} \label{sec:littlegroup}
In the previous section, we saw that the freedom in choosing the reference vector $q^\mu$ reflects gauge invariance of the amplitude. Here we will see how another physical principle places strong constrains on the form of helicity amplitudes. We begin by observing that when trading the Weyl spinors for the bracket notation, there is some freedom in how exactly we write down Eq.~(\ref{eq:lambdatobraket}). That is, note that both $p_{a\dot{b}} = \ketsq{p}_a \bra{p}_{\dot{b}}$ and $p^{\dot{a}b} = \ket{p}^{\dot{a}} \brasq{p}^b$ are invariant under the transformation:
\begin{align} \label{eq:littlegroupscaling}
    \ket{i} \rightarrow z \ket{i} && \ketsq{i} \rightarrow z^{-1} \ketsq{i}, \qquad z \in \mathbb{C}  
\end{align}
This is known as little group scaling. Each external momentum has its own little-group transformation. This implies the following relations for spin-$1$ massless boson polarisations in Eq.~(\ref{eq:pols}):
\begin{align}
    \eps^\mu_- (p,q) \rightarrow z^2 \eps^\mu_- (p,q) &&  \eps^\mu_+ (p,q) \rightarrow z^{-2} \eps^\mu_+ (p,q) \,.
\end{align}
Note that the polarisation vectors are invariant under re-scalings of the reference momenta. Thus, for scattering of massless particles, the little-group scaling of the corresponding amplitude is determined by the helicities of the external particles. Specifically, if we re-scale the spinor brackets associated with the momentum $p_i$, the amplitude scales according to:
\begin{equation}
    A_n \left(\ldots, \{p_i, h_i\}, \ldots \right) \xrightarrow[\ketsq{i} \rightarrow z_i^{-1}\ketsq{i}]{\ket{i} \rightarrow z_i\ket{i}} z_i^{-2h_i}  A_n \left(\ldots, \{p_i, h_i\}, \ldots \right),
\end{equation}
where $h_i=\pm\frac{1}{2}$ for fermions and $h_i=\pm1$ for massless spin\=/1 bosons. It turns out that this property places a strong constraint on the spinor bracket expression of the amplitude. We will consider tree-level 3-gluon scattering as a basic example. We have already seen that for 3-particle kinematics, the amplitude must be written in terms of angle \textit{or} square brackets only, but we do not know the general form of the expression. Let us re-scale all three momenta with separate shifts $z_1, z_2, z_3$ in an MHV configuration. Then:
\begin{equation}
    A_3^{(0)} (1^-2^-3^+) \rightarrow z_1^2 z_2^2 z_3^{-2} A_3^{(0)}(1^-2^-3^+) \,.
\end{equation}
Assuming that this amplitude depends only on angle brackets:
\begin{equation} \label{eq:3gscaling}
    A_3^{(0)} (1^-2^-3^+) \propto \braket{12}^{x_{12}} \braket{13}^{x_{13}} \braket{23}^{x_{23}} \,,
\end{equation}
Using Eq.~\ref{eq:littlegroupscaling}, we solve for the exponents and get $\{x_{12} = 3, x_{13} = -1, x_{23} = -1\}$. An identical exercise can be performed (assuming square brackets this type) for the anti-MHV amplitude $A_3^{(0)} (1^+2^+3^-)$, leading to an analogous result. Therefore, the 3-point gluon amplitudes are fixed (up to an overall constant) by the special 3-point kinematics and little group scaling:
\begin{align} \label{eq:3gMHV}
    A_3^{(0)} (1^-2^-3^+) = \kappa_1 \frac{\braket{12}^3}{\braket{23}\braket{31}} && A_3^{(0)} (1^+2^+3^-) = \kappa_2 \frac{\braketsq{12}^3}{\braketsq{23}\braketsq{31}} \,.
\end{align}
We also note that flipping all helicities corresponds to exchanging $\braket{\phantom{p}} \leftrightarrow \braketsq{\phantom{p}}$. With a wrong choice of the bracket type in Eq.~\ref{eq:3gscaling}, one can show by considering the mass dimension of the amplitude that the couplings $\kappa_1$ and $\kappa_2$ would have to come from terms in the Lagrangian that are non-local\footnote{The mass dimension of $\ampl{n}{}$ in $D=4$ is $4-n$. From Eq.~\ref{eq:pslashed2}, we see that both $\braket{\phantom{i}}$ and $\braketsq$ must have mass dimension 1. Thus, the constants $\kappa_1$ and $\kappa_2$ in Eq.~\ref{eq:3gMHV} have mass dimension 0, which is consistent with the fact that they must have come from the 3-gluon interaction term in the Lagrangian (Eq.~\ref{eq:QCDLagrangian}), \textasciitilde$A^\mu A_\mu \partial^\nu A_\nu$. Had we assumed incorrect bracket types, both constants would need to have dimension 2. Thus, the corresponding term in the Lagrangian would be \textasciitilde$A^\mu A_\mu \frac{\partial^\nu}{\square} A_\nu$, which is non-local (it describes an interaction whose effects become more important with distance).}. We thus reject them as unphysical. Moreover, the same argument can be used to show that the $(---)$ and $(+++)$ configurations cannot have a non-vanishing amplitude:
\begin{align}
    A_3^{(0)} (1^-2^-3^-) = 0 && A_3^{(0)} (1^+2^+3^+) = 0\,.
\end{align}
In fact, the simplicity we have seen so far generalises to higher-point gluon amplitudes. With a smart choice of reference vectors, it can be shown that:
\begin{align}
    A_n^{(0)} (1^-2^- \ldots n^-) = 0 && A_n^{(0)} (1^+2^+ \ldots n^+) = 0\,,
\end{align}
as well as:
\begin{align}
    A_n^{(0)} (1^+2^- \ldots n^-) = 0 && A_n^{(0)} (1^-2^+ \ldots n^+) = 0\,.
\end{align}
The first non-vanishing amplitudes are the MHV/anti-MHV configurations:
\begin{align} \label{eq:ParkeTaylor}
    A_n^{(0)} (1^+2^+\ldots i^- \ldots j^- \ldots n^+) &= \frac{\braket{ij}^4}{\braket{12} \braket{23} \ldots \braket{n1}} \,, \\
    A_n^{(0)} (1^-2^-\ldots i^+ \ldots j^+ \ldots n^-) &= \frac{\braketsq{ij}^4}{\braketsq{12} \braketsq{23} \ldots \braketsq{n1}} \,.
\end{align}
This result is known as the Parke-Taylor formula \cite{Mangano:1990by}. It can be proved inductively using the BCFW recursion relations~\cite{Britto:2004ap, Britto:2005fq}, with the 3-point MHV amplitudes of Eq.~\ref{eq:3gMHV} serving as the starting point. Overall, it is now clear that the spinor-helicity formalism, together with little group scaling and locality of the Lagrangian, produce astonishingly compact results for amplitudes which are traditionally calculated as sums of hundreds or thousands of Feynman diagrams.
\section{Momentum twistors} \label{sec:MTs}
In the previous section, we have seen that the spinor-helicity formalism provides a convenient framework to describe helicity amplitudes. By using spinor brackets, which are intrinsically tied to helicity, we were able to exploit properties of the amplitude to arrive at remarkably neat expressions. Nonetheless, this formalism comes with certain drawbacks. Firstly, note that kinematic identities such as momentum conservation are not automatically satisfied by the spinor brackets. We can in fact use the properties listed in Eq.~\ref{eq:spinorsids} to arrive at a minimal set of these variables. In practice, however, this proves to be cumbersome, especially for high-multiplicity processes. Moreover, the appearance of square roots 
%such as~\ref{eq:delta3} and \ref{eq:delta5}
complicates our computational setup, which will be made clear in the next section. We would therefore like to have a parametrisation of external kinematics which solves both these problems simultaneously.

In recent years, many amplitude computations have exploited variables known as momentum twistors (MTs)~\cite{Hodges:2009hk, Badger:2013gxa, Badger:2016uuq}. As the first step, we define dual-space coordinates $x_i^\mu$ as:
\begin{equation} \label{eq:dualspacedef}
    p_i^\mu = x_i^\mu - x_{i+1}^\mu\,.
\end{equation}
Using the massless Weyl equation (Eq.~\ref{eq:weyleq}) and the slash notation in the sense of Eq.~\ref{eq:pslashed1}, it then follows that:
\begin{equation} \label{eq:incidencerel}
    \brasq{\mu_i} \equiv \bra{i}\slashed{x}_i = \bra{i}\slashed{x}_{i+1}\,,
\end{equation}
The new variables allow us to define the momentum twistors $Z_i^I$:
\begin{equation} \label{eq:momtwistor}
    Z_i^I = 
    \begin{pmatrix}
        \ket{i} \\
        \brasq{\mu_i} 
    \end{pmatrix}\,.
\end{equation}
where the index $I$ is understood as $I=\{\dot{a},a\}$. Since $\slashed{p} = \ket{i}\brasq{i}$, we can also express $\brasq{i}$ in terms of $Z_i^I$. To this end, the dual twistor is defined as:
\begin{equation} \label{eq:dualmomtwistor}
    W_i^I = 
    \begin{pmatrix}
        \ket{\mu_i} \\
        \brasq{i}
    \end{pmatrix} = 
    \frac{\eps^{ABCD} Z_{(i-1)B} Z_{iC} Z_{(i+1)D}}{\braket{i-1, i} \braket{i, i+1}}\,,
\end{equation}
where $\eps^{ABCD}$ is the 4-dimensional Levi\=/Civita symbol. We can then expand this equation and read off the last two components:
\begin{equation}
    \brasq{i} = \frac{\braket{i, i+1}\brasq{\mu_{i-1}} + \braket{i+1, i-1}\brasq{\mu_i} + \braket{i-1, i}\brasq{\mu_{i+1}} }{\braket{i-1, i}\braket{i, i+1}}\,.
\end{equation}
Each momentum twistor $Z_i^I$ has four components, thus the matrix of all twistors has $4n$ entries for $n$\=/particle scattering. However, not all of them are independent. Firstly, momentum twistors are invariant under the 10-dimensional Poincar√© group. Additionally, they exhibit the $U(1)$ symmetry as well, for each particle separately. We can see this from Eq.~\ref{eq:incidencerel}, which implies that under the little group scaling of $\ket{i}\rightarrow t_i\ket{i}$, with $t_i \in \mathbb{C}$, the momentum twistors scale as: $Z_i \rightarrow t_i Z_i$. At the same time, this transformation does not affect the underlying momentum $p_i^\mu$, thus $Z_i$ are defined projectively. Therefore, the number of independent momentum-twistor variables needed to generate the $Z_i$ for $n$\=/particles is\footnote{In amplitude jargon, the term `momentum twistors' most often refers to these $3n-10$ independent variables, rather than the $Z_i$'s themselves. We will also adopt this terminology in subsequent sections.}: $4n-10-n\times1 = 3n-10$. \JK{Make sure to understand the $3n-10$ thing.} \JK{I forgot what it means to be defined projectively.}.

The momentum twistors $Z_i$, together with their dual equivalent $W_i$, serve as a useful way to generate numerical phase-space points according to the following recipe:
\begin{enumerate}
    \item Fill the twistor matrix $Z_i^I, i=1, \ldots, n$ with random integers.
    \item Compute the dual twistor matrix $W_i^I$.
    \item Read off spinors $\ket{i}$ and $\brasq{i}$.
    \item Calculate the momenta according to the Gordon identity, $p^\mu = \frac{1}{2}\brasq{i}\gamma^\mu |i\rangle$.
\end{enumerate}
Phase-space points generated in this manner are complex and rational. We can also populate the twistor matrix with rational functions instead. The corresponding phase-space parametrisation is guaranteed to automatically implement momentum conservation and the Schouten identity. It is also possible to make a specific choice which leads to $\tr_5$ being rational, which is a trick that will be useful in the next section\JK{Why do MTs rationalise $\tr_5$?}. It is not clear, however, how to choose these functions such that the corresponding parametrisation leads to the simplest possible amplitude expressions \JK{Mention how the MTs break symmetries of helicity amplitudes}. We will present two judicious choices in Sections~\ref{sec:Hbb} and \ref{sec:Wyj}\JK{Why do we use different MT parametrisations in these papers?}.

A small drawback of using the momentum-twistor variables is that they lose the phase information carried by the spinor brackets $\ket{i}$ and $\brasq{i}$. This is because in reducing the number of independent variables from $4n$ to $3n-10$, imposing the symmetries essentially fixes the frame in which we evaluate the kinematics. Thus, strictly speaking, only phase-free expressions can be obtained in MT-variables. On the other hand, each helicity amplitude is a `phase-full' quantity (as opposed to the squared, spin-summed amplitude), therefore we need to restore this information at the end of our computation. This can be achieved by multiplying our MT-variable expression by any factor with the same phase content, normalised such that its magnitude is 1 \JK{Apparently that is not true - ask Ryan.}. In practice, we most often choose the spinor-bracket expression of the tree-level amplitude for the corresponding helicity, and divide it by its MT-variable expression.
\section{Finite fields} \label{sec:FF}
In the previous section, we have introduced a new, minimal set of independent variables that automatically implement constraints such as momentum conservation, as well as rationalise all square roots that appear in our kinematics. This is not just an elegant mathematical exercise. It turns out that momentum-twistor variables (henceforth referred to as just momentum twistors, MTs) provide us with a powerful computational framework that goes hand in hand with yet another tool we employ in amplitude computations.
\subsection{Rational numbers} \label{sec:ratnums}
The problem of enormous algebraic expressions plagues almost every calculation in QFT. At the same time, sweeping cancellations often occur, leading to much more compact answers. Indeed, we have already seen how at tree-level the MHV gluon amplitudes can be described by remarkably simple expressions, despite the fact that they come from hundreds, if not thousands, of Feynman diagrams. A key idea that has emerged over the past several years is to avoid this complexity at the intermediate stages of the computation by working with numerical expressions instead~\cite{Peraro:2016wsq, Peraro:2019svx, Klappert:2019emp, Klappert:2020aqs, vonManteuffel:2014ixa, Abreu:2020xvt}. Crucially, the analytic dependence can still be recovered from the numerics at the very end. In this section, we introduce the concept of \textbf{finite fields} and show how it can be used to our advantage.

A finite field is a field with a finite number of elements. We are interested in finite fields of non-negative integers:
\begin{equation} \label{eq:ffdefinition}
    \mathbb{Z}_n = \{0, \ldots, n-1\}\,,    
\end{equation}
where $n$ is referred to as the size of the field. In particular, we will work with fields whose size is a large prime number $p$, as prime fields satisfy many properties which make the corresponding arithmetic especially simple. Basic operations, such as addition, subtraction and multiplication, are defined over $\mathbb{Z}_p$ through the standard modular arithmetic $\text{mod } p$. We can also define a multiplicative inverse $b\in \mathbb{Z}_p$ for all $a\neq0 \in \mathbb{Z}_p$:
\begin{equation}
    a^{-1} \equiv b \mod p \qquad \Longleftrightarrow \qquad ab = 1 \mod p\,.
\end{equation}
In fact, the existence of the inverse for all non-zero $a$ is guaranteed only for prime fields. We can see this by considering the following set:
\begin{equation}
    S = \{a, 2a, 3a, \ldots, (p-1)a\}\,.
\end{equation}
Now, note that for any two integers $x, x'$ such that: $x \neq x' \text{ mod } p$, we have:
\begin{equation}
    a(x-x') \neq 0 \mod p \qquad \qquad (a \neq 0) \,.
\end{equation}
This inequality, however, holds only because $\gcd(a, p) = 1$. It follows that the set $S \text{ mod } n$ contains all unique, non-zero elements of $\mathbb{Z}_p$, one of which must be 1. This proves the existence of the multiplicative inverse for all $a \neq 0$ (it can be calculated using the \textit{extended Euclidean algorithm}). Consequently, we can conclude that rational operations over $\mathbb{Z}_p$ are well-defined. Moreover, it allows us to define a map from rational numbers to the prime field, $\mathbb{Q} \rightarrow \mathbb{Z}_p\,$. For $q=\frac{x}{y} \in \mathbb{Q}\,$: 
\begin{equation}
    q \text{ mod } p = \left(x \times (y^{-1} \text{ mod } p \right) \text{ mod } p\,.
\end{equation}
This map is not invertible, since it maps infinitely many elements of $\mathbb{Q}$ onto the finite set $\mathbb{Z}_p$. Nonetheless, the rational numbers $q$ can be recovered from their image in $\mathbb{Z}_p$ with a very high probability using Wang's algorithm \cite{10.1145/800206.806398, 10.1145/1089292.1089293}. This process is referred to as rational reconstruction. We remark that this algorithm is successful if  $|x|, |y| < \sqrt{p/2}\,$. Therefore, $p$ should be chosen sufficiently large so that it is possible to reconstruct all rational numbers appearing in the problem. However, this defeats the purpose of using finite fields in the first place, which was to keep the size of numbers below a certain bound imposed by modular arithmetic $\text{mod } p\,$. Moreover, from the practical point of view, we want to use the efficiency of machine-size integers, which means we are usually constrained to $p<2^{64}$. Fortunately, rational numbers exceeding such thresholds can be reconstructed without using prohibitively large prime fields. Recall the essence of the Chinese remainder theorem: knowledge of the congruences of an integer $x$ modulo $\{n_1, n_2, \ldots, n_k\}$, where all the $n_i$ are pairwise co-prime, allows us to obtain the congruence of $x$ modulo $n_1n_2\ldots n_k$. The same idea holds even for our map $\mathbb{Q} \rightarrow \mathbb{Z}_p$. Thus, by calculating:
\begin{align}
    q = a_{p_1} &\mod p_1 \nonumber \\
    q = a_{p_2} &\mod p_2 \nonumber \\
    &\vdots \nonumber \\
    q = a_{p_k} &\mod p_k\,,
\end{align}
we can obtain:
\begin{equation}
    q = a_{p_1 p_2 \ldots p_k} \mod (p_1 p_2 \ldots p_k)\,.
\end{equation}
Hence, combining the images of $q$ over several prime fields $\mathbb{Z}_{p_i}$ allows us to use Wang's algorithm on $\mathbb{Z}_{p_1 p_2 \ldots p_k}$ and successfully reconstruct $q$ in $\mathbb{Q}$.
\subsection{Rational functions} \label{sec:ratfuncs}
So far, we have seen how we can exploit finite fields to keep the size of numerical expressions from growing throughout our computation\footnote{An alternative approach would be to use floating-point numbers instead of rational numbers, however this would quickly lead to issues with precision.}. It should not come as a surprise that this concept can be extended to allow for the reconstruction of not only rational numbers, but also rational functions in multiple variables. 

Let us consider the so-called black box interpolation problem. Suppose we have a set of $n$ variables $\mathbf{x}=\{x_1, x_2, \ldots, x_n\}$. These variables will serve as the arguments of a rational function $f(\mathbf{x})$. In general, the analytic form of $f$ is obtained by applying a series of rational operations on $\mathbf{x}$ . We do not know $f$ analytically at any of these steps, however we assume that we have a way of implementing them \textit{numerically} --- this is what we call the `black box'. Specifically, the numerical operations will be done over a prime field $\mathbb{Z}_p$. We start by evaluating the variables $\mathbf{x}$ at random numerical values in $\mathbb{Z}_p$. \JK{are they actually random? Idk how FF chooses the sample points.}We then apply the rational operations represented by $f$, all within the same prime field. After passing through this black box, the result is a number within that field, which we denote as $f(\mathbf{x}) \text{ mod } p$. Therefore, we have obtained one sample point of the analytic result corresponding to the initial values we chose for $\mathbf{x}$.

They key idea of finite field methods is that it is possible to reconstruct the full analytic dependence of $f(\mathbf{x})$, with coefficients of $x_i$ in $\mathbb{Q}$, by sampling it in this manner at multiple points. The first step of this procedure is in essence a linear fit problem. Any multivariate rational function can be written as:
\begin{equation} \label{eq:ratfun}
     R(\mathbf{x}) = \frac{
     \sum_{\bm{\alpha}} a_{\bm{\alpha}} \mathbf{x}^{\bm{\alpha}}
     }{
     \sum_{\bm{\beta}} b_{\bm{\beta}} \mathbf{x}^{\bm{\beta}}
     }\,.
\end{equation}
Here, $a_{\bm{\alpha}}, b_{\bm{\beta}} \in \mathbb{Z}_p$ are coefficients of the multivariate monomials $\mathbf{x}^\alpha$:
\begin{equation}
    \mathbf{x}^\alpha = \prod_{i=1}^n x_i^{\alpha_i}
\end{equation}
and $\bm{\alpha}$ denotes a collective set of exponents $\bm{\alpha} = \{\alpha_1, \alpha_2, \ldots, \alpha_n\}$. It is also useful to define the \textit{total degree} of the monomial as the sum of all its exponents:
\begin{equation}
    \deg (\mathbf{x}^{\bm{\alpha}}) \equiv |\bm{\alpha}| = \sum_{i=1}^n \alpha_i\,.
\end{equation}
In the context of a rational function, the total degree $\degmax(f)$ is understood as the maximal total degree of any of its monomials.

With this representation of $f(\mathbf{x})$ in Eq.~\ref{eq:ratfun}, we can try to reconstruct its analytic dependence from the numerical samples over the prime field $\mathbb{Z}_p$. In a very naive approach, we would construct the most general ansatz covering all possible monomials up to degree $\degmax (f)$. We make this explicit by writing the ansatz as:
\begin{equation}
    R(\mathbf{x}) = \frac{
    \sum\limits_{\bm{\alpha}:\, |\bm{\alpha}| \le u} a_{\bm{\alpha}} \mathbf{x}^{\bm{\alpha}}
    }{
    \sum\limits_{\bm{\beta}:\, |\bm{\beta}| \le v} b_{\bm{\beta}} \mathbf{x}^{\bm{\beta}}
    }\,,
\end{equation}
where we have abbreviated the numerator/denominator degrees as $u=\degmax(\text{num}(f))$ and $v=\degmax(\text{den}(f))$. We can then formulate a system of linear equations in $a_{\bm{\alpha}}, b_{\bm{\beta}}$ by evaluating both the monomials $\mathbf{x}^{\bm{\alpha}}$ in the ansatz and the black box function $f(\mathbf{x})$ at a chosen value $\mathbf{x}_j$ in $\mathbb{Z}_p$:
\begin{align}
    \sum_{\bm{\alpha}} a_{\bm{\alpha}} \mathbf{x}_j^{\bm{\alpha}} - 
    f(\mathbf{x}_j) \sum_{\bm{\beta}} b_{\bm{\beta}} \mathbf{x}_j^{\bm{\beta}} = 0 
&&    
j \in \{1, \ldots, |R(\mathbf{x})|\} \,.
\end{align}
Finding the values of these coefficients requires solving the system using linear algebra methods. In order for the system to close, we need to perform such evaluations on as many sample points as the number of ansatz terms. This is far from optimal, however, since such a generic ansatz grows rapidly with both the degree as well as the number of variables. In fact, one can show that the number of terms present in $R(\mathbf{x})$ is \JK{I took this from SAGEX lectures, not sure if I should do a reference.}:
\begin{equation} \label{eq:naiveansatzlength}
    |R(\mathbf{x})| = 
    \begin{pmatrix}
        u + n \\
        n
    \end{pmatrix}
    +
    \begin{pmatrix}
        v + n \\
        n
    \end{pmatrix}\,.
\end{equation}
Since the time complexity of the corresponding Gaussian elimination is $\order{|R(\mathbf{x})|^3}$, this can prove prohibitively expensive. As an example, in practice we will be dealing with cases such as six-variable functions with $u=30, v=10$, which gives $|R(\mathbf{x})| \approx 2\times 10^6$. Row reducing such a system is simply not feasible \JK{Is this true for us or in general? Maybe on some supercomputer it would be possible. Idk what the corresponding time estimate is.}. Another complication arises due to the fact that in general, even though the black box operations are implemented numerically over finite fields, obtaining each evaluation of $f(\mathbf{x})$ in the field $\mathbb{Z}_p$ might still take a long time due to the number and complexity of these operations. We refer to this as the evaluation time per point.

Overall, it is clear that we need to avoid using such a naive ansatz to interpolate a rational function from its evaluation over finite fields. Besides, in most applications the polynomial degrees $u$ and $v$ are not known \textit{a priori}, so it is difficult to construct an ansatz in the first place. Fortunately, we can make use of more elaborate interpolation methods\footnote{For a detailed description of these methods and their implementation, see~\cite{Peraro:2016wsq}.}. For univariate polynomials, the strategy is based on Newton's polynomial representation~\cite{Abramowitz1965HandbookOM}. This method is particularly useful in cases where the total degree is not known, as it allows for the inclusion of higher-degree terms until their coefficients are found to be 0, at which point the iterative procedure terminates. For univariate rational functions, we distinguish between two further cases based on whether the degrees $u$ and $v$ are known or not. If they are known, it turns out that the naive ansatzing described above performs well enough, as for $n=1$ the ansatz length $|R(x_1)|$ in Eq.~\ref{eq:naiveansatzlength} is sufficiently small to allow for efficient row reduction of the system. Finally, if the degrees are not known, the reconstruction strategy is based on a rational generalisation of Newton's formula known as Thiele's interpolation formula~\cite{Abramowitz1965HandbookOM}. 

Multivariate reconstruction from finite fields can be achieved as well. For multivariate polynomials, it is sufficient to apply the univariate Newton's formula recursively. That is, a multivariate polynomial $P(\mathbf{x})$ is first treated as a univariate polynomial in $x_1$ with coefficients that are polynomials in $x_2, x_3, \ldots, x_n$. These coefficients then become the subject of $(n-1)$-variable reconstruction and so forth, up until $n=1$. Reconstructing multivariate rational functions is significantly more complicated. The strategy is also based on a recursive use of Newton's formula, but with some important modifications such as adding an auxiliary variable to $\mathbf{x}$. We refer the reader to Refs.~\cite{CUYT20111445, Peraro:2016wsq} for details.

Having completed the interpolation through one of the methods above, the only thing left to do is to recover the monomial coefficients in $\mathbb{Q}$ from their images $a_{\bm{\alpha}}, b_{\bm{\beta}} \in \mathbb{Z}_p$ using Wang's algorithm. As explained earlier, if one prime field $\mathbb{Z}_{p_1}$ is not enough, we can always perform the interpolation in another field $\mathbb{Z}_{p_2}$ and combine these results with the Chinese remainder theorem to obtain the interpolation in $\mathbb{Z}_{p_1 p_2}$. In this way, a rational function with arbitrarily large coefficients can be recovered. The reconstruction time can be estimated according to:
\begin{equation} \label{eq:rectimeschematic}
    \text{Reconstruction time} \approx (\text{number of sample points}) \times (\text{evaluation time per point})\,.
\end{equation}
It is of great practical importance to reduce both these factors as much as possible, which renders the reconstruction of increasingly complicated rational functions possible. We will elaborate on this topic in Sections~\ref{sec:Hbb} and \ref{sec:Wyj}.

Let us make three final remarks. Firstly, we emphasise that the reconstructed function is minimal in terms of the numerator and denominator degrees, that is $\gcd \left(\text{num}(f), \text{den}(f) \right) = 1$. Note that this is also needed for the reconstruction ansatz to be unique. Secondly, for functions which are homogeneous, in the sense that they satisfy:
\begin{align}
    f(\lambda \mathbf{x}) = \lambda^u f(\mathbf{x}) && \lambda \in \mathbb{C} \,,
\end{align}
where $u=\deg (f)$, it is possible to reduce the number of variables in the problem by one. This is easy to see if we define an auxiliary function:
\begin{equation}
    \tilde{f}(x_2, \ldots, x_n) \equiv f(x_1=1, x_2, \ldots, x_n)\,.
\end{equation}
Then, requiring the correct scaling gives:
\begin{equation}
    f(x_1, \ldots, x_n) = x_1^u \, \tilde{f}\left(\frac{x_2}{x_1}, \ldots, \frac{x_n}{x_1}\right) \,.
\end{equation}
\JK{Check out Weinzierl's Feynman Integrals, Eq. 2.144}
Thus, we can reconstruct $\tilde{f}$ and restore the homogeneity of $f$ a posteriori. In our applications, even though the functions we will be dealing with are in general not homogeneous, it turns out we can still discard one variable. We will usually set $s_{12} = 1$ and recover its analytic dependence a posteriori through dimensional analysis \JK{I don't think I understand this.} \JK{Are these two methods of discarding a variable the same?}. Finally, recall that in the computation of amplitudes, we often have to deal with the presence of square roots in the kinematics associated with the amplitude. This is a problem because it is not always possible to take a square root of a field element $a \in \mathbb{Z_p}$. Specifically, for a field of size $p>2$, there are only $(p+1)/2$ so-called quadratic residues, i.e. solutions to this equation $x^2=a \text{ mod } p$ \cite{hardy2008introduction}. This fact is easy to understand as we can actually enumerate all the residues. Note that this equation admits two solutions, since it is equivalent to $(x-b)(x+b)=0 \text{ mod } p \Longrightarrow x = \pm b \text{ mod } p$. Thus, the set of solutions $\{b_i\} \in \mathbb{Z}_p$ will lead to distinct quadratic residues only if no two $b_i$ are negatives of each other in the field. This constrains us to the first half of the elements, i.e. $\{0, 1, 2, \ldots, (p-1)/2 \}$. Indeed, if we try to add another solution, $(p+1)/2$, it would square to the same residue as $(p-1)/2$, because $b_i^2 = (p-b_i)^2 \text{ mod }p$
and we have precisely $p - (p+1)/2 = (p-1)/2 \text{ mod }p$. Thus, the full set of distinct residues is:
\begin{equation}
    \left\{0^2, 1^2, \ldots, \left(\frac{p-1}{2}\right)^2  \right\}\,.
\end{equation}
This fact implies that we can take the square root of a number almost exactly $50\%$ of the time. One approach to dealing with the other half would be to simply reject the points for which the expressions under the square roots
%Eqs.~\ref{eq:delta3} and \ref{eq:delta5tr5}
do not correspond to residues and repeat the black-box sampling procedure at another point\footnote{It is also possible to adjoin the needed square roots to the field, i.e. $\mathbb{Z}_p \rightarrow \mathbb{Z}_p \left( \sqrt{a} \right)$}. However, in practice we find it convenient to deal with the square roots in a different manner, which will be explained in detail in Sections~\ref{sec:Hbb} and \ref{sec:Wyj}.

Overall, we have seen that any algorithm which can be expressed as a chain of rational operations can be implemented over finite fields. This applies to both pure rational numbers, as well as analytic expressions in the form of rational functions. This turns out to be tremendously useful, since many of the steps required in amplitude computations are precisely such rational transformations. By exploiting the finite field methods, we can entirely sidestep the analytic complexity in the intermediate stages, yet still enjoy the \textit{exact} cancellations that occur, since we are not forced to resort to floating-point numbers. The often insurmountable task of computing a function analytically has been turned into the much simpler task of providing its fast numerical evaluation over finite fields. Armed with this knowledge, we can move on to the next steps in our procedure.

\section{Reduction onto scalar integrals} \label{sec:reduction}
Before we begin, let us briefly summarise what we have learnt so far about our workflow for computing scattering amplitudes (see Fig.~\ref{fig:outline}). We started by generating all Feynman diagrams that contribute to a desired loop amplitude. We then decomposed this amplitude in colour space and defined an new object, the colour-ordered amplitude, by considering only the diagrams which contribute to a particular colour factor. We then specified the helicities of external fermions and bosons in the so-called helicity amplitudes. We have learned that not all such amplitudes are equally challenging to compute (in fact, some will vanish or be free of divergences) and due to symmetries, we will not even have to compute all possible helicity configurations. We subsequently decided to employ a language which naturally captures the helicity information of the particles, that is the spinor-helicity formalism. However, we have also seen that it suffers from several drawbacks which can be remediated by one last variable change --- into momentum twistor variables. Not only does this parametrisation of external momenta automatically satisfy kinematic identities of Eq.~\ref{eq:spinorsids}, but it also allows us to rationalise some of the square roots we need to deal with. Last, but not least, we have also learnt that by performing numerical calculations over finite fields, we can bypass the analytic complexity which typically characterises QFT problems.

At this point, the task of computing a colour-ordered helicity amplitude for $n$-particle scattering amounts to computing loop integrals of the form:
\begin{equation} \label{eq:ampschematic}
    	A_n^{(L)} \left(1^{h_1}, 2^{h_2}, \ldots, n^{h_n} \right) =  \sum_{T\in \text{topologies}} \left[\, \prod_{l=1}^{L} \left( \int \frac{\dd^d k_l}{\ii \, \pi^{d/2}}\right) \frac{\sum_i c_i(p, \eps )\times m_i(k,p)}{\prod_{t\in T} D_t(k,p)} \right].
\end{equation}
The general structure of this formula can be understood simply by considering the QCD Feynman rules and all the possible Lorentz contractions that follow from them. The main sum runs over distinct integral topologies, that is sets of inverse propagators $D_t$ associated with a given Feynman diagram (see Fig.~\ref{fig:topologies} for a few examples). Each inverse propagator depends on external momenta $p$, as well as one or more loop momenta $k$, which act as the integration variables for the $d$-dimensional integrals (see Sec.~\ref{sec:divergences}). In the numerator of each topology, we have a sum of monomials $m_i$ of \textit{both} loop and external momenta, multiplied by coefficients $c_i(p)$ that depend \textit{only} on external momenta $p$. The monomials are composed of scalar products $p_i \cdot p_i,\, k_i \cdot k_j,\, k_i\cdot p_j$; as well as the spinor brackets $\braket{ij},\, \braketsq{ij},\, \langle i |k_i \ketsq{j},\, \langle i |p_5 \ketsq{j},\, \bra{i}k_i p_5\ket{j},\, \brasq{i}k_i p_5 \ketsq{j}\,$. Similar objects appear in their coefficients, but because they do not contain any dependence on the loop momenta, we will usually express the external kinematics in the coefficients in terms of the momentum twistor variables $\mathbf{x} = \{x_1, \ldots, x_{3n-10}\}$, i.e. $c_i(p) = c_i(p(\mathbf{x}))$. This ensures the coefficients are rational, parametrised through a minimal set of variables and can be processed using the finite field framework. In fact, all the remaining steps in our workflow that deal with the computation of the coefficients will be implemented over finite fields. 
%Finally, we note that unless otherwise stated, in the following we will specialise to the case $L=2$ since we are interested in computing two-loop amplitudes.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \hspace*{1em}\raisebox{1.1cm}{ % needed to align the diagrams
        \begin{tikzpicture}
        	\begin{feynman}[small]
        		\vertex (v1);
          
        		\vertex[above left = 0.8cm of v1, yshift=-0.2cm] (i1);
        		\vertex[below left =  0.8cm of v1, yshift=+0.2cm] (i2);
        		\vertex[right = of v1] (v2);			
        		\vertex[right = of v2] (v3);			
        		
        		\vertex[above right = 0.8cm of v3, yshift=-0.2cm] (f1);
        		\vertex[right = 0.70cm of v3] (f2);
        		\vertex[below right = 0.8cm of v3, yshift=+0.2cm] (f3);

        		\diagram*{
        			(i1) -- (v1) -- [out=60, in=120] (v2) -- [out=60, in=120] (v3) -- (f1),
        			(i2) -- (v1) -- [out=-60, in=-120] (v2) -- [out=-60, in=-120] (v3) -- (f3),
                    (v3) -- (f2),
                
        		};
        	\end{feynman}
        \end{tikzpicture}}
    \caption{} \label{fig:topologya}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \hspace*{1em}
        \begin{tikzpicture}
    	\begin{feynman}[small]
    		\vertex (v1);
      
    		\vertex[above left = 0.8cm of v1, yshift=-0.2cm] (i1);
    		\vertex[below left =  0.8cm of v1, yshift=+0.2cm] (i2);
    		\vertex[above right = of v1] (v2);			
    		\vertex[below right = of v1] (v3);			
    		\vertex[right = of v2] (v4);			
    		\vertex[right = of v3] (v5);			
    		
    		\vertex[above right = 0.8cm of v4] (f1);
    		\vertex[below right = 0.5cm of v5, yshift=-0.4cm] (f2);
    		\vertex[below right = 0.5cm of v5, xshift=+0.4cm] (f3);
    		
    		\diagram*{
    			(i1) -- (v1) -- (v2) -- (v4) -- (f1),
    			(i2) -- (v1) -- (v3) -- (v5) -- (f2),
                (v2) -- (v3),
                (v4) -- (v5),
                (v5) -- (f3),
    		};
    	\end{feynman}
    \end{tikzpicture}
    \caption{} \label{fig:topologyb}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \hspace*{1em}
        \begin{tikzpicture}
    	\begin{feynman}[small]
    		\vertex (i1);
    		\vertex[below right= 0.8cm of i1] (v1);
    		\vertex[right = of v1] (v2);
    		\vertex[yshift=0.3cm, right = 0.8cm of v2] (v3);
    		\vertex[above right = 0.8cm of v3] (f1);			
    		
    		\vertex[below = of v1] (v7) ;
    		\vertex[below left = 0.8cm of v7] (i2) ;
    		\vertex[right = of v7] (v6) ;
    		\vertex[yshift=-0.3cm, right = 0.8cm of v6] (v5) ;
    		\vertex[below right = 0.8cm of v5] (f3) ;
    		
    		\vertex[xshift=0.6cm, yshift=0.2cm, below = of v3] (v4);
    		\vertex[right = 0.8cm of v4] (f2) ;		
    		
    		\diagram*{
    			(i1) -- (v1) -- (v2) -- (v3),
    			(i2) -- (v7) -- (v6) -- (v5),
    			(v3) -- (f1),
    			(v3) -- (v4) -- (v5),
    			(v4) -- (f2),
    			(v5) -- (f3),
    			(v1) -- (v7),
    			(v2) -- (v6),
    		};
    	\end{feynman}
    \end{tikzpicture}
    \caption{} \label{fig:topologyc}
    \end{subfigure}
\caption{Examples of diagram topologies associated with five-particle Feynman diagrams. It is easy to see that topologies (a) and (b) can be obtained from the maximal topology (c) by pinching some of its propagators.}
\label{fig:topologies}
\end{figure}

We point out that not all topologies contributing to each helicity amplitude are independent, in the sense that some topologies can be written as \textit{sub}topologies of others. This is illustrated in Fig.~\ref{fig:topologies}. It is clear that topologies (a) and (b) can be viewed as subtopologies of topology (c) with a few propagators absent. Topologies with the maximum number of propagators allowed for $L$-loop, $n$-particle kinematics are referred to as maximal topologies. All other topologies can be obtained from them by `pinching' appropriate propagators. Thus, the next step in our procedure is to express all topologies which contribute to Eq.~\ref{eq:ampschematic} as subtopologies of these maximal topologies. Moreover, after this mapping we will write the monomials $m_i(k, p)$ in terms of the propagators of the target maximal topology. In this way, we will remove all loop momentum dependence from the numerators, so that the amplitude will be a linear combination of scalar integrals over the maximal topologies, with rational coefficients of external kinematics parametrised by the momentum twistor variables.
\subsection{Reduction onto maximal topologies}
\JK{Figure out how the reduction is done in our code}
\JK{I tried and I failed...}
\subsection{A simple example}
\JK{Maybe 1L box? Idk if a 2L example will be too complicated.}

\section{Integration-by-parts relations} \label{sec:IBP}
\JK{Need to mention somewhere earlier that an integral family is defined as propagators + ISPs.}\\
Thanks to the effort in the previous section, the helicity amplitude we need to compute is now expressed as a sum of \textit{scalar} loop integrals over maximal topologies:
\begin{equation} \label{eq:ampreducedschematic}
    	A_n^{(L)} \left(1^{h_1}, 2^{h_2}, \ldots, n^{h_n} \right) =  
     \sum_{\mathclap{\substack{T \in \\ \text{maximal} \\ \text{topologies}}}} \sum_{\bm{\nu}} e_{T, \bm{\nu}}(p(\mathbf{x}), \eps) \left[\, \prod_{l=1}^{L} \left(\int \mathrm{d}^d k_l \right) \frac{1}{\prod_{t\in T} D^{\nu_t}_t(k,p)} \right]\,.
\end{equation}
\JK{Is the notation here and below clear?...}.\\
Here, we have made explicit the dependence of $D_t$ on the exponents $\alpha_t$. As explained earlier, the index $t$ must now run over not only the inverse propagators associated with a given maximal topology, but also the ISPs that were introduced to build a complete integral family. Overall, each element in the square brackets is a $d$-dimensional scalar integral defined by its maximal topology $T$ and a set of exponents $\bm{\nu}$. We write:
\begin{equation} \label{eq:ibpintegral}
    I_T^{(L)}(\nu_1, \nu_2, \ldots) = \prod_{l=1}^{L} \left(\int \mathrm{d}^d k_l \right) \prod_{t\in T} \frac{1}{D^{\nu_t}_t(k,p)}\,,
\end{equation}
where $\nu_t \geq 0$ if $D_t$ is an inverse propagator and $\nu_t\le0$ if $D_t$ is an ISP. As one might expect, not all $I_T$ are linearly independent. Within each family, it turns out we can reduce the integrals onto a basis of so-called master integrals, which we denote as $\text{MI}_T$. The problem of identifying such a basis and computing the reduction coefficients of each $I_T$ onto that basis is one of the most important steps in our workflow. However, it also turns out to be extremely computationally expensive. In this section, we introduce the reader to the concept of integration-by-parts reduction, as well as to the related concepts of differential equations, symbols and uniform transcendentality. These key ideas are crucial not only from the computational perspective, but also provide an insight into the analytic structure of the amplitudes.
\subsection{A brief introduction to IBP relations} \label{sec:ibpintro}
We start by making a trivial observation: the choice of the loop momentum $k$ in Feynman integrals is arbitrary. It can be re-scaled by a constant, shifted by the external momenta or even shifted by another loop momentum (in the case of multi-loop integrals) --- the physics this integral represents does not change. Surprisingly, this simple fact leads to powerful results. To see this, consider an integrand $f(k)$ and let us shift $k \rightarrow k + \lambda q$, where $\lambda$ is a small constant and $q$ is an arbitrary momentum. First, assume that $q^\mu = p^\mu$. Then, by translational invariance~\ref{eq:translationalinvariance}:
\begin{equation}
    \int \dd^d k\, f(k) = \int \dd^d k\, f(k+\lambda p) \,.
\end{equation}
Expanding the integrand for small $\lambda$, we have:
\begin{equation}
    \int \dd^d k\, f(k) = \int \dd^d k\, \left[ f(k) + \lambda p^\mu \frac{\partial f(k)}{\partial k^\mu} + \order{\lambda^2} \right]\,.
\end{equation}
Thus, at $\order{\lambda}$:
\begin{equation}
    \int \dd^d k\, \frac{\partial (p^\mu f(k))}{\partial k^\mu} = 0\,,
\end{equation}
where we deliberately moved $p^\mu$ into the derivative.

In fact, the same equation holds even if $q^\mu = k^\mu$ and the derivation is very similar. In this case, note that due to the scaling relation Eq.~\ref{eq:scaling}:
\begin{equation}
    \int \dd^d k\, f(k) = (1+\lambda)^d \int \dd^d k\, f(k+\lambda k) \,.
\end{equation}
Expanding for small $\lambda$:
\begin{equation}
    \int \dd^d k\, f(k) = \left(1+\lambda d + \order{\lambda^2}\right) \int \dd^d k\, \left[ f(k) + \lambda k^\mu \frac{\partial f(k)}{\partial k^\mu} + \order{\lambda^2} \right]\,.
\end{equation}
Collecting terms at $\order{\lambda}$:
\begin{equation}
    \int \dd^d k\, \left[ k^\mu \frac{\partial f(k)}{\partial k^\mu} + d\, f(k) \right] = \int \dd^d k\, \frac{\partial(k^\mu f(k))}{\partial k^\mu} = 0\,,
\end{equation}
where we have moved $k^\mu$ into the derivative by using $\partial k^\mu / \partial k^\mu = \delta^\mu_\mu=d$. Overall, due to the linearity of $d$-dimensional integrals, Eq.~\ref{eq:linearity}, these equations must also hold for any linear combination of external and loop momenta. Thus, for a generic $q$:
\begin{equation} \label{eq:IBPboundaryterm}
    \int \dd^d k\, \frac{\partial(q^\mu f(k))}{\partial k^\mu} = 0\,.
\end{equation}
In other words, within dimensional regularisation, the boundary terms vanish. This property is the basis of the so-called integration-by-parts (IBP) relations \cite{Chetyrkin:1981qh}. The idea is to use Eq.~\ref{eq:IBPboundaryterm} to generate a system of equations between $d$-dimensional Feynman integrals $I_T^{(L)}(\nu_1, \nu_2, \ldots)$ of Eq.~\ref{eq:ibpintegral}. Note that each such identity leads to a relation between integrals within the same integral family $T$, but with different denominator exponents $\bm{\nu}$ (for plenty of worked out examples, see Chapter 5 of ~\cite{smirnov2006feynman} or Chapter 6 of \cite{Weinzierl:2022eaz})\footnote{Let us remark that there exist other kinds of integral identities, such as Lorentz-invariance and homogeneity relations (for details, see Refs. \cite{Grozin:2011mt, Lee:2008tj, Lee:2012cn}). In fact, they can be shown to be equivalent to linear combinations of the IBP relations. Nonetheless, they can still be useful in practice. They are typically appended to the `pure' IBP system and make it less under-determined.}. Therefore, we can use these identities to lower or raise the indices until we arrive at a simpler integral. Traditionally, this reduction was done by hand through tedious analysis of all identities. This of course becomes infeasible, as the number of identities is in theory infinite (and grows quickly with the loop order and number of external particles). On the other hand, modern computations typically employ the Laporta algorithm, which provides an algorithmic way of reducing integrals to master integrals by solving a \textit{finite} linear system of identities using Gaussian elimination \cite{Laporta:2001dd}. Many computer programmes based on the Laporta algorithm---often in combination with other IBP algorithms---are publicly available, e.g. \texttt{AIR} \cite{Anastasiou:2004vj}, \texttt{LiteRed} \cite{Lee:2012cn, Lee:2013mka}, \texttt{FIRE} \cite{Smirnov:2008iw, Smirnov:2019qkx} \texttt{Kira} \cite{Maierhofer:2017gsa, Klappert:2020nbg} and \texttt{Reduze} \cite{Studerus:2009ye, vonManteuffel:2012np}.

The IBP system is under-determined, which means that the values of the MIs cannot be determined from it. However, we can express any integral included in the system as a linear combination of these MIs, with coefficients being rational functions of the kinematics and the dimension $d$ (usually expressed as $d=4-2\eps$. The rationality is an important property in the context of our finite field tools in Sec.~\ref{sec:FF}. Very often, the growth of coefficients at the intermediate stages of the IBP reduction represents a computational bottleneck. For this reason, it is extremely useful to be able to exploit numerical arithmetic over finite fields in order to bypass this complexity (see Ref.~\cite{Peraro:2019svx}).

We remark that in practice it is common to force the algorithm to reduce the families onto a predetermined MI basis. This is because --- while the choice of MIs is arbitrary --- some choices turn out to be better than others, in the sense that the integrals satisfy certain properties that make them more elegant and also easier to evaluate. We will explore this idea further in Sections \ref{sec:DEs} and \ref{sec:UT}. The definitions of such predetermined MIs can be added as additional equations to the IBP system. It is then possible to assign a lower weight to these integrals during Gaussian elimination, which means that they will be preferably chosen as the independent variables \cite{Peraro:2019svx}.

In order to agree which integrals are considered `simpler', it is useful to define the concept of sectors. A sector $\bm{\theta} = \{\theta_1, \ldots, \theta_N\}$, where $N$ - total number of propagators and ISPs in the family, and $\theta_i\in \{0, 1\}$, is the set of points $\bm{\nu} = \{\nu_1, \ldots, \nu_N\}$ in the lattice $\mathbb{Z}^N$ such that:
\begin{equation} \label{eq:sectors}
    H \left(\nu_i - \frac{1}{2} \right) = \mathbf{\theta}_i \qquad i \in \{1, \ldots, N\} \,,
\end{equation}
where $H(x)$ is the Heaviside step function. For example, an integral defined by the indices $\bm{\nu} = \{1, 0, 2, 1\}$ belongs to the sector $\bm{\theta} = \{1, 0, 1, 1\}$, but $\bm{\nu} = \{1, 0, 2, -1\}$ belongs to $\bm{\theta} = \{1, 0, 1, 0\}$. For the case $N=2$, it is straightforward to visualise how sectors divide the space (see Fig.~\ref{fig:IBPsectors}). The sector $\bm{\theta} = \{1, 1, \ldots, 1\}$ is known as the `top sector', while sectors with progressively more $\theta_i = 0$ become `sub-sectors' of the top sector and of each other \JK{Is it clear what I mean here?}. Rather trivially, the strictly non-positive sector $\bm{\theta} = \{0, 0, \ldots, 0\}$ vanishes. Furthermore, the point $\bm{\nu} = \{\theta_1, \ldots, \theta_N\}$ is called the `corner point' of sector $\bm{\theta}$. A nice result is that if the integral at the corner point of a sector is 0, then the whole sector is also 0 \cite{Lee:2008tj}.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
    \begin{tikzpicture}
        %axes
        \draw[thick,->] (-3.5,0) -- (3.5,0) node[anchor=north west] {$\nu_1$};
        \draw[thick,->] (0,-3.5) -- (0,3.5) node[anchor=south east] {$\nu_2$};
        %dots and axes labels
        \draw[fill] (0,0) circle (1pt) node[anchor=north east] {$0$};
        \foreach \i in {-3,-2,-1,1,2,3}
            {
            \draw[fill] (0,\i ) circle (1pt) node[anchor=east] {$\i$};
            \draw[fill] (\i,0) circle (1pt) node[anchor=north] {$\i$};
            \foreach \j in {-3,-2,-1,1,2,3}
                \draw[fill] (\j, \i) circle (1pt);
            }
        %lines separating sectors
        \draw[thick, blue] (0.5, -3.5) -- (0.5, 3.5);
        \draw[thick, blue] (-3.5, 0.5) -- (3.5, 0.5);

        \node at (2, 2.5) {$\bm{\theta_I} = \{1, 1\}$};
        \node at (-2, 2.5) {$\bm{\theta_{II}} = \{0, 1\}$};
        \node at (2, -2.5) {$\bm{\theta_{III}} = \{1, 0\}$};
        \node at (-2, -2.5) {$\bm{\theta_{IV}} = \{0, 0\}$};

        \clip (-3.3,-3.3) rectangle (0.3, 0.3);
        \foreach \i in {-12,...,0}
            \draw[dashed, red, samples=100] plot function{-x+(0.3+0.5*\i)} node[right] {};
    \end{tikzpicture}
    \caption{Each point $(\nu_1, \nu_2)$ in the $\mathbb{Z}^2$ lattice corresponds to the $d$-dimensional Feynman integral $I(\nu_1, \nu_2)$. The lattice is divided into sectors as defined through Eq.~\ref{eq:sectors}. They are ordered $\bm{\theta}_{I}>\bm{\theta}_{II},\,\bm{\theta}_{III}>\bm{\theta}_{IV}$. In particular, sector $\bm{\theta}_{IV}$ is trivially 0.} 
    \label{fig:IBPsectors}
    \end{subfigure}
    \hfill
    \hspace*{1em}\raisebox{0.9cm}{ % needed to align the diagrams
    \begin{subfigure}[b]{0.45\textwidth}
    \begin{tikzpicture}
      %axes
        \draw[thick,->] (-0.5,0) -- (5.5,0) node[anchor=north west] {$\nu_1$};
        \draw[thick,->] (0,-0.5) -- (0,6.5) node[anchor=south east] {$\nu_2$};

        %dots and axes labels
        \draw[fill] (0,0) circle (1pt) node[anchor=north east] {$0$};
        \draw[fill] (0,6) circle (1pt) node[anchor=east] {$6$};
        \foreach \i in {1,...,5}
            {
            \draw[fill] (\i,0) circle (1pt) node[anchor=north] {$\i$};
            \draw[fill] (0,\i ) circle (1pt) node[anchor=east] {$\i$};
            \foreach \j in {1,...,6}
                {
                \draw[fill] (\i, \j) circle (1pt);
                }
            }

        %lines separating sectors
        \draw[thick, blue] (0.5, -0.5) -- (0.5, 6.5);
        \draw[thick, blue] (-0.5, 0.5) -- (5.5, 0.5);

        \draw[orange, thick, ->] (5, 4) edge (5,3) (5, 3) edge (4,3) (4,3) edge (4,2) (4,2) edge (3,2) (3,2) edge (2,2) (2,2) edge (2,1) (2, 1) -- (1,1);
        
        \node at (5, 6.5) {$\bm{\theta_I}$};
        \node at (1, 1) [anchor=south] {$\text{MI}(1, 1)$};
        \node at (5, 4) [anchor=south] {$I(5, 4)$};
    \end{tikzpicture}
    \caption{A hypothetical reduction pathway within the top sector $\bm{\theta}_I$. An integral $I(5, 4)$ is reduced to the master integral $\text{MI}(1,1)$ through a series of IBP relations lowering the denominator exponents.} \label{fig:IBPreduction}
    \end{subfigure}
    }
    \caption{A lattice of points visualising the integrals and sectors used in the IBP reduction ($N=2$).}
    \label{fig:IBPschematic}
\end{figure}
\begin{figure}
    \centering
        \begin{tikzpicture}
        	\begin{feynman}
        		\vertex (v1);
          
        		\vertex[right = of v1] (v2);			
        		\vertex[below = of v2] (v3);			
        		\vertex[left = of v3] (v4);			
          
        		\vertex[above left = 0.7cm of v1] (i1) {$p_1$};
        		\vertex[above right =  0.7cm of v2] (i2) {$p_2$};
        		\vertex[below right =  0.7cm of v3] (i3) {$p_3$};
        		\vertex[below left =  0.7cm of v4] (i4) {$p_4$};

                \node (dot) at ($(v2)!0.5!(v3)$);
                \draw[fill] (dot) circle (2pt);
          
        		\diagram*{
                    (v1) -- (v2) -- (v3) -- (v4) -- [momentum=$k_1$] (v1),
                    (i1) -- (v1),
                    (i2) -- (v2),
                    (i3) -- (v3),
                    (i4) -- (v4),
                    };
        	\end{feynman}
        \end{tikzpicture}
    \caption{One-loop box Feynman diagram $I_\text{box}(1,1,2,1)$ with a `dotted' propagator. The corresponding denominators are: $\{k_1^2, (k_1-p_1)^2, \left((k_1-p_1-p_2)^2\right)^2, (k_1+p_4)^2\}$\,.} \label{fig:1Lboxdot}
\end{figure}
It is now possible to order (albeit not uniquely) the sectors $\bm{\theta}$ with respect to each other, as well as the integrals $I_T(\bm{\nu})$ within each sector. It is somewhat natural to think of sectors with fewer unique denominators as simpler. Thus, $\sum_{i=1}^N \theta_i$ defines an ordering between the sectors. In the $N=2$ example, we say that the sector $\{1, 1\}$ is higher than its sub-sectors $\{1, 0\}$ and $\{0, 1\}$, which are both higher than their sub-sector $\{0, 0\}$ (which vanishes anyway). However, sectors $\{1, 0\}$ and $\{0, 1\}$ are equal. Within such equal sectors, we can define further criteria, for example based on the total power of the denominators, followed by the total power of the numerators (ISPs), etc. With the help of an arbitrary ordering, we can replace higher $I_T(\bm{\nu})$ by lower ones through the IBP reduction procedure, as visualised in Fig.~\ref{fig:IBPschematic}. It should not be surprising that the set of master integrals remaining after this reduction depends on the ordering. We remark that, depending on our needs, it is possible to choose an ordering that leads to an ISP-free basis or conversely a dot-free basis (see Fig.~\ref{fig:1Lboxdot}).

Overall, IBP relations allow us to express each helicity amplitude of Eq.~\ref{eq:ampreducedschematic} in terms of a much smaller number of master integrals:
\begin{equation} \label{eq:ampafterIBP}
    	A_n^{(L)} \left(1^{h_1}, 2^{h_2}, \ldots, n^{h_n} \right) =  
     \sum_{i=1}^{|\text{MI}|} g_i(p, \eps) \times \text{MI}_i(p,\eps)\,,
\end{equation}
where $|\text{MI}|$ denotes the total number of linearly independent MIs in all the families. Once again, the coefficients are rational functions of external kinematics, as well as the dimensional parameter $\eps$. 
\subsection{Improving the performance of large IBP reductions} \label{sec:altibps}
\JK{Maybe improve this section according to what we put in the QED paper in the end?} \\
In this section, we discuss how we can incorporate the IBP reduction in our workflow for computing amplitudes in a more efficient way as compared to the standard approach. This method is based on a certain realisation about the integral families we are dealing with, as well as the fact that the reduction can be implemented as a series of numerical operations over finite fields.
\begin{figure}
    \centering
    \begin{adjustbox}{minipage=\textwidth,scale=0.9}
    \raisebox{0.2cm}{
    \begin{subfigure}[b]{0.25\linewidth}
        \hspace{-1cm}
        \begin{tikzpicture}
    	\begin{feynman}[small]
    		\vertex (v1);
    		\vertex[above left = 0.8cm of v1] (i1) {$p_5$};
    		\vertex[right = of v1] (v2);
    		\vertex[yshift=0.3cm, right = 0.8cm of v2] (v3);
    		\vertex[above right = 0.8cm of v3] (f1) {$p_1$};			
    		
    		\vertex[below = of v1] (v7) ;
    		\vertex[below left = 0.8cm of v7] (i2) {$p_4$};
    		\vertex[right = of v7] (v6) ;
    		\vertex[yshift=-0.3cm, right = 0.8cm of v6] (v5) ;
    		\vertex[below right = 0.8cm of v5] (f3) {$p_3$};
    		
    		\vertex[xshift=0.6cm, yshift=0.2cm, below = of v3] (v4);
    		\vertex[right = 0.8cm of v4] (f2) {$p_2$};		

            \draw ($(v1)!0.5!(v2)$) node[cross,red] (5cm) {};
            \draw ($(v4)!0.5!(v3)$) node[cross,red,rotate=45] (5cm) {};
            
    		\diagram*{
    			(i1) -- [very thick] (v1) -- (v2) -- (v3),
    			(i2) -- (v7) -- (v6) -- (v5),
    			(v3) -- (f1),
    			(v3) -- (v4) -- (v5),
    			(v4) -- (f2),
    			(v5) -- (f3),
    			(v1) -- (v7),
    			(v2) -- (v6),
    		};
    	\end{feynman}
    \end{tikzpicture}
    \caption{} \label{fig:pentabox}
    \end{subfigure}
    }
    %\end{adjustbox}
    \hspace{1.3cm}
    %\begin{adjustbox}{minipage=\linewidth,scale=0.4}
    \begin{subfigure}[b]{0.25\linewidth}
        \hspace{-1cm}
        \begin{tikzpicture}
        \begin{feynman}[small]
			\vertex (v1);
			\vertex[left = 0.8cm of v1] (i1) {$p_4$};
			\vertex[above right =1cm of v1] (v2);
			\vertex[below right =1cm of v1] (v3);				
			\vertex[yshift=0.3cm, right = 0.7cm of v2] (v4);	
			\vertex[yshift=-0.6cm, right = 0.7cm of v4] (v5);
			\vertex[yshift=-0.3cm, right = 0.7cm of v3] (v7);
			\vertex[yshift=0.6cm, right = 0.7cm of v7] (v6);	
			\vertex[above right = 0.8cm of v4] (i2) {$p_5$};
			\vertex[yshift=0.2cm, right = 0.8cm of v5] (i3) {$p_2$};
			\vertex[yshift=-0.2cm, right = 0.8cm of v6] (i4) {$p_1$};
			\vertex[below right = 0.8cm of v7] (i5) {$p_3$};
			
            \draw ($(v5)!0.5!(v6)$) node[cross,red] (5cm) {};
            \draw ($(v2)!0.5!(v4)$) node[cross,red, rotate=30] (5cm) {};
            
			\diagram*{
				(i1) -- (v1) -- (v2) -- (v3) -- (v1),
				(v2) -- (v4) -- (v5) -- (v6) -- (v7) -- (v3),
				(v4) -- [very thick] (i2),
				(v5) -- (i3),
				(v6) -- (i4),
				(v7) -- (i5),
			};
		\end{feynman}
        \end{tikzpicture}
    \caption{} \label{fig:hexatriangle}
    \end{subfigure}
    %\end{adjustbox}
    \hspace{1.3cm}
    %\begin{adjustbox}{minipage=\linewidth,scale=0.4}
    \raisebox{0.2cm}{
    \begin{subfigure}[b]{0.25\linewidth}
        \hspace{1em}
        \begin{tikzpicture}
    	\begin{feynman}[small]
    		\vertex (v1);
      
    		\vertex[left = 0.8cm of v1] (i1) {$p_4$};
    		\vertex[above right = of v1] (v2);			
    		\vertex[below right = of v1] (v3);			
    		\vertex[right = of v2] (v4);			
    		\vertex[right = of v3] (v5);			
    		
    		\vertex[below right = 0.8cm of v5] (i2) {$p_3$};
    		\vertex[above right = 0.6cm of v4, xshift=0.2cm, yshift=-0.2cm] (i3) {$p_2$};
    		\vertex[above right = 0.4cm of v4, yshift=+0.4cm] (i4) {$p_1$};
    		\vertex[above =  0.7cm of v2] (i5) {$p_5$};
    		
    		\diagram*{
                (i1) -- (v1) -- (v2) -- (v4),
                (v1) -- (v3) -- (v5) -- (i2),
                (v2) -- (v3),
                (v4) -- (v5),
                (v4) -- (i3),
                (v4) -- (i4),
                (v2) -- [very thick] (i5),
    		};
    	\end{feynman}
    \end{tikzpicture}
    \caption{}
    \end{subfigure}
    }
    \end{adjustbox}
\caption{A simple example of possible non-IBP relations between integrals within different (maximal) topologies. Integrals in lower sectors often map to each other. The bold line indicates the massive momentum, $p_5$, while red crosses denote a vanishing denominator, i.e. the corresponding $\nu_i$ is 0. Here, diagrams $(a)$ and $(b)$ both collapse onto diagram $(c)$. Such relations are often hard to discover just from analysing the propagators of the integrals, especially if the two families use vastly different conventions for loop momentum routing. However, a visual representation often makes spotting these relations trivial.}
\label{fig:symmetryrelation}
\end{figure}
We start by pointing out an interesting subtlety related to the number of MIs in Eq.~\ref{eq:ampafterIBP} after the reduction of multiple families:
\begin{equation} \label{eq:noofmis}
    |\text{MI}| \equiv |\text{MI}_{\bigcup_T}| \neq \sum_{\mathclap{\substack{T \in \\ \text{maximal} \\ \text{topologies}}}} |\text{MI}_T| \,,
\end{equation}
where $\bigcup_T$ indicates the union of all maximal topologies. That is, the number of MIs for all families considered together is not the same as the sum of numbers of MIs for these families considered separately. This statement might seem surprising. Indeed, if we formulate one big system of IBP relations between integrals in all the families and perform the IBP reduction, the resultant MI basis is equal to the sum of the bases obtained by solving smaller systems of IBP relations within each family one by one. This is because IBP relations, by their very nature, cannot include integrals from multiple families. However, it turns out that integrals belonging to sectors other than the top sector can often be mapped between families (see  Fig.~\ref{fig:symmetryrelation} for an easy to understand example). Therefore, an amplitude reduced using $N_T$ separate IBP systems contains leftover MIs that can be further reduced onto each other with the help of non-IBP mappings. To avoid this redundancy, we typically build a combined system which covers all the families in the problem. It contains the pure IBP relations, as well as the additional inter-family mappings\footnote{In practice, these mappings are found by comparing integrals based on their representation in terms of the $\mathcal{U}$ and $\mathcal{F}$ Symanzik polynomials (see Section 4 of Ref.~\cite{Lee:2012cn} and also Ref.~\cite{Pak:2011xt}).}. Unfortunately, for large $N_T$, this often leads to a prohibitively long time required to solve the system and may even constitute the main bottleneck of the whole amplitude computation.

There is, however, a more efficient way of performing the reduction. Recall that the integral families fed into the reduction are defined by their propagators and ISPs. Each family can therefore be identified by its `type' and a permutation of external legs within that type. In the presence of an external mass, we need to further distinguish between topology subtypes based on the position of the mass which breaks the symmetry of integrals under the permutations of external legs. For example, we refer to the topology of Fig.~\ref{fig:pentabox} as `zzz penta-box$(12345)$', and to that of Fig.~\ref{fig:hexatriangle} as `mzz hexa-triangle$(52134)$' (we follow the naming convention of Ref.~\cite{Abreu:2020jxa}).

It is not hard to notice that that if two given families are of the same (sub)type and differ only by a permutation $\sigma$ of the massless legs, i.e. $T'=T(\sigma(ijklm))$, where $\sigma$ preserves the position of the external mass, then the IBP reduction of any integral $I_{T'}(\bm{\nu})$ can be expressed by permuting the reduction of $I_T(\bm{\nu})$ according to $\sigma$:
\begin{align}
    I_{T(ijklm)}(\bm{\nu}) &= \sum_{i=1}^{|\text{MI}_T|} c_i(p, \eps) \times \text{MI}_{T(ijklm),\,i} \\
    \implies I_{T'}(\bm{\nu}) &= \sum_{i=1}^{|\text{MI}_T|} c_i(\sigma(p), \eps) \times \text{MI}_{T^\prime,\,i}\,.
\end{align}
That is, the new reduction coefficients $c^\prime_i = c_i(\sigma(p), \eps)$ are computed as a permutation of the external kinematics in the old coefficients $c_i(p, \eps)$, while the new master integrals MI$_{T^\prime}$ are obtained by permuting the external momenta in the propagators and ISPs of MI$_T$\footnote{In the case of UT MIs, which are often defined as combinations of integrals $I_T$, we also need to permute any potential kinematic prefactor of these integrals according to $\sigma$.}.

In our applications, the reduction coefficients $c_i$ will be parametrised by the momentum twistors $\mathbf{x}$. Then, since they will be rational functions of $\mathbf{x}$ and $\eps$, permuting the external kinematics to obtain $c_i^\prime$ is trivial. Moreover, the rationality means that this new approach to IBP reduction is perfectly compatible with the finite field sampling procedure. Permuting the coefficients amounts to a change of variables, which can be implemented over the field\footnote{When working with the $s_{ij}$ variables, the permutation $s_{\sigma(ij)}$ will in general take us outside of their minimal set, but we can always express the new Mandelstam variables through the same minimal set by using momentum conservation.}. Thus, we never have to know and permute the analytic form of the coefficients, which leads to a significant reduction in the computational cost. For an integral family with $n$ massless particles, we do not need to explicitly perform the IBP reduction for up to $n!$ permutations of it. In fact, we only need to do it once per maximal topology --- in an arbitrarily chosen permutation of each topology, which we label $T_{\sigma_\text{id}}$. Then, the reduction of any permutation of this family is obtained by considering $\sigma$ such that $\sigma(\sigma_\text{id})$ brings us onto the target topology, that is $T' = T_{\sigma(\sigma_\text{id})}$, and applying the steps above.

Let us now discuss a small issue with this new strategy. As pointed out in the beginning of this section, the total number of MIs after the reduction is smaller than the sum of MIs in the individual families. This redundancy is not taken care of in the approach we have described above, since the reduction is performed only within the ‚Äòmain‚Äô families $T_{\sigma_\text{id}}$ and then permuted to cover all needed permutations. Thus, the inter-family mappings between the MIs are not found and the reduced amplitude will contain MIs which are in fact not fully linearly independent. Strictly speaking, there is no requirement for the amplitude to be expressed in terms of independent objects. However, it is always desirable to do so, as this ensures that any cancellations happen analytically and prevents instabilities or issues with precision during the numerical evaluation of the amplitude. Moreover, it is more computationally efficient, as it reduces the number of independent coefficients that have to be processed when expanding the amplitude into special functions (to be discussed in Sec.~\ref{sec:specialfunctions}).

In practice, we obtain these missing relations between MIs by performing their reduction in the traditional approach (i.e. all families included together in one system) in \texttt{LiteRed}. This expresses the MIs from all families in terms of the truly independent ones:
\begin{equation}
    \text{MI}_{i} = \sum_j^{|\text{MI}|} f_{ij}(p, \eps) \times \text{MI}_j \qquad i \in \left\{1, \ldots, \sum_T |\text{MI}_T|\right\}\,,
\end{equation}
where $i$ runs over the `naive' MIs from all maximal topologies. We stress that this additional reduction does not present such a high complexity as the reduction needed for the amplitude itself. Firstly, the identities generated through the Laporta algorithm only need to cover the rank and number of dotted propagators present in the MI bases, rather than the full amplitude. Secondly, all the missing relations between MIs can be worked out once and for all --- they will be applicable to any process at the same loop order and number of particles (and external masses), since the IBP reduction is specific only to the kinematic setup. Finally, an extremely useful property of UT integrals is that any inter-family relations between them cannot contain any dependence on the kinematic variables, i.e. $f(p, \eps) \rightarrow f(\eps)$. This is simply because adding such factors would break the canonical form of the DEs and would imply that we have in fact found a relation between a UT integral and a non-UT one. Thus, when using UT bases, which has become the standard in modern amplitude computations, this additional reduction can be performed with the kinematic variables set to random values (subject to momentum conservation). 

Having obtained the missing relations between MIs, we apply them to our amplitude
and collect the coefficients of the true, linearly independent MIs. Overall, we find that this alternative approach performs much better than the previously employed strategy of considering all integral families together in one shared system. The benefit grows with the number of permutations of the maximal topologies, since including additional families in the IBP system is expensive, but numerically permuting the solution from one family onto another is cheap.

\JK{UT ints have not been defined yet. Will probably have to move this section to the end of the chapter.}
\section{Differential equations} \label{sec:DEs}
Differential equations (DEs) satisfied by Feynman integrals were studied even before the invention of the IBP reduction technique (see, for example, Ref.~\cite{Golubeva:1976}). However, it was these two concepts combined together that led to the development of a powerful method for evaluating the MIs \cite{Kotikov:1990kg, Kotikov:1991314, Bern:1993kr, Remiddi:1997ny, Gehrmann:1999as}. The idea is as follows. If we differentiate an integral $I_T(\bm{\nu})$ with respect to the Mandelstam invariants $s_{ij}$ or internal masses $m_i$, we will obtain a linear combination of integrals within the same family $T$, but with different exponents $\bm{\nu}'$. These new integrals $I_T(\bm{\nu}')$ can then be IBP reduced onto the corresponding MIs. Thus, if we apply the differentiation to the MI basis itself, we obtain a set of first-order partial DEs (one for each kinematic variable). It is convenient to group the MIs as a vector $\vv{\text{MI}}$. We then have:
\begin{equation} \label{eq:DEspartial}
    \frac{\partial}{\partial \lambda} \vv{\text{MI}} = A_{\lambda}(\lambda, \eps) \vv{\text{MI}}\,,
\end{equation}
where $\lambda \in \{s_{ij}, m_i\}$ are the kinematic variables and $A_\lambda$ is a $\left(|\text{MI}| \times |\text{MI}|\right)$ matrix. Its entries are rational functions of the kinematic variables and $\eps$, which is due to the nature of IBP relations. It is also common to work with the total differential rather than partial derivatives:
\begin{equation}
    \dd \,\vv{\text{MI}} = \sum_{\lambda} \left(\frac{\partial \vv{\text{MI}}}{\partial \lambda} \right) \dd \lambda\,,
\end{equation}
as well as to define:
\begin{equation}
    A = \sum_\lambda A_\lambda \dd\lambda\,.
\end{equation}
Then, the system of DEs can be written as:
\begin{equation} \label{eq:DEsoneform}
    \dd \,\vv{\text{MI}} = A(\lambda, \eps) \,\vv{\text{MI}}\,.
\end{equation}
Naturally, to solve this equation, we also need to provide boundary values. It is convenient to use the values of MIs at a special kinematic point, for example where one of the kinematic variables $\lambda_i$ vanishes or is equal to another kinematic variable. There, the MIs become easier to evaluate. 
\subsection{The canonical form} \label{sec:canonicalform}
In general, solving the DEs satisfied by MIs is hard. Note however, that typically we are interested in only the first few coefficients of the Laurent expansion of Eq.~\ref{eq:DEsoneform} around $\eps=0$. Consider a change of basis:
\begin{equation} \label{eq:basistransformation}
    \vv{\text{MI}} \rightarrow B\,\vv{\text{MI}}\,,
\end{equation}
where $B$ is an arbitrary matrix that can depend on $\lambda$ and $\eps$. Under this transformation, the partial DEs become:
\begin{equation}
    \frac{\partial \vv{\text{MI}}}{\partial \lambda} = \left( \frac{\partial B}{\partial \lambda}B^{-1} + B A_{\lambda} B^{-1} \right) \vv{\text{MI}}
\end{equation}
A key conjecture due to Ref.~\cite{Henn:2013pwa} is that it is always possible to choose $B$ such that:
\begin{equation}
    \left( \frac{\partial B}{\partial \lambda}B^{-1} + B A_{\lambda}(\lambda, \, \eps) B^{-1} \right) = \eps \dd \tilde{A}_{\lambda}(\lambda)\,.
\end{equation}
That is, with an appropriate transformation, the $\eps$-dependence factorises out of the matrices, which now contain only kinematic dependence. Note, however, that they might now contain algebraic (i.e. non-rational) factors such as square roots. Overall, Eq.~\ref{eq:DEspartial} becomes:
\begin{equation} \label{eq:canonicalDEspartial}
    \frac{\partial}{\partial \lambda} \vv{\text{MI}} = \eps\dd \tilde{A}_{\lambda}(\lambda) \vv{\text{MI}}\,, 
\end{equation}
while Eq.~\ref{eq:DEsoneform}:
\begin{equation} \label{eq:canonicalDEsoneform}
    \dd \,\vv{\text{MI}} = \eps \dd \tilde{A}(\lambda) \,\vv{\text{MI}}\,.
\end{equation}
This is known as DEs in \textbf{canonical form} (also known as the $\eps$-form)\footnote{Several packages exist for transforming the DEs into the canonical form. See \texttt{Fuchsia}~\cite{Gituliar:2017vzm}, \texttt{Libra}~\cite{Lee:2020zfb}, \texttt{INITIAL}~\cite{Dlapa:2020cwj} and \texttt{CANONICA}~\cite{Meyer:2018feh}. See also \cite{Dlapa:2022nct} for a comprehensive review of different techniques.}. In particular, Eq.~\ref{eq:canonicalDEsoneform} admits a solution in terms of the path-ordered exponential:
\begin{equation} \label{eq:pathexponential}
    \dd \,\vv{\text{MI}}(\lambda,\, \eps) = \mathbb{P} \exp \left(\eps\int_\gamma \dd \tilde{A} \right) \vv{\text{MI}}(\lambda(\gamma=0),\,\eps )\,,
\end{equation}
where $\gamma\,:\,[0, 1] \rightarrow \mathbb{C}^{|\lambda|}$ is a path in the space of the kinematic invariants and internal masses $\lambda$. We note that the integration is independent of the path taken. Because the solution is expressed through the series expansion of the matrix exponential, it is easy to see that the MIs are obtained as iterated integrals of $\tilde{A}$. Given the Laurent expansion of the canonical MI vector:
\begin{equation} \label{eq:MIlaurentexp}
    \vv{\text{MI}}(\lambda, \, \eps) = \sum_{k=0}^\infty \eps^k\,\vv{\text{MI}}^{(k)}(\lambda) \,,
\end{equation}
we can insert it into Eq.~\ref{eq:pathexponential} and series expand both its sides in $\eps$. Then the order-by-order DE solution becomes:
\begin{equation} \label{eq:DEssolution}
    \vv{\text{MI}}^{(k)}(\lambda) = \sum_{j=0}^k \int_\gamma \underbrace{\dd \tilde{A} \cdot \ldots \cdot \dd \tilde{A}}_{j\text{ times}} \, \cdot \, \vv{\text{MI}}^{(k-j)}(\lambda(\gamma=0))\,,
\end{equation}
where the path integral for $j=0$ is defined as 1\cite{Chen:1977oja}. These integrals are known as Chen's iterated integrals (CIIs) (see the notes~\cite{Brown:2013qva} for a thorough discussion of their properties). Note that we can assume that the Laurent expansion starts from $\eps=0$, because the DEs are insensitive to a re-scaling of the integrals by a factor which does not depend on the kinematics. Thus, the MIs can be normalised to be finite, which moves any potential singularities into the coefficients of the IBP reduction. 
\subsection{The $\dd \log$ form and Goncharov Polylogarithms} \label{sec:DEdlogform}
Overall, we see that the MIs at $\order{\eps^k}$ are given by sums of up to $k$-fold integrals. The integration kernels are determined by the structure of $\tilde{A}$, which deserves further discussion. Let us consider the singularities of the DEs. It can be shown, for example by studying the Feynman parameter representation, that the Feynman integrals cannot contain so-called essential singularities, e.g. singularities of the form $\mathrm{e}^{1/\lambda} = 1+ 1/\lambda + 1/(2!\lambda^2)+\ldots\,$ at $\lambda=0$. In particular, for each singularity $\lambda_i$, we expect the leading behaviour of the Feynman integrals to be $\sim (\lambda- \lambda_i)^\alpha$ for some power $\alpha$. This implies that the corresponding DEs must have only regular singularities. In particular, around each singular point $\lambda_i$, they must have a simple pole $\sim \alpha/(\lambda-\lambda_i)$. This strongly constrains the form that DEs can take\footnote{In practice, when constructing canonical DEs, we might encounter spurious double poles or higher. However, they can be removed by a suitable basis change which leaves the DEs with only simple poles. \JK{Is this always true?}}\cite{Henn:2014qga}. Furthermore, in many cases of practical interest, it is possible to construct MI bases which result in DEs in the so-called $\dd \log$ form:
\begin{equation} \label{eq:dlogform}
    \dd\tilde{A} = \sum_{i=1}^{|\bm{w}|} a_i \times \dd \log w_i\,.
\end{equation}
Here, $w_i$ are known as `letters' (note that sometimes the differentials $\dd \log w_i$ are referred to as letters instead), while their collection $\bm{w} = \{w_1, w_2, \ldots\}$ is the `alphabet'. The matrices $a_i$ contain rational numbers only, free of any kinematic of $\eps$ dependence. The letters play a central role in the analysis of DEs in the $\dd \log$ form, since they control their singularities and determine which class of special functions the MIs are written in terms of. They are also closely connected to the `symbol' formalism, which we will introduce soon. As mentioned before, in general the kinematic dependence of $\tilde{A}$ might no longer be rational, as the transformation Eq.~\ref{eq:basistransformation} needed to bring DEs into canonical form might introduce non-rational functions into the definitions of the corresponding canonical MIs. In this case, the letters $w_k$ are algebraic functions of $\lambda$. However, if the form of Eq.~\ref{eq:dlogform} can be reached by using rational transformations only, the letters are rational functions of the form $w_i = \lambda - \lambda_i$. Then, it is particularly easy to write down the order-by-order solution Eq.~\ref{eq:DEssolution}, as the iterated integrals become the well-known Goncharov Polylogarithms (GPLs)\footnote{Also known as Multiple Polylogarithms (MPLs).}\cite{2001math......3059G, 2011arXiv1105.2076G, Vollinga:2004sn}:
\begin{equation} \label{eq:GPLs}
    G_n(a_1, a_2, \ldots, a_n; \lambda) = \int_0^\lambda \frac{\dd t}{t-a_1} G_{n-1}(a_2, \ldots, a_n; t)\,,
\end{equation}
with the `empty' GPL:
\begin{equation}
    G_0(; \lambda) = 
    \begin{cases}
        0 & \text{if } \lambda = 0\,, \\
        1 & \text{if } \lambda \neq 0\,. 
    \end{cases}
\end{equation}
Here, despite the somewhat suggestive notation, the indices $a_i$ do not have to be numbers and are considered fully-fledged arguments of $G_n$, alongside $\lambda$. The length of the vector $\vec{a} = (a_1, \ldots a_n)$, i.e. $|\vec{a}|=n$ is called the weight (or depth) of $G_n$. The GPLs are related to the usual logarithms through:
\begin{equation}
    G_n(a, a, \ldots, a; \lambda) = 
    \begin{cases}
        \frac{1}{n!}\log^n\left(1-\frac{\lambda}{a}\right) & \text{if  $a \neq 0$} \\ 
        \frac{1}{n!}\log^n x & \text{if $a = 0$}\,.
    \end{cases}
\end{equation}  
Aside from this special case of GPLs, the complexity of the integration kernels can be estimated by studying the maximal cuts of the relevant Feynman integrals~\cite{Tancredi:2017onthemaxcut, Abreu:2020jxa, abreu2021twoloop}. More generally, the letters might not be rational, or the kernels overall might not be of the $\dd \log$ form. Then, more complicated functions appear in the solution of the DEs.  For example, the presence of internal masses in the Feynman diagrams often \JK{Often or always?} leads to Elliptic Multiple Polylogarithms (eMPLs) (see Refs.~ \cite{Broedel:2017kkb, Broedel:2017siw, Adams:2017ejb, Broedel:2018qkq, Adams:2018yfj, Walden:2020odh} for a discussion of eMPLs and also Refs.~\cite{Bonciani:2016qxi, Bonciani:2019jyb, Frellesvig:2019byn} for the application to Higgs+jet production with quark mass dependence). 

\subsection{Uniform transcendentality} \label{sec:UT}
When talking about canonical DEs, it is also useful to introduce the idea of `transcendentality'. For an iterated integral $f$, its transcendental weight $\mathcal{T}$ is simply the count of iterated integrations needed to define $f$\cite{Henn:2013pwa}. For example, $\mathcal{T}(\log x) = 1$, while $\mathcal{T}(G_n(a_1, \ldots, a_n;\, x)) = n$. From the definition, it follows that $\mathcal{T}(f_1 f_2) = \mathcal{T}(f_1) + \mathcal{T}(f_2)$, but note that $\mathcal{T}(f_1 + f_2)$ cannot be defined unless $\mathcal{T}(f_1) = \mathcal{T}(f_2)$. Furthermore, algebraic functions and constants have weight 0. Special constants which can be considered as derived from iterated integrals have the corresponding weight, e.g. $\mathcal{T}(\pi) = 1$, since $\log (-1) = \pm \i \pi$ and $\mathcal{T}(i) = \mathcal{T}(-1) = 0$, while $\mathcal{T}(\zeta(n)) = n$, since $\zeta(n) = \text{Li}_n(1)$ (for $n>1$). It is also convenient to assign $\mathcal{T}(\eps)=-1$. With this choice, it is clear that every term in the solution of Eq.~\ref{eq:pathexponential} has the same transcendental weight. This property is referred to as uniform transcendentality (UT). A simple example of a UT function could be $f(x) = 1 + \eps (\log x + \pi) + \eps^2 (\log^2 x + G_2(1, 1;\, x))$, with $\mathcal{T}(f) = 0$. Additionally, 
UT functions satisfying a more stringent condition:
\begin{equation} \label{eq:purecondition}
    \mathcal{T}(\dd f) = \mathcal{T}(f) - 1
\end{equation}
are known as \textit{pure} function. In practice, this means that a pure function cannot contain algebraic factors that are not constant --- while they do not affect the transcendental weight of a function, they affect the DE it satisfies\footnote{Note that in literature, the term `UT' is often implicitly taken to mean `UT and pure'.}. It immediately follows that the iterated integral solution to the canonical DEs is built out of pure functions (the reverse is also true: given a pure basis of MIs, the DEs they satisfy will be canonical). In practice, when constructing a MI basis, we can verify its purity by checking a few conditions that the DE matrices $\tilde{A}_\lambda$ of Eq.~\ref{eq:canonicalDEspartial} have to satisfy:
\begin{subequations}
\begin{align}
    [ \tilde{A}_i, \tilde{A}_j] &= 0\,, \\
    \partial_i \tilde{A}_j - \partial_j \tilde{A}_i &= 0\,, \\
    \sum_i \lambda_i \tilde{A}_i &= [\vv{\text{MI}}] \, \mathds{1} \,,
\end{align}
\end{subequations}
where in the last equation, the sum runs over all kinematic scales, $[\vv{\text{MI}}]$ are the mass dimensions of the MIs. \JK{I don't understand the scaling property}. Finally, we point out an interesting observation on the nature of dimensionally regulated amplitudes (see e.g. Ref.~\cite{Duhr:2014woa}). For a Laurent expanded $L$-loop amplitude in $d = 4- 2\eps$, it is conjectured that the $\order{\eps^k}$ term contains functions of transcendental weight up to $2L+k$. For example, to calculate a two-loop amplitude up to $\order{\eps^0}$, we need to supply the MI expansions up to weight 4. It is expected that in $\mathcal{N}=4$ Super Yang-Mills theory, this bound is saturated, i.e. functions of weight exactly $2L+k$ are required.
\begin{table}[t]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
            \hline
            External masses & Type & Topology & Publications \\
			\hline
            \multirow{2}{0cm}{0} & planar & penta-box & \cite{Gehrmann:2015bfy,Papadopoulos:2015jft, Gehrmann:2018yef, Abreu:2018aqd, Chicherin:2020oor} \\
            \cline{2-4}
            & \multirow{2}{2cm}{non-planar} & hexa-box & \cite{Chicherin:2018mue, Chicherin:2017dob, Chicherin:2018ubl, Chicherin:2018wes, Abreu:2018rcw, Chicherin:2020oor, Abreu:2018aqd} \\
            & & double-pentagon & \cite{Chicherin:2018old, Abreu:2018aqd, Chicherin:2020oor} \\
            \hline
            \multirow{2}{0cm}{1} & planar & penta-box & \cite{Abreu:2020jxa, Chicherin:2021dyp, Canko:2020ylt} \\
            \cline{2-4}
            & \multirow{2}{2cm}{non-planar} & hexa-box & \cite{abreu2021twoloop, Kardos:2022tpo, Papadopoulos:2019iam, Chicherin:2021dyp} \\
            & & double-pentagon & \cite{Abreu:2023rco} \\
            \hline
		\end{tabular}
\end{center}
\caption{Selected works relevant to the computation of two-loop, five-point pure master integral bases. All propagators are massless.}
\label{tab:MIs}
\end{table}

Overall, pure integrals have played a central role in the derivation and evaluation of MI bases relevant to this thesis, that is bases for processes with a high number of kinematic scales. For future convenience, in Table~\ref{tab:MIs} we collect (without claiming to be exhaustive) the publications dealing with two-loop, five-point integrals with up to one external mass and massless propagators.

\section{Symbols} \label{sec:symbols}
\subsection{Introduction}
In this section, we introduce yet another useful concept related to iterated integrals. We begin by noting that the representation of the solution to the canonical DEs in Eq.~\ref{eq:canonicalDEsoneform} is not unique. Most generally, the answer is written using Chen's iterated integrals (CIIs)~\cite{Chen:1977oja}. This form is usually the most compact one. However, as pointed out above Eq.~\ref{eq:GPLs}, if the `letters' are rational in at least one variable, CIIs be expressed in terms of GPLs~\cite{Henn:2014qga}. This typically increases the number of terms, but has the advantage that robust and stable numerical evaluation of GPLs is available (see \cite{Bauer:2000cp, Vollinga:2004sn}, but also \cite{Panzer:2014caa}). Moreover, due to the conjecture pointed out in the previous section, typically we will be interested in computing the solution up to transcendental weight 4. Another remarkable conjecture exists: any transcendental function of weight $k < 4$ can be expressed in terms of the usual logarithms and polylogarithms $\text{Li}_k$ only~\cite{MR1265551, Goncharov:2010jf}, while at weight $k=4$ all functions can be expressed in terms of the following functions~\cite{Henn:2014qga}:
\begin{equation} \label{eq:weight4basis}
    \left\{\log(x)\log(y)\log(z)\log(w), \Li_2(z) \log(x) \log(y), \Li_2(x)\Li_2(y), \Li_3(y) \log (x), \Li_4(x), \Li_{2,2}(x, y) \right\}\,,
\end{equation}
Here, the arguments $x,y,z,w$ are to be determined\footnote{See Section 6 of Ref.~\cite{Duhr:2011zq}, which considers this problem for the spanning set of functions with $k\le4$ in the special case of Harmonic Polylogarithms (GPLs with all indices $a_i\in\{0,\pm1\})$.}.
The classical polylogarithms are defined as:
\begin{equation} \label{eq:Lidefinition}
    \Li_n(z) = \sum_{k=1}^\infty \frac{z^k}{k^n}\,, \qquad z, n \in \mathbb{C}\,,
\end{equation}
while the multiple polylogarithms $\Li_{m_1, \ldots, m_k}$ are their generalisations~\cite{2011arXiv1105.2076G, Duhr:2019tlz}:
\begin{subequations}
\begin{align} \label{eq:multiLidefinition}
    \Li_{m_1, \ldots, m_n}(z_1, \ldots, z_n) &= \sum_{0<k_1<\ldots<k_n} \frac{z_1^{k_1} \ldots z_n^{k_n}}{k_1^{m_1} \ldots k_n^{m_n}} \\
    &= \sum_{k_n=1}^\infty \frac{z_n^{k_n}}{k_n^{m_n}} \sum_{k_{n-1}=1}^{k_n-1} \frac{z_{n-1}^{k_{n-1}}}{k_{n-1}^{m_{n-1}}}\ldots \sum_{k_1=1}^{k_2-1} \frac{z_1^{k_1}}{k_1^{m_1}}\,. \label{eq:multiLidefinition2}
\end{align}
\end{subequations}
Note that in these two definitions, $|z_i|<1$, but the formulas can be analytically continued to cover regions where $|z_i| \ge 1$. The $\Li_{m_1, \ldots, m_k}$ MPLs can be viewed as the series representation of the GPLs defined in Eq.~\ref{eq:GPLs}. They are related through the following equation~\cite{Duhr:2011zq}:
\begin{equation} \label{eq:LiToG}
    \Li_{m_1, \ldots, m_n}(z_1, \ldots, z_n) = (-1)^n\,G\left(\vec{0}_{m_n-1}, \frac{1}{z_n}, \ldots, \vec{0}_{m_1-1}, \frac{1}{z_1\ldots z_n}; \,1 \right)\,.
\end{equation}
For example, $\Li_{2,2}(x, y)$ in the weight-4 basis Eq.~\ref{eq:weight4basis} can be written as:%admits the integral representation:
\begin{equation}
     \Li_{2,2}(x, y) = G\left(0, \frac{1}{y}, 0, \frac{1}{xy};\,1 \right)\,.
\end{equation}
%\begin{equation}
%    \Li_{2,2}(x, y) = -\int_0^1 \frac{x \dd t}{1- xt} \log t \, \Li_2(xyt)\,.
%\end{equation}
Overall, we see that the most naive solution to the canonical DEs, written in terms of CIIs, is likely to contain a large degree of redundancy. However, due to the abundance of integral representations, it is not easy to choose the most appropriate set of functions or even verify if two expressions are equivalent. We would like to have a tool which can find relations between transcendental functions and be representation independent \JK{Is it correct to say that the symbol is representation independent?}. To this end, we introduce the notion of a \textbf{symbol}\cite{Goncharov:2010jf, Duhr:2011zq}. After giving its definition and properties, we will demonstrate its power based on several illustrative examples. To motivate our effort, we remark that the main result of Ref.~\cite{Goncharov:2010jf} was the re-writing of a 17-page expression for a quantity known as the two-loop, six-point remainder function (related to Wilson loops) in $\mathcal{N}=4$ SYM~\cite{DelDuca:2009au, DelDuca:2010zg} as a remarkably compact combination of classical polylogarithms. \textcolor{gray}{Moreover, we will have a chance to exploit the power of symbols on our own when we study the structure of the `all-plus' $\phi$+gluon amplitude in Section~\ref{sec:selfdual}}.
\subsection{Definition and properties}
The symbol of a CII with $\dd \log$ kernels is defined as:
\begin{equation} \label{eq:symboldef}
    \SS \left(\int_\gamma \dd \log w_1 \cdot \ldots \cdot \dd \log w_n \right) \equiv w_1 \oo \ldots \oo w_n \,.
\end{equation}
We can think of it as an elementary $n$-fold tensor where each entry $w_i$ is implicitly understood as to mean its corresponding $\dd \log$ form. It satisfies certain properties that we would expect from the behaviour of logarithms\footnote{For a detailed discussion of symbol properties, see Ref.~\cite{Duhr:2011zq}}:
\begin{subequations}
\begin{align} \label{eq:symbolproperties}
    A \oo (ab) \oo B &= A \oo a \oo B + A \oo b \oo B\,, \\
    A \oo \left(\frac{a}{b}\right) \oo B &= A \oo a \oo B - A \oo b \oo B\,, \\
    A \oo a^n \oo B &= n(A \oo a \oo B)\,,
\end{align}
\end{subequations}
where $A, B$ are elementary tensors and $a, b$ are algebraic functions. Note that in the last line, $n$ becomes a coefficient of the symbol, rather than a part of it. Moreover, a symbol which contains a constant (not only rational) vanishes:
\begin{equation} \label{eq:symbolAcB}
    A \oo c \oo B = 0\,,
\end{equation}
which can be understood as a consequence of $\dd \log c = 0$. This means that, when working with the symbol, we are insensitive to constants, including transcendental ones like $\pi$. 

Since CIIs are defined as repeated integrations over some kernels, it is natural to expect that the total differential acts on the symbol defined through Eq.~\ref{eq:symboldef} in the following way:
\begin{equation} \label{eq:symboldiff}
    \dd (w_1 \oo \ldots \oo w_n) = \dd \log w_n \, (w_1 \oo \ldots \oo w_{n-1})\,.
\end{equation}
This should remind the reader of the definition of pure functions in Eq.~\ref{eq:purecondition}. Crucially, this differential property can be reversed and allows us to define the symbol recursively~\cite{Goncharov:2010jf}. For a transcendental function $F$ with $\mathcal{T}(F) = k$ whose total differential can be written as:
\begin{equation}
    \dd F = \sum_i f_i \, \dd \log w_i\,,
\end{equation}
where $f_i$ are functions with $\mathcal{T}(f_i)=k-1$, we have:
\begin{equation} \label{eq:symbolrecursive}
    \SS(F) = \sum_i \SS(f_i)\oo w_i \,.
\end{equation}
Let us now see how to use this definition. For now, we will focus on the classical polylogarithms $\Li_n(z)$. In this case, the starting point of the recursion is a single weight-1 function --- the logarithm: $\SS(\log w) = w$, where the $w$ on the RHS is understood as an elementary tensor in the sense of Eq.~\ref{eq:symboldef} (remember that $\SS(\pi) = \SS(\zeta(n))=0$ \JK{Is this relevant?}). This immediately allows us to obtain the symbol of any $\Li_n$. Note the weight-1 polylogarithm is just the normal logarithm:
\begin{equation} \label{eq:Li1equivlog}
    \Li_1(z) = -\log (1-z)\,. 
\end{equation}
Moreover, the following iterative definition holds:
\begin{equation} \label{eq:Liiterative}
    \Li_n(z) = \int_0^z \frac{\dd t}{t} \Li_{n-1}(t)\,, \qquad n>1\,.
\end{equation}
which is equivalent to the differential relation:
\begin{equation}
    \dd \Li_n(z) = (\dd \log z) \Li_{n-1}(z)\,, \qquad n>1\,.
\end{equation}
Thus, Eqs.~\ref{eq:Li1equivlog} and \ref{eq:symbolrecursive} give:
\begin{align} \label{eq:Lisymbols}
    \SS(\Li_1(z)) &= -(1-z)\,, \nonumber \\
    \SS(\Li_2(z)) &= -(1-z) \oo z \,, \nonumber \\
    &\,\,\, \vdots \nonumber \\
    \SS(\Li_n(z)) &= -(1-z) \oo \underbrace{z \oo \ldots \oo z}_{n-1 \text{ times}}\,.
\end{align}
Note that in deriving the first line, we used $\SS(-f) = -\SS(f)$\JK{Make sure this is correct.} and this minus sign cannot be dropped or absorbed, i.e. $\SS(\Li_1(z)) \neq (z-1)$. Moreover, it should not be confused with the symbol expression $-1 \oo (1-z)$, which vanishes due to Eq.~\ref{eq:symbolAcB}. To avoid confusion, the symbol is often written using a square bracket, e.g. $\SS(\Li_2(z)) = -[1-z, z]$.

The symbol of the GPLs can be obtained recursively in a similar manner. We will need the differential equivalent of the integral definition in Eq.~\
\ref{eq:GPLs} \cite{2001math......3059G}:
\begin{equation} \label{eq:GPLdifferential}
    \dd G(a_{n-1}, \ldots, a_1; a_n) = \sum_{i=1}^{n-1} G(a_{n-1}, \ldots, \hat{a}_i, \ldots, a_1; a_n) \dd \log\left( \frac{a_i - a_{i+1}}{a_i - a_{i-1}} \right)\,,
\end{equation}
where $\hat{a}_i$ denotes the index that should be removed in a particular GPL and $a_0$ is understood as 0. The starting point of this recursion is a bit problematic, since we do not have a neat representation of the weight-1 GPL $G(a; z)$ that would be analogous to Eq.~\ref{eq:Li1equivlog}.  Indeed, we can move down one more step on the recursion ladder:
\begin{equation}
    \dd G(a; z) = G(;z) \dd \log \left(\frac{a-z}{a} \right)\,.
\end{equation}
It is then conventional to define the symbol of the `empty' GPL as the empty symbol:
\begin{equation}
    \SS (G(;z)) = [\,]\,.
\end{equation}
Clearly, the reader will now understand why we mentioned the alternative notation for the symbol. Then the weight-1 GPL has the symbol:
\begin{equation} \label{eq:G1symbol}
    \SS(G(a; z)) = \left[\frac{a-z}{a} \right] = [a-z] - [a]\,.
\end{equation}
Starting from this equation, one then then calculate the symbols of all GPLs with higher weights.

We also need to know how to obtain the symbol of a product of functions, e.g. $\SS(\log(z) \Li_2(z))$. To this end, we point out that the CIIs satisfy so-called shuffle relations. Specialising to the case of GPLs, we have:
\begin{equation} \label{eq:GPLshuffleprod}
    G(\vec{a};z)G(\vec{b};z) = \sum_{\vec{c}=\vec{a} \shuffle \vec{b}} G(\vec{c};z)\,.
\end{equation}
The shuffle product $\vec{a} \shuffle \vec{b}$ produces all possible ways of permuting the union of indices $\vec{a}$ and $\vec{b}$ such that the ordering of indices within each set is preserved. Perhaps a simple example illustrates this definition better:
\begin{align} \label{eq:shuffleexample}
    G(a_1,a_2;z) G(b_1,b_2;z) &= G(a_1,a_2,b_1,b_2;z) + G(a_1,b_1,a_2,b_2;z) + G(a_1,b_1,b_2,a_2;z) \nonumber \\ 
    & + G(b_1,a_1,a_2,b_2;z) + G(b_1,a_1,b_2,a_2;z) + G(b_1,b_2,a_1,a_2;z)\,.
\end{align}
Therefore, the shuffle relations allow us to write a product of GPLs of weight $|\vec{a}|$ and $|\vec{b}|$ as a GPL of weight $|\vec{a}|+|\vec{b}|$. Thus, we can now calculate the symbol of the product $\SS(\log(z) \Li_2(z))$ by expressing the two functions as special instances of GPLs, taking the shuffle product of these GPLs according to Eq.~\ref{eq:GPLshuffleprod} and finally mapping the symbol over the resultant sum of higher-weight GPLs. However, it is often easier to follow an alternative route, which arises due to the fact that:
\begin{equation}
    \SS(f_1 f_2) = \SS(f_1) \times \SS(f_2)\,,
\end{equation}
where the RHS is understood as the shuffle product of the individual symbols. Indeed, the symbol itself inherits the shuffle algebra:
\begin{equation} \label{eq:symbolshuffle}
    (w_1 \oo \ldots \oo w_n) \times (v_1 \oo \ldots \oo v_m) = \sum_{\vec{z}=\vec{w} \shuffle \vec{v}} (z_1 \oo \ldots \oo z_{n+m})\,.
\end{equation}
Therefore, in our example, we may simple write: 
\begin{align}
    \SS(\log(z) \Li_2(z)) &= (z) \times (-(1-z)\oo z) \nonumber \\
    &= -z \oo (1-z) \oo z - 2 (1-z) \oo z \oo z \\
    &= -[z, 1-z, z] -2[1-z, z, z] \,. \nonumber
\end{align}
In the last line, we have again used the alternative notation in order to make the position of the minus sign and a constant absolutely clear\JK{Is the last line actually correct???}.

\subsection{Examples}
Overall, we see that thanks to symbols of the classical polylogarithms Eqs.~\ref{eq:Lisymbols} and GPLs Eq.~\ref{eq:G1symbol}, the symbol shuffling Eq.~\ref{eq:symbolshuffle} and finally the relation Eq.~\ref{eq:LiToG}, we can now compute the symbol of each function in the weight-4 basis of Eq.~\ref{eq:weight4basis}. Thinking ahead to our amplitude applications, it should now be clear that the symbol will allow us to find relations between linearly dependent objects such as GPLs and express our results in a much more compact form. Having expended the effort of understanding the last few pages, let us enjoy the fruits of our labour sprouting in the form of several illustrative examples.

\textbf{Example 1} \newline
Given a functional relation, we can check its validity ---- but only up to constants. Perhaps a better way of phrasing this is that while we can never prove with full certainty that an identity is valid by looking at its symbols, we can definitely show that an identity is not correct by proving that the symbols do not match up. Consider the well-known dilogarithm reflection identity:
\begin{equation}
    \Li_2(1-z) =- \Li_2(z) -\log(z) \log(1-z) + \frac{\pi^2}{6} \,.
\end{equation}
For the two dilogarithms, we can straightforwardly use Eq.~\ref{eq:Lisymbols}, while for the product of ordinary logs we need to use the shuffle algebra. Thus, at the symbol level:
\begin{equation}
    -[z,1-z] = + [1-z,z] - [z] \shuffle [1-z]\,.
\end{equation}
Taking this shuffle product is trivial and it yields the two sides equal. Thus, we managed to verify the identity holds (up to $\frac{\pi^2}{6}$ ).

\textbf{Example 2} \newline
If we're trying to construct a functional relation, the symbol can also help us to fix the coefficients of a function. Let us conjecture another identity:
\begin{equation} \label{eq:Li2ref2conj}
    \Li_2\left( \frac{1}{z} \right) = a\Li_2(z) + b \log^2(-z) + c\,.
\end{equation}
Then, at the symbol level:
\begin{align}
    -\left[1-\frac{1}{z}, \frac{1}{z}\right] &= -a[1-z, z] + b[-z] \shuffle [-z] \nonumber \\
    [z-1,z] - [z,z] &= -a[z-1, z] + 2 b[z]\,,
\end{align}
To get to the second line, we have expanded the LHS using the symbol properties in Eqs.~\ref{eq:symbolproperties} and \ref{eq:symbolAcB}, while on the RHS we used $[-x]=[x]$, which is a direct consequence of these properties. Thus, $a=-1, \, b=-1/2$. We still need to fix the so-called `beyond the symbol terms'. These include not only the weight-2 constants such as $\pi^2$ or $\zeta(2)$\JK{I think it's unreasonable to expect $\zeta(2)$ to appear here, but can we really exclude it?}, but also products of weight-1 constants with a $\log$. The latter type can be often fixed by differentiating both sides of the identity, thus producing a relation at lower weight. In this case, differentiating Eq.~\ref{eq:Li2ref2conj} with the known values of $a$ and $b$ shows that no $\log \times \text{constant}$ terms are missing. Hence, only weight-2 constants are allowed in $c$ and we can fix it by evaluating the identity at a chosen value for $z$. In this way, we find:
\begin{equation} \label{eq:Li2ref2}
    \Li_2\left( \frac{1}{z} \right) = -\Li_2(z) + \frac{1}{2} \log^2(-z) + \frac{\pi^2}{6}\,, \qquad z \notin (0,\, 1)\,,
\end{equation}
which is indeed a well-known dilogarithm relation. Finally, the reader might wonder we have conjectured $\log(-z)$ instead of $\log(z)$. After all, as pointed out above, both these functions have the same symbol --- $[z]$. The reason is simple: swapping the sign of the argument in the $\log$ would mean that there is no domain in which all the functions in this relation are defined and they would consequently have to be analytically continued.

\textbf{Example 3}
\newline
\JK{Not sure if these example adds anything of value...}
For more complicated identities, calculating the symbol by hand becomes unfeasible. Consider the following weight-3 relation:
\begin{align}
    \Li_3(z)&=-\Li_3\left(\frac{z}{z-1}\right)-\Li_3(1-z) \nonumber \\
    &+\frac{1}{6} \log ^3(1-z)-\frac{1}{2} \log (z) \log ^2(1-z)+\frac{1}{6} \pi ^2 \log (1-z)+\zeta (3)\,, \qquad z \notin (1, \, \infty)\,.
\end{align}
\JK{Is the domain correct here?}
The symbol of the LHS is trivial thanks to Eq.~\ref{eq:Lisymbols}:
\begin{equation} \label{eq:Li3symbol}
    \SS(\Li_3(z)) = -[1-z,z,z]\,.
\end{equation}
The RHS requires much more work, namely repeated application of shuffle products and symbol properties Eq.~\ref{eq:symbolproperties}. These operations can be easily automated, however. In particular, we make use of the \texttt{Mathematica} package \texttt{PolyLogTools}~\cite{Duhr:2019tlz}, which enables the computation of symbols of arbitrary combinations of GPLs. Indeed, with the help of this package, it is trivial to verify that the sum of symbols of the terms on the RHS is equal to Eq.~\ref{eq:Li3symbol}.

The \texttt{PolyLogTools} package introduces several powerful tools related to the symbol. In particular, we point out its functionality related to the so-called `fibration basis'. Given a GPL expression $F$ whose letters (i.e. the arguments) satisfy certain conditions, it is possible to write it in the following form:
\begin{equation}
    F = \sum_i c_i\, G(\vec{a}_i;\, x)\,,
\end{equation}
for some variable x, where $\vec{a}_i$ are independent of $x$ and the coefficients $c_i$ include only those GPLs that are independent of x (see~\cite{Duhr:2019tlz} and references therein). The GPLs appearing on the RHS of this formula are known as the fibration basis. Moreover, it is also possible to `integrate' a symbol --- given a symbol tensor $S$, it is possible to find a fibration basis such that $\SS(F) = S.$  This means we can determine the functional form corresponding to a symbol expressions (up to beyond-the-symbol terms). 
\subsection{Relation to canonical DEs and discontinuities}
Let us now take a step back and make a connection between the symbols and the differential equations satisfied by master integrals. In Section~\ref{sec:DEs}, we saw that by making a suitable change of basis, it is possible to construct MIs such that the corresponding DEs are in the canonical form of Eq.~\ref{eq:canonicalDEsoneform}. Then, their solution is given by a path-ordered exponential of the $\eps$-free DE matrix $\tilde{A}$. This exponential is understood through its series expansion, leading to repeated integrations over kernels dictated by $\tilde{A}$. Further, we introduced the notion of uniform transcendentality and an even stronger condition on `pure' functions, Eq.~\ref{eq:purecondition}, which says that the transcendental weight of pure UT functions is lowered by 1 upon differentiation. Moreover, we learned that the solution of any canonical DEs is pure and vice versa --- a given basis of pure functions will satisfy the canonical DEs. We should therefore be able to write down these DEs in a way which makes the purity property manifest. Indeed, notice that the differential equivalent of Eq.~\ref{eq:DEssolution} is:
\begin{equation}
    \dd \vv{\text{MI}}^{(k)} = \dd \tilde{A} \vv{\text{MI}}^{(k-1)}\,.
\end{equation}
If we specify to the cases where the DE matrix can be written in the $\dd \log$ form of Eq.~\ref{eq:dlogform}, then this equation becomes:
\begin{equation} \label{eq:pureMIDEs}
    \dd \vv{\text{MI}}^{(k)} = \left( \sum_i b_i^{(k)} \dd \log w_i  \right)\vv{\text{MI}}^{(k-1)}\,.
\end{equation}
\JK{Is it $b_i^{(i)}$ or the same $a_i$ as in Eq.~\ref{eq:dlogform}? Eq. (5.1) in Abreu suggests it's the latter and I would agree.}. Note that $\vv{\text{MI}}^{(0)}$ is a vector of constants since it lives in the kernel of the derivative \JK{Not sure if I understand this}. This means that the leading term in the Laurent expansion of pure MIs does not contain any kinematic dependence. 

We can use Eq.~\ref{eq:pureMIDEs} to express the solution of the canonical DEs \textit{up to} weight $n$ in the following form:
\begin{equation} \label{eq:pureMIDEsupton}
    \vv{\text{MI}}^{(n)} = \sum_{\alpha_1, \ldots, \alpha_n} \vec{c}_{\alpha_1, \ldots, \alpha_n} \int \dd \log w_{\alpha_1} \ldots \dd \log w_{\alpha_n}\,, \qquad n\ge1\,.
\end{equation}
Note that each coefficient $\vec{c}_{\alpha_1, \ldots, \alpha_n}$ is a vector of rational numbers --- they are calculated from the products of matrices \JK{$b_i^{(i)}/a_i$}, which are dotted into the weight-0 term $\vv{\text{MI}}^{(0)}$. Naturally, to obtain explicit values for $\vv{\text{MI}}^{(n)}$, we need to supply the integration contour. Nonetheless, the symbol, which discards this information, can still tell us a lot about the analytic structure of the solution. It can be trivially read off from the above equation:
\begin{equation}
    \SS\left(\vv{\text{MI}}^{(n)}\right) = \sum_{\alpha_1, \ldots, \alpha_n} \vec{c}_{\alpha_1, \ldots, \alpha_n} [w_{\alpha_1}, \ldots, w_{\alpha_n}]\,.
\end{equation}
It turns out that the first entry of the symbol encodes its branch cut structure. In particular, a Feynman integral with the symbol $[w_{\alpha_1}, \ldots, w_{\alpha_n}]$ has a discontinuity when $w_{\alpha_1}=0$ or $\infty$. Of course, singular points of Feynman integrals are not random and we expect them to be connected to special values of the Mandelstam variables. Indeed, there exists a so-called first-entry condition which states that $\vec{c}_{\alpha_1, \ldots, \alpha_n}=0$ if $w_{\alpha_1} \neq s_{ij}\,$, i.e. a symbol cannot appear in the solution of the DEs if its first letter is not a Mandelstam variable with a physical discontinuity\footnote{Strictly speaking, it is the absence of discontinuities at certain points that implies first-entry conditions, but this statement should not be inverted. It is possible to write an amplitude using a function which satisfies a first-entry condition, yet has the forbidden singularity. See Section 3 of Ref.~\cite{Zoia:2021zmb} for an explicit example.}\cite{Gaiotto:2011dt}. This turns out to be a very strong condition which in many cases uniquely fixes the values of $\vv{\text{MI}}^{(0)}$\textcolor{gray}{as we will see in Section~\ref{sec:selfdual} \JK{Are we actually going to see this there?...}}. Beyond the first entry, the structure of the symbols is further constrained by the so-called Steinmann relations, which state that there is no double continuity associated with overlapping channels (e.g. $s_{12}$ and $s_{23}$)\cite{20.500.11850/135473, Steinmann:1960, Caron-Huot:2016owq, Dixon:2016nkn}. Overall, such conditions created by physical principles constrain the structure of the symbols and canonical DEs. This is evident when studying the pure MI bases relevant to this thesis (see Refs.~\cite{Abreu:2020jxa, abreu2021twoloop}).
\section{Evaluating master integrals}
After a rather lengthy excursion into the world of IBPs, DEs and symbols, let us remind ourselves where we currently stand in the workflow for computing amplitudes presented in Fig.~\ref{fig:outline}. Having written down the helicity-dependent numerators of each colour-ordered amplitude, we were faced with the task of integrating an enormous number of tensor integrals that belong to many integral families. Then, in Sec.~\ref{sec:reduction}, we mapped these integrals onto integrals within significantly fewer maximal topologies. We then built a system of IBP relations for each of these maximal topologies (containing also the non-IBP, inter-family relations) and reduced the integrals further onto a manageable set of master integrals. In principle, we can now end the amplitude computation. Our result is written in terms of rational coefficients of external kinematics, which are trivial to evaluate the full phase space \JK{Is statement this too strong?}, while the MIs that these coefficients multiply are in general functions that are extremely hard to compute analytically and that exhibit a complicated branch cut structure. As we will see in the next section, there are good reasons why we typically extend our workflow and express the MIs in terms of appropriately chosen special functions. However, it is entirely possible to work with the amplitude at the level of MIs. Indeed, in some cases we have no other choice, since the expansion of the relevant MIs into special functions may not be known. Having phenomenological applications in mind, we list below a few methods which allow us to evaluate these MIs numerically at chosen kinematic points. In fact, we will use the last two in subsequent chapters.
\begin{itemize}
    \item \textbf{Sector decomposition}: This method relies on the parametrisation of integrals in terms of the Feynman parameters and the $\mathcal{U}$ and $\mathcal{F}$ Symanzik polynomials. The integration phase-space of the parameters is split into sectors and subsectors based on a relative ordering between the parameters. This allows us to resolve the singularities present in the integrals and place them in simple functions that can be integrated analytically. What remains to be computed are the coefficients of the $\eps$ poles. They receive contributions from finite integrals only and are computed numerically~\cite{Binoth:2000ps, Binoth:2003ak}. There exist several public codes implementing this algorithm~\cite{Bogner:2007cr, Smirnov:2015mct, Borowka:2015mxa, Heinrich:2023til}.
    \item \textbf{Expansion by regions}: In this method, the integration domain of the loop momenta is divided into appropriate regions where a certain kinematic quantity is small, e.g. $m_i^2/p_j^2 \ll 1$. For each such limit, the integrand is expanded in the corresponding small parameter, resulting in a simpler function. The expanded integrands are then integrated over the \textit{full} space of loop momenta. With certain conditions ~\cite{Jantzen:2011nz}, the original integral can be recovered by summing the individual contributions from the various regions~\cite{Beneke:1997zp, Smirnov:1998vk, Smirnov:1999bza, Pak:2010pt, Ananthanarayan:2018tog}. For numerical implementation, see Refs.~\cite{Ananthanarayan:2020ptw, Jantzen:2012mw, Smirnov:2015mct, Heinrich:2021dbf}. 
    \item \textbf{Generalised series expansion}: For integrals satisfying canonical DEs in Eq.~\ref{eq:canonicalDEsoneform}, it is possible to integrate these DEs along a one-dimensional line segment\footnote{We stress that this method is applicable not only to canonical MIs, but to any set of functions satisfying the canonical DEs. We will explore this further in Chapter~\ref{sec:Hbb}.}. If we know the value of the integrals at one point in the kinematic phase space (i.e. the boundary condition of the DEs) and want to know their value at another point, we construct a path $\gamma(\lambda)$ between them and solve the DEs along it. Thus, the solution is reduced to a one-dimensional problem in $\lambda$, with all the kinematic invariants set to numbers. The solution is then obtained by using an ansatz in the form of a series expansion. 

    Naturally, each such series has a certain radius of convergence within which it is valid. Typically, it is the distance from the centre point of the expansion to the nearest singularity. Thus, in order to obtain the full solution across the entire length of $\gamma(\lambda)$, we split the path into multiple (usually straight line) segments and solve the DEs on them one by one. The value of the solution from the previous segment can then serve as a boundary condition for solving the DEs in the subsequent segment~\cite{Moriello:2019yhu, Bonciani:2019jyb, Frellesvig:2019byn}. We provide a more intuitive, graphical representation of this idea in Fig.~\ref{fig:seriesexp}. Recently, a public implementation of this method has been completed in Ref.~\cite{Hidding:2020ytt}.
    \item \textbf{Auxiliary mass flow}: This method relies on constructing the DEs not with respect to the traditional kinematic invariants, but an auxiliary mass parameter $\eta$. The original integrals can then be recovered by solving the $\eta$-DEs with $\eta = \infty$ as the boundary condition and letting $\eta$ `flow' from $\infty$ to $\ii \varepsilon^+$ \JK{Not sure if this should be $\varepsilon^+$ or $\varepsilon^-$.}. Crucially, the integrals involved in the boundary condition are simpler than the ones we are aiming for. If these prove still too hard to compute, we can iterate the procedure: set up new $\eta^\prime$-DEs for these integrals, obtain the new boundary condition in terms of even simpler integrals, and so on. Eventually, the boundary terms can be expressed in terms of scaleless integrals (which vanish in DR) or single-scale vacuum integrals (which are very simple)~\cite{Liu:2017jxz, Liu:2021wks}. A public implementation of this method has been made available in Ref.~\cite{Liu:2022chg}. \JK{We will return to auxiliary mass flow in Chapter~\ref{sec:selfdual}}.
    \item \JK{Anything else here? Tropical integrals?...}
    \item \JK{Maybe this discussion should be improved - talk about relative timings, precision, etc.}
\end{itemize}
\begin{figure}
    \centering
    \begin{tikzpicture}
		\node (start) at (0,0) {};
		\node (startcaption) at ([yshift=-2pt]start.south) {$s_0$};
		\node (i1) [right of=start, xshift=1em, yshift=0.5em] {};
		\node (i2) [right of=i1, xshift=0.9em, yshift=-1.8em] {};
		\node (finish) [right of=i2, xshift=1em, yshift=1em] {};
		\node (finishcaption) at ([yshift=-2pt]finish.south) {$s$};
        \node (label1) [right of=i1, xshift=-0.6cm, yshift=-0.2cm] {};
        \node (label2) [above right= 1cm of label1] {$\gamma(\lambda)$};
		
		\foreach \i in {start,i1,i2,finish}{
			\filldraw[red] (\i.center) circle (0.1em);
			\draw[red,dashed] (\i.center) circle (2.1em);
		}
		\draw (start.center) to [out=70,in=250] (i1.center) to [out=70,in=180] (i2.center) to [out=0,in=90] (finish.center);
        \draw [{Stealth[scale=1.5]}-] (label1) -- (label2);
	\end{tikzpicture}
    \caption{A pictorial representation of the generalised series expansion method. We connect the kinematic point $s_0$, at which the solution to the DEs is known, to the target point $s$ with a one-dimensional path $\gamma(\lambda)$. The path is split into (usually, but not necessarily, straight) line segments as necessitated by singularities. Each segment admits a solution in the form of a series expansion and has a certain radius of convergence (marked by dashed circles). Stitching together the solutions from individual segments allows us to transport the solution from $s_0$ to $s$.}
    \label{fig:seriesexp}
\end{figure}
%\section{Syzygy relations} \label{sec:syzygies} 
%
\section{Special functions and finite remainders} \label{sec:specialfunctions}
As mentioned above, after reducing the amplitude onto a small set of MIs, we typically make use of the available results for these MIs and expand them into special functions, which we will denote as $\{f_i(p)\}$ for now, with $p$ indicating the collective dependence on external momenta. These functions could involve, e.g. GPLs, elliptic polylogarithms or more case-specific functions better suited to a particular computation. More details on this topic will be presented in Secs.~\ref{Hbbsec:Hbasis} and ~\ref{wyjsec:Permutations}. \JK{Maybe I should make a list here, like in the previous section?}. The expansion of MIs onto $\{f_i(p)\}$ can be easily implemented over the finite fields as a multiplication of the MI coefficients $g_i(p, \eps)$ in Eq.~\ref{eq:ampafterIBP} by a matrix encoding these MIs in terms of the special functions. Schematically, we are left with:
\begin{equation} \label{eq:ampexpandedspfuncs}
    A_n^{(L)} \left(1^{h_1}, 2^{h_2}, \ldots, n^{h_n} \right) = \sum_i q_i(p, \eps) \times \text{mon}_i \big(\{f_i(p)\}\big)\,,
\end{equation}
where mon$_i$ denote monomials formed from the special functions.
%Finally, with a constantly improving understanding of special functions, in many cases it is beneficial to work with a set of functions that provides a fast, stable numerical evaluation. 

Expanding the amplitude onto special functions has several advantages. First and foremost, it unveils the analytic structure that is simply hidden within the MI representation. Second, \JK{Quicker to evaluate?}. Finally, it allows us to subtract from the amplitude its UV and IR poles and define the so-called \textbf{finite remainder}:
\begin{equation} \label{eq:finrem}
    F_n^{(L)} = \lim_{\eps \to 0} \left( A^{(L)} - P^{(L)} A^{(0)} \right)\,,
\end{equation}
where the $L$-loop pole operator $P^{(L)}$ contains both UV and IR divergences. A few words on why we want to work with the finite remainder are in order. Since the UV poles of an amplitude are eliminated in renormalisation, while the IR poles are determined from lower-loop information~\cite{Catani:1998bh, Becher:2009cu, Becher:2009qa, Gardi:2009qi} (see also Appendix~\ref{app:polestructure}), the genuinely new information we compute is contained solely in the non-singular terms. Moreover, the fact of being able to subtract these pre-determined poles from our result, such that we are left with a finite quantity in the limit $\eps \rightarrow 0$, is in itself already a strong check on the calculation. Finally, it has been observed that finite remainders often exhibit simpler analytic structure than the corresponding full amplitude, with certain special functions dropping out after subtracting the poles. Thus, it is the finite remainder, rather than the amplitude itself, that we take as the end point of our computational algorithm in Fig.~\ref{fig:outline}:
\begin{equation}
    F_n^{(L)} \left(1^{h_1}, 2^{h_2}, \ldots, n^{h_n} \right) = \sum_i r_i(p, \eps) \times \text{mon}_i \big(\{f_i(p)\}\big)\,.
\end{equation}
Let us make three remarks about the finite remainder. First, we emphasise that it is dependent on both the renormalisation and regularisation schemes. It is important to bear that in mind when cross-checking results or trying to restore the full amplitude from its remainder. Second, the pole subtraction in Eq.~\ref{eq:finrem} can be straightforwardly implemented over finite fields as a subtraction on the monomial coefficients $q_i$ in Eq.~\ref{eq:ampexpandedspfuncs}. Third, the $\eps$ dependence of the coefficients $r_i$ can be removed by performing their Laurent expansion. This is done up to $\order{\eps^0}$, since any higher-order terms are removed by the limit $\eps \rightarrow 0$ anyway. Once again, the Laurent expansion can be easily encoded within the finite field workflow.

While the special functions $\{f_i\}$ that come from the MIs are complicated functions with poles and branch cuts, the coefficients $r_i$ are rational functions that are computed and reconstructed from finite fields. We remind the reader that they will be usually expressed in terms of the momentum twistors $\mathbf{x}$. In practical applications for computations at the state of the art, the reconstruction processes can be enormously expensive even on modern CPU clusters. For this reason, we find it unavoidable to implement several tools which simplify the coefficients before we need to reconstruct them. They lower the polynomial degrees (see Sec.~\ref{sec:ratfuncs}) and will be covered in detail when discussing the reconstruction strategies in Chapters~\ref{sec:Hbb} and \ref{sec:Wyj}.

We are now ready to take on the computation of scattering amplitudes at the cutting edge of current knowledge. In the next two chapters, we will use the workflow developed here to tackle two-loop, five-point amplitudes with an external mass for two processes directly relevant to LHC phenomenology, $\ppbbh$ and $\ppWgj$.
\end{document}